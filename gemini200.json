{
  "nodes": [
    {
      "id": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "paper",
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "abstract": "In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "citation_count": 977,
      "reference_count": 0,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks.",
      "external_id_dblp": "journals/corr/abs-2403-05530",
      "external_id_arxiv": "2403.05530",
      "external_id_corpusid": 268297180
    },
    {
      "id": "1557386977",
      "type": "author",
      "name": "Machel Reid"
    },
    {
      "id": "2417003",
      "type": "author",
      "name": "Nikolay Savinov"
    },
    {
      "id": "3035073",
      "type": "author",
      "name": "Denis Teplyashin"
    },
    {
      "id": "150077954",
      "type": "author",
      "name": "Dmitry Lepikhin"
    },
    {
      "id": "2542999",
      "type": "author",
      "name": "T. Lillicrap"
    },
    {
      "id": "2285263",
      "type": "author",
      "name": "Jean-Baptiste Alayrac"
    },
    {
      "id": "1737285",
      "type": "author",
      "name": "Radu Soricut"
    },
    {
      "id": "2672644",
      "type": "author",
      "name": "Angeliki Lazaridou"
    },
    {
      "id": "2273534960",
      "type": "author",
      "name": "Orhan Firat"
    },
    {
      "id": "4337102",
      "type": "author",
      "name": "Julian Schrittwieser"
    },
    {
      "id": "2460849",
      "type": "author",
      "name": "Ioannis Antonoglou"
    },
    {
      "id": "1508890387",
      "type": "author",
      "name": "Rohan Anil"
    },
    {
      "id": "148016269",
      "type": "author",
      "name": "Sebastian Borgeaud"
    },
    {
      "id": "2273563615",
      "type": "author",
      "name": "Andrew M. Dai"
    },
    {
      "id": "2143434227",
      "type": "author",
      "name": "Katie Millican"
    },
    {
      "id": "2275180676",
      "type": "author",
      "name": "Ethan Dyer"
    },
    {
      "id": "2143471164",
      "type": "author",
      "name": "Mia Glaese"
    },
    {
      "id": "2070364520",
      "type": "author",
      "name": "Thibault Sottiaux"
    },
    {
      "id": "2275292292",
      "type": "author",
      "name": "Ben-jamin Lee"
    },
    {
      "id": "2290484786",
      "type": "author",
      "name": "Fabio Viola"
    },
    {
      "id": "47447264",
      "type": "author",
      "name": "Malcolm Reynolds"
    },
    {
      "id": "2145139570",
      "type": "author",
      "name": "Yuanzhong Xu"
    },
    {
      "id": "2065370007",
      "type": "author",
      "name": "James Molloy"
    },
    {
      "id": "2249566095",
      "type": "author",
      "name": "Jilin Chen"
    },
    {
      "id": "2090818",
      "type": "author",
      "name": "M. Isard"
    },
    {
      "id": "152399055",
      "type": "author",
      "name": "P. Barham"
    },
    {
      "id": "2146532222",
      "type": "author",
      "name": "Tom Hennigan"
    },
    {
      "id": "2275176102",
      "type": "author",
      "name": "Ross McIlroy"
    },
    {
      "id": "2275525680",
      "type": "author",
      "name": "Melvin Johnson"
    },
    {
      "id": "1698491",
      "type": "author",
      "name": "J. Schalkwyk"
    },
    {
      "id": "2275181648",
      "type": "author",
      "name": "Eli Collins"
    },
    {
      "id": "2143538252",
      "type": "author",
      "name": "Eliza Rutherford"
    },
    {
      "id": "2275185558",
      "type": "author",
      "name": "Erica Moreira"
    },
    {
      "id": "34122449",
      "type": "author",
      "name": "Kareem W. Ayoub"
    },
    {
      "id": "2275186741",
      "type": "author",
      "name": "Megha Goel"
    },
    {
      "id": "1406288863",
      "type": "author",
      "name": "Clemens Meyer"
    },
    {
      "id": "2005813",
      "type": "author",
      "name": "Gregory Thornton"
    },
    {
      "id": "2275729730",
      "type": "author",
      "name": "Zhen Yang"
    },
    {
      "id": "47407464",
      "type": "author",
      "name": "H. Michalewski"
    },
    {
      "id": "2275185143",
      "type": "author",
      "name": "Zaheer Abbas"
    },
    {
      "id": "74530494",
      "type": "author",
      "name": "Nathan Schucher"
    },
    {
      "id": "12679121",
      "type": "author",
      "name": "Ankesh Anand"
    },
    {
      "id": "2275185509",
      "type": "author",
      "name": "Richard Ives"
    },
    {
      "id": "2058168486",
      "type": "author",
      "name": "James Keeling"
    },
    {
      "id": "3257286",
      "type": "author",
      "name": "Karel Lenc"
    },
    {
      "id": "40269586",
      "type": "author",
      "name": "Salem Haykal"
    },
    {
      "id": "2944868",
      "type": "author",
      "name": "Siamak Shakeri"
    },
    {
      "id": "67311962",
      "type": "author",
      "name": "Pranav Shyam"
    },
    {
      "id": "2841893",
      "type": "author",
      "name": "Aakanksha Chowdhery"
    },
    {
      "id": "81387328",
      "type": "author",
      "name": "Roman Ring"
    },
    {
      "id": "2135383313",
      "type": "author",
      "name": "Stephen Spencer"
    },
    {
      "id": "1413718981",
      "type": "author",
      "name": "Eren Sezener"
    },
    {
      "id": "2289035179",
      "type": "author",
      "name": "Luke Vilnis"
    },
    {
      "id": "2275186577",
      "type": "author",
      "name": "Oscar Chang"
    },
    {
      "id": "2288269273",
      "type": "author",
      "name": "Nobuyuki Morioka"
    },
    {
      "id": "2275183383",
      "type": "author",
      "name": "George Tucker"
    },
    {
      "id": "2276211400",
      "type": "author",
      "name": "Ce Zheng"
    },
    {
      "id": "2275186839",
      "type": "author",
      "name": "Oliver Woodman"
    },
    {
      "id": "80930649",
      "type": "author",
      "name": "Nithya Attaluri"
    },
    {
      "id": "2367821",
      "type": "author",
      "name": "Tomás Kociský"
    },
    {
      "id": "2275187189",
      "type": "author",
      "name": "Evgenii Eltyshev"
    },
    {
      "id": "2275535939",
      "type": "author",
      "name": "Xi Chen"
    },
    {
      "id": "2275188933",
      "type": "author",
      "name": "Timothy Chung"
    },
    {
      "id": "1394635460",
      "type": "author",
      "name": "Vittorio Selo"
    },
    {
      "id": "1791585",
      "type": "author",
      "name": "Siddhartha Brahma"
    },
    {
      "id": "1737522",
      "type": "author",
      "name": "Petko Georgiev"
    },
    {
      "id": "133666998",
      "type": "author",
      "name": "Ambrose Slone"
    },
    {
      "id": "2275539055",
      "type": "author",
      "name": "Zhenkai Zhu"
    },
    {
      "id": "2266398107",
      "type": "author",
      "name": "James Lottes"
    },
    {
      "id": "2275178766",
      "type": "author",
      "name": "Siyuan Qiao"
    },
    {
      "id": "2290484287",
      "type": "author",
      "name": "Ben Caine"
    },
    {
      "id": "2287841795",
      "type": "author",
      "name": "Sebastian Riedel"
    },
    {
      "id": "2275176047",
      "type": "author",
      "name": "Alex Tomala"
    },
    {
      "id": "2159545857",
      "type": "author",
      "name": "Martin Chadwick"
    },
    {
      "id": "2253158807",
      "type": "author",
      "name": "J Christopher Love"
    },
    {
      "id": "2070068655",
      "type": "author",
      "name": "Peter Choy"
    },
    {
      "id": "2073395505",
      "type": "author",
      "name": "Sid Mittal"
    },
    {
      "id": "2815290",
      "type": "author",
      "name": "N. Houlsby"
    },
    {
      "id": "2269752766",
      "type": "author",
      "name": "Yunhao Tang"
    },
    {
      "id": "2289446231",
      "type": "author",
      "name": "Matthew Lamm"
    },
    {
      "id": "2275159462",
      "type": "author",
      "name": "Libin Bai"
    },
    {
      "id": "2197671266",
      "type": "author",
      "name": "Qiao Zhang"
    },
    {
      "id": "2253917827",
      "type": "author",
      "name": "Luheng He"
    },
    {
      "id": "2275287219",
      "type": "author",
      "name": "Yong Cheng"
    },
    {
      "id": "2275186692",
      "type": "author",
      "name": "Peter Humphreys"
    },
    {
      "id": "2275290025",
      "type": "author",
      "name": "Yujia Li"
    },
    {
      "id": "1786259",
      "type": "author",
      "name": "Sergey Brin"
    },
    {
      "id": "51042571",
      "type": "author",
      "name": "Albin Cassirer"
    },
    {
      "id": "2283231534",
      "type": "author",
      "name": "Ying-Qi Miao"
    },
    {
      "id": "1780245",
      "type": "author",
      "name": "Lukás Zilka"
    },
    {
      "id": "2275189014",
      "type": "author",
      "name": "Taylor Tobin"
    },
    {
      "id": "2266735761",
      "type": "author",
      "name": "Kelvin Xu"
    },
    {
      "id": "2161966573",
      "type": "author",
      "name": "Lev Proleev"
    },
    {
      "id": "2275175792",
      "type": "author",
      "name": "Daniel Sohn"
    },
    {
      "id": "2275182383",
      "type": "author",
      "name": "Alberto Magni"
    },
    {
      "id": "2258347245",
      "type": "author",
      "name": "Lisa Anne Hendricks"
    },
    {
      "id": "2290513267",
      "type": "author",
      "name": "Isabel Gao"
    },
    {
      "id": "2217756237",
      "type": "author",
      "name": "Santiago Ontan'on"
    },
    {
      "id": "2275177720",
      "type": "author",
      "name": "Oskar Bunyan"
    },
    {
      "id": "2275185667",
      "type": "author",
      "name": "Nathan Byrd"
    },
    {
      "id": "2275537981",
      "type": "author",
      "name": "Abhanshu Sharma"
    },
    {
      "id": "48335426",
      "type": "author",
      "name": "Biao Zhang"
    },
    {
      "id": "2290662953",
      "type": "author",
      "name": "Mario Pinto"
    },
    {
      "id": "2275170606",
      "type": "author",
      "name": "Rishika Sinha"
    },
    {
      "id": "18138802",
      "type": "author",
      "name": "Harsh Mehta"
    },
    {
      "id": "2275186531",
      "type": "author",
      "name": "Dawei Jia"
    },
    {
      "id": "1413064976",
      "type": "author",
      "name": "Sergi Caelles"
    },
    {
      "id": "1991019030",
      "type": "author",
      "name": "Albert Webson"
    },
    {
      "id": "2275162692",
      "type": "author",
      "name": "Alex Morris"
    },
    {
      "id": "2080504963",
      "type": "author",
      "name": "Becca Roelofs"
    },
    {
      "id": "2290634593",
      "type": "author",
      "name": "Yifan Ding"
    },
    {
      "id": "86898863",
      "type": "author",
      "name": "Robin Strudel"
    },
    {
      "id": "2275193471",
      "type": "author",
      "name": "Xuehan Xiong"
    },
    {
      "id": "39687627",
      "type": "author",
      "name": "Marvin Ritter"
    },
    {
      "id": "2256989598",
      "type": "author",
      "name": "Mostafa Dehghani"
    },
    {
      "id": "1706980",
      "type": "author",
      "name": "R. Chaabouni"
    },
    {
      "id": "2078909017",
      "type": "author",
      "name": "Abhijit Karmarkar"
    },
    {
      "id": "2290484705",
      "type": "author",
      "name": "Guangda Lai"
    },
    {
      "id": "3468078",
      "type": "author",
      "name": "Fabian Mentzer"
    },
    {
      "id": "2290664586",
      "type": "author",
      "name": "Bibo Xu"
    },
    {
      "id": "2261797906",
      "type": "author",
      "name": "YaGuang Li"
    },
    {
      "id": "2275534739",
      "type": "author",
      "name": "Yujing Zhang"
    },
    {
      "id": "40470211",
      "type": "author",
      "name": "T. Paine"
    },
    {
      "id": "40034895",
      "type": "author",
      "name": "Alex Goldin"
    },
    {
      "id": "3007442",
      "type": "author",
      "name": "Behnam Neyshabur"
    },
    {
      "id": "1734809439",
      "type": "author",
      "name": "Kate Baumli"
    },
    {
      "id": "6639036",
      "type": "author",
      "name": "Anselm Levskaya"
    },
    {
      "id": "2274104519",
      "type": "author",
      "name": "Michael Laskin"
    },
    {
      "id": "2275193644",
      "type": "author",
      "name": "Wenhao Jia"
    },
    {
      "id": "34269227",
      "type": "author",
      "name": "Jack W. Rae"
    },
    {
      "id": "2268673324",
      "type": "author",
      "name": "Kefan Xiao"
    },
    {
      "id": "2290485493",
      "type": "author",
      "name": "Antoine He"
    },
    {
      "id": "2290487747",
      "type": "author",
      "name": "Skye Giordano"
    },
    {
      "id": "2307454258",
      "type": "author",
      "name": "Lakshman Yagati"
    },
    {
      "id": "143783339",
      "type": "author",
      "name": "Jean-Baptiste Lespiau"
    },
    {
      "id": "122704930",
      "type": "author",
      "name": "Paul Natsev"
    },
    {
      "id": "2275185831",
      "type": "author",
      "name": "Sanjay Ganapathy"
    },
    {
      "id": "144097210",
      "type": "author",
      "name": "Fangyu Liu"
    },
    {
      "id": "2290487616",
      "type": "author",
      "name": "Danilo Martins"
    },
    {
      "id": "2249840944",
      "type": "author",
      "name": "Nanxin Chen"
    },
    {
      "id": "2275191526",
      "type": "author",
      "name": "Yunhan Xu"
    },
    {
      "id": "2275180117",
      "type": "author",
      "name": "Megan Barnes"
    },
    {
      "id": "2268760156",
      "type": "author",
      "name": "Rhys May"
    },
    {
      "id": "2275188533",
      "type": "author",
      "name": "Arpi Vezer"
    },
    {
      "id": "2275114643",
      "type": "author",
      "name": "Junhyuk Oh"
    },
    {
      "id": "2118834006",
      "type": "author",
      "name": "Ken Franko"
    },
    {
      "id": "2273670422",
      "type": "author",
      "name": "Sophie Bridgers"
    },
    {
      "id": "2275832693",
      "type": "author",
      "name": "Ruizhe Zhao"
    },
    {
      "id": "2275291909",
      "type": "author",
      "name": "Boxi Wu"
    },
    {
      "id": "40608942",
      "type": "author",
      "name": "Basil Mustafa"
    },
    {
      "id": "2290487447",
      "type": "author",
      "name": "Sean Sechrist"
    },
    {
      "id": "3166516",
      "type": "author",
      "name": "Emilio Parisotto"
    },
    {
      "id": "2598683",
      "type": "author",
      "name": "Thanumalayan Sankaranarayana Pillai"
    },
    {
      "id": "2290487293",
      "type": "author",
      "name": "Chris Larkin"
    },
    {
      "id": "2275149073",
      "type": "author",
      "name": "Chenjie Gu"
    },
    {
      "id": "2275186804",
      "type": "author",
      "name": "Christina Sorokin"
    },
    {
      "id": "2048712",
      "type": "author",
      "name": "M. Krikun"
    },
    {
      "id": "2275182203",
      "type": "author",
      "name": "Alexey Guseynov"
    },
    {
      "id": "2065404873",
      "type": "author",
      "name": "Jessica Landon"
    },
    {
      "id": "2275187147",
      "type": "author",
      "name": "Romina Datta"
    },
    {
      "id": "1863250",
      "type": "author",
      "name": "A. Pritzel"
    },
    {
      "id": "2151245633",
      "type": "author",
      "name": "Phoebe Thacker"
    },
    {
      "id": "2275801132",
      "type": "author",
      "name": "Fan Yang"
    },
    {
      "id": "2290487874",
      "type": "author",
      "name": "Kevin Hui"
    },
    {
      "id": "119556335",
      "type": "author",
      "name": "A.E. Hauth"
    },
    {
      "id": "2273556813",
      "type": "author",
      "name": "Chih-Kuan Yeh"
    },
    {
      "id": "2290481818",
      "type": "author",
      "name": "David Barker"
    },
    {
      "id": "1423275766",
      "type": "author",
      "name": "J. Mao-Jones"
    },
    {
      "id": "2166051497",
      "type": "author",
      "name": "Sophia Austin"
    },
    {
      "id": "2307453241",
      "type": "author",
      "name": "Hannah Sheahan"
    },
    {
      "id": "2620528",
      "type": "author",
      "name": "Parker Schuh"
    },
    {
      "id": "2275188153",
      "type": "author",
      "name": "James Svensson"
    },
    {
      "id": "2275193365",
      "type": "author",
      "name": "Rohan Jain"
    },
    {
      "id": "96641652",
      "type": "author",
      "name": "V. Ramasesh"
    },
    {
      "id": "2275185833",
      "type": "author",
      "name": "Anton Briukhov"
    },
    {
      "id": "2275180366",
      "type": "author",
      "name": "D. Chung"
    },
    {
      "id": "51029932",
      "type": "author",
      "name": "Tamara von Glehn"
    },
    {
      "id": "2275166845",
      "type": "author",
      "name": "Christina Butterfield"
    },
    {
      "id": "2275184551",
      "type": "author",
      "name": "Priya Jhakra"
    },
    {
      "id": "2275252154",
      "type": "author",
      "name": "Matt Wiethoff"
    },
    {
      "id": "2275193725",
      "type": "author",
      "name": "Justin Frye"
    },
    {
      "id": "2275175432",
      "type": "author",
      "name": "Jordan Grimstad"
    },
    {
      "id": "2158369306",
      "type": "author",
      "name": "Beer Changpinyo"
    },
    {
      "id": "153892869",
      "type": "author",
      "name": "Charline Le Lan"
    },
    {
      "id": "2275181572",
      "type": "author",
      "name": "Anna Bortsova"
    },
    {
      "id": "2275892922",
      "type": "author",
      "name": "Yonghui Wu"
    },
    {
      "id": "2767859",
      "type": "author",
      "name": "P. Voigtlaender"
    },
    {
      "id": "2279918122",
      "type": "author",
      "name": "Tara N. Sainath"
    },
    {
      "id": "2275575378",
      "type": "author",
      "name": "Charlotte Smith"
    },
    {
      "id": "2191689971",
      "type": "author",
      "name": "Will Hawkins"
    },
    {
      "id": "2275191626",
      "type": "author",
      "name": "Kris Cao"
    },
    {
      "id": "2275186515",
      "type": "author",
      "name": "James Besley"
    },
    {
      "id": "2059763226",
      "type": "author",
      "name": "S. Srinivasan"
    },
    {
      "id": "3175815",
      "type": "author",
      "name": "Mark Omernick"
    },
    {
      "id": "2160887964",
      "type": "author",
      "name": "Colin Gaffney"
    },
    {
      "id": "1956049835",
      "type": "author",
      "name": "G. Surita"
    },
    {
      "id": "2290484991",
      "type": "author",
      "name": "Ryan Burnell"
    },
    {
      "id": "2143374656",
      "type": "author",
      "name": "Bogdan Damoc"
    },
    {
      "id": "2275220028",
      "type": "author",
      "name": "Junwhan Ahn"
    },
    {
      "id": "2285740851",
      "type": "author",
      "name": "Andrew Brock"
    },
    {
      "id": "2146532125",
      "type": "author",
      "name": "Mantas Pajarskas"
    },
    {
      "id": "2275187155",
      "type": "author",
      "name": "Anastasia Petrushkina"
    },
    {
      "id": "30155667",
      "type": "author",
      "name": "Seb Noury"
    },
    {
      "id": "2275186192",
      "type": "author",
      "name": "Lorenzo Blanco"
    },
    {
      "id": "1754860",
      "type": "author",
      "name": "Kevin Swersky"
    },
    {
      "id": "2275185727",
      "type": "author",
      "name": "Arun Ahuja"
    },
    {
      "id": "2261737895",
      "type": "author",
      "name": "Thi Avrahami"
    },
    {
      "id": "40055795",
      "type": "author",
      "name": "Vedant Misra"
    },
    {
      "id": "2275184736",
      "type": "author",
      "name": "Raoul de Liedekerke"
    },
    {
      "id": "2275181534",
      "type": "author",
      "name": "Mariko Iinuma"
    },
    {
      "id": "144703404",
      "type": "author",
      "name": "A. Polozov"
    },
    {
      "id": "143981350",
      "type": "author",
      "name": "Sarah York"
    },
    {
      "id": "47568983",
      "type": "author",
      "name": "George van den Driessche"
    },
    {
      "id": "2275185732",
      "type": "author",
      "name": "Paul Michel"
    },
    {
      "id": "2273650801",
      "type": "author",
      "name": "Justin Chiu"
    },
    {
      "id": "46901218",
      "type": "author",
      "name": "Rory Blevins"
    },
    {
      "id": "2275185661",
      "type": "author",
      "name": "Zach Gleicher"
    },
    {
      "id": "39257069",
      "type": "author",
      "name": "Adrià Recasens"
    },
    {
      "id": "2275186093",
      "type": "author",
      "name": "Alban Rrustemi"
    },
    {
      "id": "1980809",
      "type": "author",
      "name": "E. Gribovskaya"
    },
    {
      "id": "2275277736",
      "type": "author",
      "name": "Aurko Roy"
    },
    {
      "id": "2290487054",
      "type": "author",
      "name": "Wiktor Gworek"
    },
    {
      "id": "2275186656",
      "type": "author",
      "name": "S'ebastien M. R. Arnold"
    },
    {
      "id": "2275291886",
      "type": "author",
      "name": "Lisa Lee"
    },
    {
      "id": "2267341862",
      "type": "author",
      "name": "James Lee-Thorp"
    },
    {
      "id": "2090812426",
      "type": "author",
      "name": "M. Maggioni"
    },
    {
      "id": "2275183119",
      "type": "author",
      "name": "Enrique Piqueras"
    },
    {
      "id": "2051018967",
      "type": "author",
      "name": "Kartikeya Badola"
    },
    {
      "id": "2425230",
      "type": "author",
      "name": "S. Vikram"
    },
    {
      "id": "2275585027",
      "type": "author",
      "name": "Lucas Gonzalez"
    },
    {
      "id": "2275186584",
      "type": "author",
      "name": "Anirudh Baddepudi"
    },
    {
      "id": "2268665228",
      "type": "author",
      "name": "Evan Senter"
    },
    {
      "id": "2261961752",
      "type": "author",
      "name": "J. Devlin"
    },
    {
      "id": "47901308",
      "type": "author",
      "name": "James Qin"
    },
    {
      "id": "2275148073",
      "type": "author",
      "name": "Michael Azzam"
    },
    {
      "id": "1994939814",
      "type": "author",
      "name": "Maja Trebacz"
    },
    {
      "id": "35930544",
      "type": "author",
      "name": "M. Polacek"
    },
    {
      "id": "2290485355",
      "type": "author",
      "name": "Kashyap Krishnakumar"
    },
    {
      "id": "2275193337",
      "type": "author",
      "name": "Shuo-yiin Chang"
    },
    {
      "id": "2275176212",
      "type": "author",
      "name": "Matthew Tung"
    },
    {
      "id": "2275187196",
      "type": "author",
      "name": "Ivo Penchev"
    },
    {
      "id": "2258551072",
      "type": "author",
      "name": "Rishabh Joshi"
    },
    {
      "id": "2275180557",
      "type": "author",
      "name": "Kate Olszewska"
    },
    {
      "id": "2275184985",
      "type": "author",
      "name": "Carrie Muir"
    },
    {
      "id": "2275185968",
      "type": "author",
      "name": "Mateo Wirth"
    },
    {
      "id": "2275184113",
      "type": "author",
      "name": "Ale Jakse Hartman"
    },
    {
      "id": "2160888100",
      "type": "author",
      "name": "Joshua Newlan"
    },
    {
      "id": "2252586080",
      "type": "author",
      "name": "S. Kashem"
    },
    {
      "id": "2218882489",
      "type": "author",
      "name": "Vijay Bolina"
    },
    {
      "id": "2290484418",
      "type": "author",
      "name": "Elahe Dabir"
    },
    {
      "id": "3038326",
      "type": "author",
      "name": "Joost R. van Amersfoort"
    },
    {
      "id": "2275130349",
      "type": "author",
      "name": "Zafarali Ahmed"
    },
    {
      "id": "2275185511",
      "type": "author",
      "name": "James Cobon-Kerr"
    },
    {
      "id": "2269391198",
      "type": "author",
      "name": "Aishwarya B Kamath"
    },
    {
      "id": "2259962018",
      "type": "author",
      "name": "A. M. Hrafnkelsson"
    },
    {
      "id": "2274787555",
      "type": "author",
      "name": "Le Hou"
    },
    {
      "id": "2290485798",
      "type": "author",
      "name": "Ian Mackinnon"
    },
    {
      "id": "2156930381",
      "type": "author",
      "name": "Alexandre Frechette"
    },
    {
      "id": "51210148",
      "type": "author",
      "name": "Eric Noland"
    },
    {
      "id": "2275182246",
      "type": "author",
      "name": "Xiance Si"
    },
    {
      "id": "2779842",
      "type": "author",
      "name": "Emanuel Taropa"
    },
    {
      "id": "2347193353",
      "type": "author",
      "name": "Dong Li"
    },
    {
      "id": "2275183277",
      "type": "author",
      "name": "Phil Crone"
    },
    {
      "id": "4478284",
      "type": "author",
      "name": "Anmol Gulati"
    },
    {
      "id": "2275180682",
      "type": "author",
      "name": "S'ebastien Cevey"
    },
    {
      "id": "2275173231",
      "type": "author",
      "name": "Jonas Adler"
    },
    {
      "id": "2275786213",
      "type": "author",
      "name": "Ada Ma"
    },
    {
      "id": "2275185813",
      "type": "author",
      "name": "David Silver"
    },
    {
      "id": "148152480",
      "type": "author",
      "name": "Simon Tokumine"
    },
    {
      "id": "2067745837",
      "type": "author",
      "name": "Richard Powell"
    },
    {
      "id": "2275280377",
      "type": "author",
      "name": "Stephan Lee"
    },
    {
      "id": "2275571997",
      "type": "author",
      "name": "Michael B. Chang"
    },
    {
      "id": "2275396476",
      "type": "author",
      "name": "Samer Hassan"
    },
    {
      "id": "2007712128",
      "type": "author",
      "name": "Diana Mincu"
    },
    {
      "id": "2064599701",
      "type": "author",
      "name": "Antoine Yang"
    },
    {
      "id": "153898744",
      "type": "author",
      "name": "Nir Levine"
    },
    {
      "id": "2275186701",
      "type": "author",
      "name": "Jenny Brennan"
    },
    {
      "id": "2249764807",
      "type": "author",
      "name": "Mingqiu Wang"
    },
    {
      "id": "2265053608",
      "type": "author",
      "name": "Sarah Hodkinson"
    },
    {
      "id": "2144551262",
      "type": "author",
      "name": "Jeffrey Zhao"
    },
    {
      "id": "2290487597",
      "type": "author",
      "name": "Josh Lipschultz"
    },
    {
      "id": "20702300",
      "type": "author",
      "name": "Aedan Pope"
    },
    {
      "id": "2275767067",
      "type": "author",
      "name": "Cheng Li"
    },
    {
      "id": "2121764",
      "type": "author",
      "name": "Laurent El Shafey"
    },
    {
      "id": "2264591527",
      "type": "author",
      "name": "M. Paganini"
    },
    {
      "id": "2269733876",
      "type": "author",
      "name": "Sholto Douglas"
    },
    {
      "id": "2266464503",
      "type": "author",
      "name": "Bernd Bohnet"
    },
    {
      "id": "2274107421",
      "type": "author",
      "name": "Fabio Pardo"
    },
    {
      "id": "2275182230",
      "type": "author",
      "name": "Seth Odoom"
    },
    {
      "id": "2269541835",
      "type": "author",
      "name": "Mihaela Rosca"
    },
    {
      "id": "2267546965",
      "type": "author",
      "name": "Cicero Nogueira dos Santos"
    },
    {
      "id": "2275185640",
      "type": "author",
      "name": "Kedar Soparkar"
    },
    {
      "id": "35099444",
      "type": "author",
      "name": "A. Guez"
    },
    {
      "id": "2275187110",
      "type": "author",
      "name": "Tom Hudson"
    },
    {
      "id": "2275188563",
      "type": "author",
      "name": "Steven Hansen"
    },
    {
      "id": "50844587",
      "type": "author",
      "name": "Chulayuth Asawaroengchai"
    },
    {
      "id": "104000494",
      "type": "author",
      "name": "Ravichandra Addanki"
    },
    {
      "id": "2290486855",
      "type": "author",
      "name": "Tianhe Yu"
    },
    {
      "id": "3448463",
      "type": "author",
      "name": "Wojciech Stokowiec"
    },
    {
      "id": "2258793616",
      "type": "author",
      "name": "Mina Khan"
    },
    {
      "id": "2243002880",
      "type": "author",
      "name": "Justin Gilmer"
    },
    {
      "id": "2253808003",
      "type": "author",
      "name": "Jaehoon Lee"
    },
    {
      "id": "2290485108",
      "type": "author",
      "name": "Carrie Grimes Bostock"
    },
    {
      "id": "1996199677",
      "type": "author",
      "name": "Keran Rong"
    },
    {
      "id": "2263289033",
      "type": "author",
      "name": "Jonathan Caton"
    },
    {
      "id": "2275184275",
      "type": "author",
      "name": "Pedram Pejman"
    },
    {
      "id": "1696719",
      "type": "author",
      "name": "Filip Pavetic"
    },
    {
      "id": "2259937157",
      "type": "author",
      "name": "Geoff Brown"
    },
    {
      "id": "2290595918",
      "type": "author",
      "name": "Vivek Sharma"
    },
    {
      "id": "2170162986",
      "type": "author",
      "name": "Mario Luvci'c"
    },
    {
      "id": "2275176205",
      "type": "author",
      "name": "Rajkumar Samuel"
    },
    {
      "id": "2941141",
      "type": "author",
      "name": "Josip Djolonga"
    },
    {
      "id": "2063800905",
      "type": "author",
      "name": "Amol Mandhane"
    },
    {
      "id": "2275187845",
      "type": "author",
      "name": "Lars Lowe Sjosund"
    },
    {
      "id": "118801223",
      "type": "author",
      "name": "Elena Buchatskaya"
    },
    {
      "id": "2275158927",
      "type": "author",
      "name": "Elspeth White"
    },
    {
      "id": "2201776471",
      "type": "author",
      "name": "Natalie Clay"
    },
    {
      "id": "2260169185",
      "type": "author",
      "name": "Jiepu Jiang"
    },
    {
      "id": "2275798209",
      "type": "author",
      "name": "Hyeontaek Lim"
    },
    {
      "id": "38637384",
      "type": "author",
      "name": "Ross Hemsley"
    },
    {
      "id": "2275184618",
      "type": "author",
      "name": "Jane Labanowski"
    },
    {
      "id": "41019080",
      "type": "author",
      "name": "Nicola De Cao"
    },
    {
      "id": "2275188258",
      "type": "author",
      "name": "David Steiner"
    },
    {
      "id": "3362306",
      "type": "author",
      "name": "Sayed Hadi Hashemi"
    },
    {
      "id": "2288056644",
      "type": "author",
      "name": "Jacob Austin"
    },
    {
      "id": "2105841261",
      "type": "author",
      "name": "Anita Gergely"
    },
    {
      "id": "2221119859",
      "type": "author",
      "name": "Tim Blyth"
    },
    {
      "id": "2275190309",
      "type": "author",
      "name": "Joe Stanton"
    },
    {
      "id": "2272718153",
      "type": "author",
      "name": "K. Shivakumar"
    },
    {
      "id": "9356387",
      "type": "author",
      "name": "Aditya Siddhant"
    },
    {
      "id": "39552848",
      "type": "author",
      "name": "Anders Andreassen"
    },
    {
      "id": "2279996944",
      "type": "author",
      "name": "Carlos L. Araya"
    },
    {
      "id": "2275187415",
      "type": "author",
      "name": "Nikhil Sethi"
    },
    {
      "id": "2934334",
      "type": "author",
      "name": "Rakesh Shivanna"
    },
    {
      "id": "2275161833",
      "type": "author",
      "name": "Steven Hand"
    },
    {
      "id": "12295226",
      "type": "author",
      "name": "Ankur Bapna"
    },
    {
      "id": "2402489",
      "type": "author",
      "name": "A. Khodaei"
    },
    {
      "id": "19200186",
      "type": "author",
      "name": "Antoine Miech"
    },
    {
      "id": "2287809580",
      "type": "author",
      "name": "Garrett Tanzer"
    },
    {
      "id": "1394189636",
      "type": "author",
      "name": "Andy Swing"
    },
    {
      "id": "41037204",
      "type": "author",
      "name": "S. Thakoor"
    },
    {
      "id": "2291169360",
      "type": "author",
      "name": "Zhufeng Pan"
    },
    {
      "id": "81408931",
      "type": "author",
      "name": "Zachary Nado"
    },
    {
      "id": "2218062983",
      "type": "author",
      "name": "Stephanie Winkler"
    },
    {
      "id": "2256337021",
      "type": "author",
      "name": "Dian Yu"
    },
    {
      "id": "144413479",
      "type": "author",
      "name": "Mohammad Saleh"
    },
    {
      "id": "108173905",
      "type": "author",
      "name": "Lorenzo Maggiore"
    },
    {
      "id": "2159207795",
      "type": "author",
      "name": "Iain Barr"
    },
    {
      "id": "2275187490",
      "type": "author",
      "name": "Minh Giang"
    },
    {
      "id": "2275186582",
      "type": "author",
      "name": "Thais Kagohara"
    },
    {
      "id": "1841008",
      "type": "author",
      "name": "Ivo Danihelka"
    },
    {
      "id": "2275176043",
      "type": "author",
      "name": "Amit Marathe"
    },
    {
      "id": "2275181199",
      "type": "author",
      "name": "Vladimir Feinberg"
    },
    {
      "id": "2275176049",
      "type": "author",
      "name": "Mohamed Elhawaty"
    },
    {
      "id": "3404697",
      "type": "author",
      "name": "Nimesh Ghelani"
    },
    {
      "id": "48257711",
      "type": "author",
      "name": "Dan Horgan"
    },
    {
      "id": "2275121046",
      "type": "author",
      "name": "Helen Miller"
    },
    {
      "id": "2275184334",
      "type": "author",
      "name": "Lexi Walker"
    },
    {
      "id": "1825728",
      "type": "author",
      "name": "Richard Tanburn"
    },
    {
      "id": "2275180099",
      "type": "author",
      "name": "Mukarram Tariq"
    },
    {
      "id": "2275113487",
      "type": "author",
      "name": "Disha Shrivastava"
    },
    {
      "id": "2290487337",
      "type": "author",
      "name": "Fei Xia"
    },
    {
      "id": "2284761701",
      "type": "author",
      "name": "Chung-Cheng Chiu"
    },
    {
      "id": "2333511945",
      "type": "author",
      "name": "Zoe Ashwood"
    },
    {
      "id": "2290486431",
      "type": "author",
      "name": "Khuslen Baatarsukh"
    },
    {
      "id": "2412073",
      "type": "author",
      "name": "Sina Samangooei"
    },
    {
      "id": "2275177971",
      "type": "author",
      "name": "Fred Alcober"
    },
    {
      "id": "2163521750",
      "type": "author",
      "name": "Axel Stjerngren"
    },
    {
      "id": "2258235140",
      "type": "author",
      "name": "P. Komarek"
    },
    {
      "id": "2275185589",
      "type": "author",
      "name": "Katerina Tsihlas"
    },
    {
      "id": "11167300",
      "type": "author",
      "name": "Anudhyan Boral"
    },
    {
      "id": "89066101",
      "type": "author",
      "name": "R. Comanescu"
    },
    {
      "id": "2275275439",
      "type": "author",
      "name": "Jeremy Chen"
    },
    {
      "id": "7247867",
      "type": "author",
      "name": "Ruibo Liu"
    },
    {
      "id": "2275185808",
      "type": "author",
      "name": "Dawn Bloxwich"
    },
    {
      "id": "2182971260",
      "type": "author",
      "name": "Charlie Chen"
    },
    {
      "id": "2265240845",
      "type": "author",
      "name": "Yanhua Sun"
    },
    {
      "id": "2275173841",
      "type": "author",
      "name": "Fangxi-aoyu Feng"
    },
    {
      "id": "2251517316",
      "type": "author",
      "name": "M. Mauger"
    },
    {
      "id": "1404332584",
      "type": "author",
      "name": "Xerxes Dotiwalla"
    },
    {
      "id": "2297847306",
      "type": "author",
      "name": "V. Hellendoorn"
    },
    {
      "id": "2275184531",
      "type": "author",
      "name": "Michael Sharman"
    },
    {
      "id": "2275187038",
      "type": "author",
      "name": "Ivy Zheng"
    },
    {
      "id": "2256873459",
      "type": "author",
      "name": "Krishna Haridasan"
    },
    {
      "id": "1403998955",
      "type": "author",
      "name": "Gabriel Barth-Maron"
    },
    {
      "id": "2275181554",
      "type": "author",
      "name": "Craig Swanson"
    },
    {
      "id": "2275184739",
      "type": "author",
      "name": "Dominika Rogozi'nska"
    },
    {
      "id": "2290741315",
      "type": "author",
      "name": "Alek Andreev"
    },
    {
      "id": "2249760524",
      "type": "author",
      "name": "P. Rubenstein"
    },
    {
      "id": "2275189194",
      "type": "author",
      "name": "Ruoxin Sang"
    },
    {
      "id": "2265528853",
      "type": "author",
      "name": "Dan Hurt"
    },
    {
      "id": "2275189864",
      "type": "author",
      "name": "Gamaleldin Elsayed"
    },
    {
      "id": "2290529512",
      "type": "author",
      "name": "Ren-shen Wang"
    },
    {
      "id": "2290485332",
      "type": "author",
      "name": "Dave Lacey"
    },
    {
      "id": "2279830514",
      "type": "author",
      "name": "Anastasija Ili'c"
    },
    {
      "id": "2275112414",
      "type": "author",
      "name": "Yao Zhao"
    },
    {
      "id": "2215449616",
      "type": "author",
      "name": "Woohyun Han"
    },
    {
      "id": "2257256357",
      "type": "author",
      "name": "Lora Aroyo"
    },
    {
      "id": "2275177173",
      "type": "author",
      "name": "Chimezie Iwuanyanwu"
    },
    {
      "id": "48942032",
      "type": "author",
      "name": "Vitaly Nikolaev"
    },
    {
      "id": "40627523",
      "type": "author",
      "name": "Balaji Lakshminarayanan"
    },
    {
      "id": "2290484919",
      "type": "author",
      "name": "Sadegh Jazayeri"
    },
    {
      "id": "31713635",
      "type": "author",
      "name": "Raphael Lopez Kaufman"
    },
    {
      "id": "2150348369",
      "type": "author",
      "name": "Mani Varadarajan"
    },
    {
      "id": "118505443",
      "type": "author",
      "name": "Chetan Tekur"
    },
    {
      "id": "2275187305",
      "type": "author",
      "name": "Doug Fritz"
    },
    {
      "id": "2140488873",
      "type": "author",
      "name": "Misha Khalman"
    },
    {
      "id": "2257286979",
      "type": "author",
      "name": "David Reitter"
    },
    {
      "id": "2290487762",
      "type": "author",
      "name": "Kingshuk Dasgupta"
    },
    {
      "id": "1658856741",
      "type": "author",
      "name": "Shourya Sarcar"
    },
    {
      "id": "103861813",
      "type": "author",
      "name": "T. Ornduff"
    },
    {
      "id": "2265527968",
      "type": "author",
      "name": "Javier Snaider"
    },
    {
      "id": "2174667321",
      "type": "author",
      "name": "Fantine Huot"
    },
    {
      "id": "2275694953",
      "type": "author",
      "name": "Johnson Jia"
    },
    {
      "id": "2275186180",
      "type": "author",
      "name": "Rupert Kemp"
    },
    {
      "id": "1702423",
      "type": "author",
      "name": "Nejc Trdin"
    },
    {
      "id": "2275186554",
      "type": "author",
      "name": "Anitha Vijayakumar"
    },
    {
      "id": "2290490486",
      "type": "author",
      "name": "Lucy Kim"
    },
    {
      "id": "2269460640",
      "type": "author",
      "name": "Christof Angermueller"
    },
    {
      "id": "2290485653",
      "type": "author",
      "name": "Li Lao"
    },
    {
      "id": "2275249023",
      "type": "author",
      "name": "Tianqi Liu"
    },
    {
      "id": "2290556567",
      "type": "author",
      "name": "Haibin Zhang"
    },
    {
      "id": "2290485958",
      "type": "author",
      "name": "David Engel"
    },
    {
      "id": "2275190069",
      "type": "author",
      "name": "Somer Greene"
    },
    {
      "id": "2275169305",
      "type": "author",
      "name": "Anais White"
    },
    {
      "id": "2290488307",
      "type": "author",
      "name": "Jessica Austin"
    },
    {
      "id": "2290666013",
      "type": "author",
      "name": "Lilly Taylor"
    },
    {
      "id": "2275181309",
      "type": "author",
      "name": "Shereen Ashraf"
    },
    {
      "id": "2290539499",
      "type": "author",
      "name": "Dangyi Liu"
    },
    {
      "id": "2280669236",
      "type": "author",
      "name": "Maria Georgaki"
    },
    {
      "id": "2290485789",
      "type": "author",
      "name": "Irene Cai"
    },
    {
      "id": "2275177999",
      "type": "author",
      "name": "Yana Kulizhskaya"
    },
    {
      "id": "2096063076",
      "type": "author",
      "name": "Sonam Goenka"
    },
    {
      "id": "4125424",
      "type": "author",
      "name": "Brennan Saeta"
    },
    {
      "id": "4529644",
      "type": "author",
      "name": "Kiran Vodrahalli"
    },
    {
      "id": "2290488254",
      "type": "author",
      "name": "Christian Frank"
    },
    {
      "id": "47182967",
      "type": "author",
      "name": "D. Cesare"
    },
    {
      "id": "2275186837",
      "type": "author",
      "name": "Brona Robenek"
    },
    {
      "id": "2290487825",
      "type": "author",
      "name": "Harry Richardson"
    },
    {
      "id": "2275186342",
      "type": "author",
      "name": "Mahmoud Alnahlawi"
    },
    {
      "id": "2275187959",
      "type": "author",
      "name": "Christopher Yew"
    },
    {
      "id": "2275181434",
      "type": "author",
      "name": "Priya Ponnapalli"
    },
    {
      "id": "1749128",
      "type": "author",
      "name": "M. Tagliasacchi"
    },
    {
      "id": "2275188906",
      "type": "author",
      "name": "Alex Korchemniy"
    },
    {
      "id": "2275999038",
      "type": "author",
      "name": "Yelin Kim"
    },
    {
      "id": "2275195333",
      "type": "author",
      "name": "Dinghua Li"
    },
    {
      "id": "2080520726",
      "type": "author",
      "name": "B. Rosgen"
    },
    {
      "id": "2275186260",
      "type": "author",
      "name": "Kyle Levin"
    },
    {
      "id": "2275187022",
      "type": "author",
      "name": "Jeremy Wiesner"
    },
    {
      "id": "2275187506",
      "type": "author",
      "name": "Praseem Banzal"
    },
    {
      "id": "2275182960",
      "type": "author",
      "name": "Praveen Srinivasan"
    },
    {
      "id": "2254035020",
      "type": "author",
      "name": "Hongkun Yu"
    },
    {
      "id": "2275186671",
      "type": "author",
      "name": "cCauglar Unlu"
    },
    {
      "id": "2275179889",
      "type": "author",
      "name": "David Reid"
    },
    {
      "id": "9941702",
      "type": "author",
      "name": "Zora Tung"
    },
    {
      "id": "2591720",
      "type": "author",
      "name": "D. Finchelstein"
    },
    {
      "id": "2290629265",
      "type": "author",
      "name": "Ravin Kumar"
    },
    {
      "id": "2288791213",
      "type": "author",
      "name": "A. Elisseeff"
    },
    {
      "id": "2290557316",
      "type": "author",
      "name": "Jin Huang"
    },
    {
      "id": "2290594698",
      "type": "author",
      "name": "Ming Zhang"
    },
    {
      "id": "2070271342",
      "type": "author",
      "name": "Rui Zhu"
    },
    {
      "id": "2275185644",
      "type": "author",
      "name": "Ricardo Aguilar"
    },
    {
      "id": "2275181171",
      "type": "author",
      "name": "Mai Gim'enez"
    },
    {
      "id": "2275552322",
      "type": "author",
      "name": "Jiawei Xia"
    },
    {
      "id": "2770149",
      "type": "author",
      "name": "Olivier Dousse"
    },
    {
      "id": "145556052",
      "type": "author",
      "name": "W. Gierke"
    },
    {
      "id": "1735318",
      "type": "author",
      "name": "S. Yeganeh"
    },
    {
      "id": "2290486731",
      "type": "author",
      "name": "Damion Yates"
    },
    {
      "id": "153776147",
      "type": "author",
      "name": "Komal Jalan"
    },
    {
      "id": "2275716550",
      "type": "author",
      "name": "Lu Li"
    },
    {
      "id": "2275189350",
      "type": "author",
      "name": "Eri Latorre-Chimoto"
    },
    {
      "id": "2112293680",
      "type": "author",
      "name": "Duc Dung Nguyen"
    },
    {
      "id": "2275187515",
      "type": "author",
      "name": "Ken Durden"
    },
    {
      "id": "2561675",
      "type": "author",
      "name": "Praveen Kallakuri"
    },
    {
      "id": "2290524803",
      "type": "author",
      "name": "Yaxin Liu"
    },
    {
      "id": "2275221227",
      "type": "author",
      "name": "Matthew Johnson"
    },
    {
      "id": "2275175372",
      "type": "author",
      "name": "Tomy Tsai"
    },
    {
      "id": "2275188417",
      "type": "author",
      "name": "Alice Talbert"
    },
    {
      "id": "2275539011",
      "type": "author",
      "name": "Jasmine Liu"
    },
    {
      "id": "40390373",
      "type": "author",
      "name": "Alexander Neitz"
    },
    {
      "id": "2217508229",
      "type": "author",
      "name": "C. Elkind"
    },
    {
      "id": "2269473701",
      "type": "author",
      "name": "Marco Selvi"
    },
    {
      "id": "2275188903",
      "type": "author",
      "name": "Mimi Jasarevic"
    },
    {
      "id": "2258550407",
      "type": "author",
      "name": "Livio Baldini Soares"
    },
    {
      "id": "7353832",
      "type": "author",
      "name": "Livio Baldini Soares"
    },
    {
      "id": "2164862499",
      "type": "author",
      "name": "Pidong Wang"
    },
    {
      "id": "2290580195",
      "type": "author",
      "name": "Alek Wenjiao Wang"
    },
    {
      "id": "2181807096",
      "type": "author",
      "name": "Xinyu Ye"
    },
    {
      "id": "2214770531",
      "type": "author",
      "name": "Krystal Kallarackal"
    },
    {
      "id": "2275188993",
      "type": "author",
      "name": "Lucia Loher"
    },
    {
      "id": "2290486901",
      "type": "author",
      "name": "Hoi Lam"
    },
    {
      "id": "2290485721",
      "type": "author",
      "name": "Josef Broder"
    },
    {
      "id": "1404655176",
      "type": "author",
      "name": "D. Holtmann-Rice"
    },
    {
      "id": "2275150753",
      "type": "author",
      "name": "Nina Martin"
    },
    {
      "id": "2257926827",
      "type": "author",
      "name": "Bramandia Ramadhana"
    },
    {
      "id": "1393948967",
      "type": "author",
      "name": "Daniel Toyama"
    },
    {
      "id": "2290488378",
      "type": "author",
      "name": "Mrinal Shukla"
    },
    {
      "id": "2266467648",
      "type": "author",
      "name": "Sujoy Basu"
    },
    {
      "id": "2290784246",
      "type": "author",
      "name": "Abhi Mohan"
    },
    {
      "id": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "type": "paper",
      "title": "Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-initialization",
      "abstract": "The Mixture of Experts (MoE) architecture reduces the training and inference cost significantly compared to a dense model of equivalent capacity. Upcycling is an approach that initializes and trains an MoE model using a pre-trained dense model. While upcycling leads to initial performance gains, the training progresses slower than when trained from scratch, leading to suboptimal performance in the long term. We propose Drop-Upcycling - a method that effectively addresses this problem. Drop-Upcycling combines two seemingly contradictory approaches: utilizing the knowledge of pre-trained dense models while statistically re-initializing some parts of the weights. This approach strategically promotes expert specialization, significantly enhancing the MoE model's efficiency in knowledge acquisition. Extensive large-scale experiments demonstrate that Drop-Upcycling significantly outperforms previous MoE construction methods in the long term, specifically when training on hundreds of billions of tokens or more. As a result, our MoE model with 5.9B active parameters achieves comparable performance to a 13B dense model in the same model family, while requiring approximately 1/4 of the training FLOPs. All experimental resources, including source code, training data, model checkpoints and logs, are publicly available to promote reproducibility and future research on MoE.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "citation_count": 0,
      "reference_count": 48,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Drop-Upcycling combines two seemingly contradictory approaches: utilizing the knowledge of pre-trained dense models while statistically re-initializing some parts of the weights, significantly enhancing the MoE model's efficiency in knowledge acquisition.",
      "external_id_arxiv": "2502.19261",
      "external_id_corpusid": 276617861
    },
    {
      "id": "2294513555",
      "type": "author",
      "name": "Taishi Nakamura"
    },
    {
      "id": "2347347332",
      "type": "author",
      "name": "Takuya Akiba"
    },
    {
      "id": "2298889885",
      "type": "author",
      "name": "Kazuki Fujii"
    },
    {
      "id": "2266470694",
      "type": "author",
      "name": "Yusuke Oda"
    },
    {
      "id": "2294362068",
      "type": "author",
      "name": "Rio Yokota"
    },
    {
      "id": "2347348643",
      "type": "author",
      "name": "Jun Suzuki"
    },
    {
      "id": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "type": "paper",
      "title": "MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs and Deep Learning",
      "abstract": "In the competitive landscape of advertising, success hinges on effectively navigating and leveraging complex interactions among consumers, advertisers, and advertisement platforms. These multifaceted interactions compel advertisers to optimize strategies for modeling consumer behavior, enhancing brand recall, and tailoring advertisement content. To address these challenges, we present MindMem, a multimodal predictive model for advertisement memorability. By integrating textual, visual, and auditory data, MindMem achieves state-of-the-art performance, with a Spearman's correlation coefficient of 0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently surpassing existing methods. Furthermore, our analysis identified key factors influencing advertisement memorability, such as video pacing, scene complexity, and emotional resonance. Expanding on this, we introduced MindMem-ReAd (MindMem-Driven Re-generated Advertisement), which employs Large Language Model-based simulations to optimize advertisement content and placement, resulting in up to a 74.12% improvement in advertisement memorability. Our results highlight the transformative potential of Artificial Intelligence in advertising, offering advertisers a robust tool to drive engagement, enhance competitiveness, and maximize impact in a rapidly evolving market.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/90f20af3e16a825e0b976e968fd8e99a3366e438",
      "citation_count": 0,
      "reference_count": 20,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work presented MindMem, a multimodal predictive model for advertisement memorability, which employs Large Language Model-based simulations to optimize advertisement content and placement, resulting in up to a 74.12% improvement in advertisement memorability.",
      "external_id_arxiv": "2502.18371",
      "external_id_corpusid": 276580118
    },
    {
      "id": "2284386733",
      "type": "author",
      "name": "Sepehr Asgarian"
    },
    {
      "id": "118993525",
      "type": "author",
      "name": "Qayam Jetha"
    },
    {
      "id": "2286650249",
      "type": "author",
      "name": "Jouhyun Jeon"
    },
    {
      "id": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "paper",
      "title": "Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks",
      "abstract": "The explosion of high-performing conversational language models (LMs) has spurred a shift from classic natural language processing (NLP) benchmarks to expensive, time-consuming and noisy human evaluations - yet the relationship between these two evaluation strategies remains hazy. In this paper, we conduct a large-scale study of four Chat Llama 2 models, comparing their performance on 160 standard NLP benchmarks (e.g., MMLU, ARC, BIG-Bench Hard) against extensive human preferences on more than 11k single-turn and 2k multi-turn dialogues from over 2k human annotators. Our findings are striking: most NLP benchmarks strongly correlate with human evaluations, suggesting that cheaper, automated metrics can serve as surprisingly reliable predictors of human preferences. Three human evaluations, such as adversarial dishonesty and safety, are anticorrelated with NLP benchmarks, while two are uncorrelated. Moreover, through overparameterized linear regressions, we show that NLP scores can accurately predict human evaluations across different model scales, offering a path to reduce costly human annotation without sacrificing rigor. Overall, our results affirm the continued value of classic benchmarks and illuminate how to harness them to anticipate real-world user satisfaction - pointing to how NLP benchmarks can be leveraged to meet evaluation needs of our new era of conversational AI.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "citation_count": 0,
      "reference_count": 119,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A large-scale study of four Chat Llama 2 models, comparing their performance on 160 standard NLP benchmarks against extensive human preferences shows that NLP scores can accurately predict human evaluations across different model scales, offering a path to reduce costly human annotation without sacrificing rigor.",
      "external_id_arxiv": "2502.18339",
      "external_id_corpusid": 276580137
    },
    {
      "id": "1749176844",
      "type": "author",
      "name": "Rylan Schaeffer"
    },
    {
      "id": "2146367061",
      "type": "author",
      "name": "Punit Singh Koura"
    },
    {
      "id": "2237987675",
      "type": "author",
      "name": "Binh Tang"
    },
    {
      "id": "2293725986",
      "type": "author",
      "name": "R. Subramanian"
    },
    {
      "id": "2306863572",
      "type": "author",
      "name": "Aaditya K. Singh"
    },
    {
      "id": "39980906",
      "type": "author",
      "name": "Todor Mihaylov"
    },
    {
      "id": "51229603",
      "type": "author",
      "name": "Prajjwal Bhargava"
    },
    {
      "id": "151093281",
      "type": "author",
      "name": "Lovish Madaan"
    },
    {
      "id": "22193324",
      "type": "author",
      "name": "Niladri S. Chatterji"
    },
    {
      "id": "28554843",
      "type": "author",
      "name": "Vedanuj Goswami"
    },
    {
      "id": "2343771482",
      "type": "author",
      "name": "Sergey Edunov"
    },
    {
      "id": "3449411",
      "type": "author",
      "name": "Dieuwke Hupkes"
    },
    {
      "id": "123593472",
      "type": "author",
      "name": "Sanmi Koyejo"
    },
    {
      "id": "46617804",
      "type": "author",
      "name": "Sharan Narang"
    },
    {
      "id": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "type": "paper",
      "title": "Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction",
      "abstract": "Interpreting data is central to modern research. Large language models (LLMs) show promise in providing such natural language interpretations of data, yet simple feature extraction methods such as prompting often fail to produce accurate and versatile descriptions for diverse datasets and lack control over granularity and scale. To address these limitations, we propose a domain-agnostic method for dataset featurization that provides precise control over the number of features extracted while maintaining compact and descriptive representations comparable to human expert labeling. Our method optimizes the selection of informative binary features by evaluating the ability of an LLM to reconstruct the original data using those features. We demonstrate its effectiveness in dataset modeling tasks and through two case studies: (1) Constructing a feature representation of jailbreak tactics that compactly captures both the effectiveness and diversity of a larger set of human-crafted attacks; and (2) automating the discovery of features that align with human preferences, achieving accuracy and robustness comparable to expert-crafted features. Moreover, we show that the pipeline scales effectively, improving as additional features are sampled, making it suitable for large and diverse datasets.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "citation_count": 0,
      "reference_count": 77,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work proposes a domain-agnostic method for dataset featurization that provides precise control over the number of features extracted while maintaining compact and descriptive representations comparable to human expert labeling, and demonstrates its effectiveness in dataset modeling tasks.",
      "external_id_arxiv": "2502.17541",
      "external_id_corpusid": 276580735
    },
    {
      "id": "2347041888",
      "type": "author",
      "name": "Michal Bravansky"
    },
    {
      "id": "2347041748",
      "type": "author",
      "name": "Vaclav Kubon"
    },
    {
      "id": "2347043065",
      "type": "author",
      "name": "Suhas Hariharan"
    },
    {
      "id": "2347043097",
      "type": "author",
      "name": "Robert Kirk"
    },
    {
      "id": "a5e85f729fa022c77c2c4b838efc910df83a3a9d",
      "type": "paper",
      "title": "Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals",
      "abstract": "Retrieval-augmented generation (RAG) has shown impressive capabilities in mitigating hallucinations in large language models (LLMs). However, LLMs struggle to handle misleading retrievals and often fail to maintain their own reasoning when exposed to conflicting or selectively-framed evidence, making them vulnerable to real-world misinformation. In such real-world retrieval scenarios, misleading and conflicting information is rampant, particularly in the political domain, where evidence is often selectively framed, incomplete, or polarized. However, existing RAG benchmarks largely assume a clean retrieval setting, where models succeed by accurately retrieving and generating answers from gold-standard documents. This assumption fails to align with real-world conditions, leading to an overestimation of RAG system performance. To bridge this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate the robustness of RAG systems against misleading retrievals. Unlike prior benchmarks that rely on synthetic noise, our dataset constructs its retrieval corpus from Reddit discussions, capturing naturally occurring misinformation. It categorizes retrieved evidence into three types: supporting, misleading, and irrelevant, providing a realistic and challenging testbed for assessing how well RAG systems navigate different retrieval information. Our benchmark experiments reveal that when exposed to misleading retrievals, all tested LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no retrieval at all), highlighting their susceptibility to noisy environments. To the best of our knowledge, RAGuard is the first benchmark to systematically assess RAG robustness against misleading evidence. We expect this benchmark will drive future research toward improving RAG systems beyond idealized datasets, making them more reliable for real-world applications.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/a5e85f729fa022c77c2c4b838efc910df83a3a9d",
      "citation_count": 0,
      "reference_count": 51,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "RAGuard is the first benchmark to systematically assess RAG robustness against misleading evidence, and is expected to drive future research toward improving RAG systems beyond idealized datasets, making them more reliable for real-world applications.",
      "external_id_arxiv": "2502.16101",
      "external_id_corpusid": 276575912
    },
    {
      "id": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "type": "paper",
      "title": "Social Genome: Grounded Social Reasoning Abilities of Multimodal Models",
      "abstract": "Social reasoning abilities are crucial for AI systems to effectively interpret and respond to multimodal human communication and interaction within social contexts. We introduce Social Genome, the first benchmark for fine-grained, grounded social reasoning abilities of multimodal models. Social Genome contains 272 videos of interactions and 1,486 human-annotated reasoning traces related to inferences about these interactions. These traces contain 5,777 reasoning steps that reference evidence from visual cues, verbal cues, vocal cues, and external knowledge (contextual knowledge external to videos). Social Genome is also the first modeling challenge to study external knowledge in social reasoning. Social Genome computes metrics to holistically evaluate semantic and structural qualities of model-generated social reasoning traces. We demonstrate the utility of Social Genome through experiments with state-of-the-art models, identifying performance gaps and opportunities for future research to improve the grounded social reasoning abilities of multimodal models.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "citation_count": 0,
      "reference_count": 54,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Social Genome is introduced, the first benchmark for fine-grained, grounded social reasoning abilities of multimodal models and computes metrics to holistically evaluate semantic and structural qualities of model-generated social reasoning traces.",
      "external_id_arxiv": "2502.15109",
      "external_id_corpusid": 276558424
    },
    {
      "id": "2259929826",
      "type": "author",
      "name": "Leena Mathur"
    },
    {
      "id": "2347017366",
      "type": "author",
      "name": "Marian Qian"
    },
    {
      "id": "28130078",
      "type": "author",
      "name": "Paul Pu Liang"
    },
    {
      "id": "49933077",
      "type": "author",
      "name": "Louis-Philippe Morency"
    },
    {
      "id": "020c82cbe72629c014c430e198d08d020115c67d",
      "type": "paper",
      "title": "LongCaptioning: Unlocking the Power of Long Caption Generation in Large Multimodal Models",
      "abstract": "Large multimodal models (LMMs) have shown remarkable performance in video understanding tasks and can even process videos longer than one hour. However, despite their ability to handle long inputs, generating outputs with corresponding levels of richness remains a challenge. In this paper, we explore the issue of long outputs in LMMs using video captioning as a proxy task, and we find that open-source LMMs struggle to consistently generate outputs exceeding about 300 words. Through controlled experiments, we find that the scarcity of paired examples with long-captions during training is the primary factor limiting the model's output length. However, manually annotating long-caption examples is time-consuming and expensive. To address this, we propose the LongCaption-Agent, a framework that synthesizes long caption data by aggregating multi-level descriptions. Using LongCaption-Agent, we curated a new long-caption dataset, LongCaption-10K. We also develop LongCaption-Bench, a benchmark designed to comprehensively evaluate the quality of long captions generated by LMMs. By incorporating LongCaption-10K into training, we enable LMMs to generate captions exceeding 1,000 words, while maintaining high output quality. In LongCaption-Bench, our 8B parameter model achieved state-of-the-art performance, even surpassing larger proprietary models. We will release the dataset and code after publication.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/020c82cbe72629c014c430e198d08d020115c67d",
      "citation_count": 0,
      "reference_count": 50,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The LongCaption-Agent is proposed, a framework that synthesizes long caption data by aggregating multi-level descriptions and curated a new long-caption dataset, LongCaption-10K, which enables LMMs to generate captions exceeding 1,000 words, while maintaining high output quality.",
      "external_id_arxiv": "2502.15393",
      "external_id_corpusid": 276558067
    },
    {
      "id": "2228362755",
      "type": "author",
      "name": "Hongchen Wei"
    },
    {
      "id": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "type": "paper",
      "title": "Emulating Retrieval Augmented Generation via Prompt Engineering for Enhanced Long Context Comprehension in LLMs",
      "abstract": "This paper addresses the challenge of comprehending very long contexts in Large Language Models (LLMs) by proposing a method that emulates Retrieval Augmented Generation (RAG) through specialized prompt engineering and chain-of-thought (CoT) reasoning. While recent LLMs support over 100,000 tokens in a single prompt, simply enlarging context windows has not guaranteed robust multi-hop reasoning when key details are scattered across massive input. Our approach treats the model as both the retriever and the reasoner: it first tags relevant segments within a long passage, then employs a stepwise CoT workflow to integrate these pieces of evidence. This single-pass method thereby reduces reliance on an external retriever, yet maintains focus on crucial segments. We evaluate our approach on selected tasks from BABILong, which interleaves standard bAbI QA problems with large amounts of distractor text. Compared to baseline (no retrieval) and naive RAG pipelines, our approach more accurately handles multi-fact questions such as object location tracking, counting, and indefinite knowledge. Furthermore, we analyze how prompt structure, including the order of question, relevant-text tags, and overall instructions, significantly affects performance. These findings underscore that optimized prompt engineering, combined with guided reasoning, can enhance LLMs' long-context comprehension and serve as a lightweight alternative to traditional retrieval pipelines.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "citation_count": 0,
      "reference_count": 17,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "",
      "external_id_arxiv": "2502.12462",
      "external_id_corpusid": 276421566
    },
    {
      "id": "2346055707",
      "type": "author",
      "name": "Joon Park"
    },
    {
      "id": "10424769",
      "type": "author",
      "name": "Kyohei Atarashi"
    },
    {
      "id": "2243408877",
      "type": "author",
      "name": "Koh Takeuchi"
    },
    {
      "id": "2247886893",
      "type": "author",
      "name": "Hisashi Kashima"
    },
    {
      "id": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "type": "paper",
      "title": "LAMD: Context-driven Android Malware Detection and Classification with LLMs",
      "abstract": "The rapid growth of mobile applications has escalated Android malware threats. Although there are numerous detection methods, they often struggle with evolving attacks, dataset biases, and limited explainability. Large Language Models (LLMs) offer a promising alternative with their zero-shot inference and reasoning capabilities. However, applying LLMs to Android malware detection presents two key challenges: (1)the extensive support code in Android applications, often spanning thousands of classes, exceeds LLMs' context limits and obscures malicious behavior within benign functionality; (2)the structural complexity and interdependencies of Android applications surpass LLMs' sequence-based reasoning, fragmenting code analysis and hindering malicious intent inference. To address these challenges, we propose LAMD, a practical context-driven framework to enable LLM-based Android malware detection. LAMD integrates key context extraction to isolate security-critical code regions and construct program structures, then applies tier-wise code reasoning to analyze application behavior progressively, from low-level instructions to high-level semantics, providing final prediction and explanation. A well-designed factual consistency verification mechanism is equipped to mitigate LLM hallucinations from the first tier. Evaluation in real-world settings demonstrates LAMD's effectiveness over conventional detectors, establishing a feasible basis for LLM-driven malware analysis in dynamic threat landscapes.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "citation_count": 0,
      "reference_count": 58,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Evaluation in real-world settings demonstrates LAMD's effectiveness over conventional detectors, establishing a feasible basis for LLM-driven malware analysis in dynamic threat landscapes.",
      "external_id_arxiv": "2502.13055",
      "external_id_corpusid": 276421840
    },
    {
      "id": "2346517767",
      "type": "author",
      "name": "Xingzhi Qian"
    },
    {
      "id": "2220672771",
      "type": "author",
      "name": "Xinran Zheng"
    },
    {
      "id": "2346104589",
      "type": "author",
      "name": "Yiling He"
    },
    {
      "id": "2346055417",
      "type": "author",
      "name": "Shuo Yang"
    },
    {
      "id": "2344628799",
      "type": "author",
      "name": "Lorenzo Cavallaro"
    },
    {
      "id": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "type": "paper",
      "title": "Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities",
      "abstract": "This paper introduces Code-Vision, a benchmark designed to evaluate the logical understanding and code generation capabilities of Multimodal Large Language Models (MLLMs). It challenges MLLMs to generate a correct program that fulfills specific functionality requirements based on a given flowchart, which visually represents the desired algorithm or process. Code-Vision comprises three subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding abilities across basic programming, algorithmic, and mathematical problem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision. Experimental results demonstrate that there is a large performance difference between proprietary and open-source models. On Hard problems, GPT-4o can achieve 79.3% pass@1, but the best open-source model only achieves 15%. Further experiments reveal that Code-Vision can pose unique challenges compared to other multimodal reasoning benchmarks MMCode and MathVista. We also explore the reason for the poor performance of the open-source models. All data and codes are available at https://github.com/wanghanbinpanda/CodeVision.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "citation_count": 0,
      "reference_count": 32,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Experimental results demonstrate that there is a large performance difference between proprietary and open-source models, and reveal that Code-Vision can pose unique challenges compared to other multimodal reasoning benchmarks MMCode and MathVista.",
      "external_id_arxiv": "2502.11829",
      "external_id_corpusid": 276408465
    },
    {
      "id": "2329617551",
      "type": "author",
      "name": "Hanbin Wang"
    },
    {
      "id": "2345914083",
      "type": "author",
      "name": "Xiaoxuan Zhou"
    },
    {
      "id": "2346246767",
      "type": "author",
      "name": "Zhipeng Xu"
    },
    {
      "id": "2346578527",
      "type": "author",
      "name": "Keyuan Cheng"
    },
    {
      "id": "2345818907",
      "type": "author",
      "name": "Yuxin Zuo"
    },
    {
      "id": "2345818783",
      "type": "author",
      "name": "Kai Tian"
    },
    {
      "id": "2345873396",
      "type": "author",
      "name": "Jingwei Song"
    },
    {
      "id": "2346054898",
      "type": "author",
      "name": "Junting Lu"
    },
    {
      "id": "2346673644",
      "type": "author",
      "name": "Wenhui Hu"
    },
    {
      "id": "1699104018",
      "type": "author",
      "name": "Xueyang Liu"
    },
    {
      "id": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "paper",
      "title": "A Survey on Mixture of Experts",
      "abstract": "Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of LLMs (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts (MoE) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on MoE. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE. We first briefly introduce the structure of the MoE layer, followed by proposing a new taxonomy of MoE. Next, we overview the core designs for various MoE models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of MoE in practice, and outline some potential directions for future research. To facilitate ongoing updates and the sharing of cutting-edge developments in MoE research, we have established a resource repository accessible at https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "citation_count": 47,
      "reference_count": 216,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The structure of the MoE layer is introduced, followed by proposing a new taxonomy of MoE, and the core designs for various MoE models including both algorithmic and systemic aspects are overviewed, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations.",
      "external_id_arxiv": "2407.06204",
      "external_id_dblp": "journals/corr/abs-2407-06204",
      "external_id_doi": "10.48550/arXiv.2407.06204",
      "external_id_corpusid": 271064424
    },
    {
      "id": "2296001947",
      "type": "author",
      "name": "Weilin Cai"
    },
    {
      "id": "2294682530",
      "type": "author",
      "name": "Juyong Jiang"
    },
    {
      "id": "2304542351",
      "type": "author",
      "name": "Fan Wang"
    },
    {
      "id": "2310483288",
      "type": "author",
      "name": "Jing Tang"
    },
    {
      "id": "2257349580",
      "type": "author",
      "name": "Sunghun Kim"
    },
    {
      "id": "2295676687",
      "type": "author",
      "name": "Jiayi Huang"
    },
    {
      "id": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "paper",
      "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
      "abstract": "We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/07894aeadab9158fdb97647c4792816ede1b60b9",
      "citation_count": 43,
      "reference_count": 40,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously.",
      "external_id_arxiv": "2403.07816",
      "external_id_dblp": "journals/corr/abs-2403-07816",
      "external_id_doi": "10.48550/arXiv.2403.07816",
      "external_id_corpusid": 268363969
    },
    {
      "id": "2265067",
      "type": "author",
      "name": "Sainbayar Sukhbaatar"
    },
    {
      "id": "2290916129",
      "type": "author",
      "name": "Olga Golovneva"
    },
    {
      "id": "2237990986",
      "type": "author",
      "name": "Vasu Sharma"
    },
    {
      "id": "2298402817",
      "type": "author",
      "name": "Hu Xu"
    },
    {
      "id": "2255374957",
      "type": "author",
      "name": "Xi Victoria Lin"
    },
    {
      "id": "3361236",
      "type": "author",
      "name": "Baptiste Rozière"
    },
    {
      "id": "2253401183",
      "type": "author",
      "name": "Jacob Kahn"
    },
    {
      "id": "2530311",
      "type": "author",
      "name": "Shang-Wen Li"
    },
    {
      "id": "2072801764",
      "type": "author",
      "name": "Wen-tau Yih"
    },
    {
      "id": "2267341626",
      "type": "author",
      "name": "Jason Weston"
    },
    {
      "id": "2315344189",
      "type": "author",
      "name": "Xian Li"
    },
    {
      "id": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "paper",
      "title": "GPT-4 Technical Report",
      "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
      "year": 2023,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04",
      "citation_count": 10179,
      "reference_count": 0,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs, is developed, a Transformer-based model pre-trained to predict the next token in a document which exhibits human-level performance on various professional and academic benchmarks.",
      "external_id_arxiv": "2303.08774",
      "external_id_corpusid": 257532815
    },
    {
      "id": "2275249853",
      "type": "author",
      "name": "OpenAI Josh Achiam"
    },
    {
      "id": "2275250875",
      "type": "author",
      "name": "Steven Adler"
    },
    {
      "id": "144517868",
      "type": "author",
      "name": "Sandhini Agarwal"
    },
    {
      "id": "2274773568",
      "type": "author",
      "name": "Lama Ahmad"
    },
    {
      "id": "2258629",
      "type": "author",
      "name": "Ilge Akkaya"
    },
    {
      "id": "2275244794",
      "type": "author",
      "name": "Florencia Leoni Aleman"
    },
    {
      "id": "2275252021",
      "type": "author",
      "name": "Diogo Almeida"
    },
    {
      "id": "2275252424",
      "type": "author",
      "name": "Janko Altenschmidt"
    },
    {
      "id": "2275245579",
      "type": "author",
      "name": "Sam Altman"
    },
    {
      "id": "2275246437",
      "type": "author",
      "name": "Shyamal Anadkat"
    },
    {
      "id": "2275139370",
      "type": "author",
      "name": "Red Avila"
    },
    {
      "id": "2256699302",
      "type": "author",
      "name": "Igor Babuschkin"
    },
    {
      "id": "2054519183",
      "type": "author",
      "name": "S. Balaji"
    },
    {
      "id": "2275251659",
      "type": "author",
      "name": "Valerie Balcom"
    },
    {
      "id": "47626612",
      "type": "author",
      "name": "Paul Baltescu"
    },
    {
      "id": "2275198557",
      "type": "author",
      "name": "Haim-ing Bao"
    },
    {
      "id": "2275251620",
      "type": "author",
      "name": "Mo Bavarian"
    },
    {
      "id": "2275245092",
      "type": "author",
      "name": "J. Belgum"
    },
    {
      "id": "4689792",
      "type": "author",
      "name": "Irwan Bello"
    },
    {
      "id": "2275245414",
      "type": "author",
      "name": "Jake Berdine"
    },
    {
      "id": "2275245581",
      "type": "author",
      "name": "Gabriel Bernadett-Shapiro"
    },
    {
      "id": "133740015",
      "type": "author",
      "name": "Christopher Berner"
    },
    {
      "id": "2275251674",
      "type": "author",
      "name": "Lenny Bogdonoff"
    },
    {
      "id": "2275246071",
      "type": "author",
      "name": "Oleg Boiko"
    },
    {
      "id": "2275248137",
      "type": "author",
      "name": "Made-laine Boyd"
    },
    {
      "id": "2275245419",
      "type": "author",
      "name": "Anna-Luisa Brakman"
    },
    {
      "id": "2065151121",
      "type": "author",
      "name": "Greg Brockman"
    },
    {
      "id": "2275219628",
      "type": "author",
      "name": "Tim Brooks"
    },
    {
      "id": "35167962",
      "type": "author",
      "name": "Miles Brundage"
    },
    {
      "id": "2146257251",
      "type": "author",
      "name": "Kevin Button"
    },
    {
      "id": "2275157286",
      "type": "author",
      "name": "Trevor Cai"
    },
    {
      "id": "2274782053",
      "type": "author",
      "name": "Rosie Campbell"
    },
    {
      "id": "2275245404",
      "type": "author",
      "name": "Andrew Cann"
    },
    {
      "id": "2275246368",
      "type": "author",
      "name": "Brittany Carey"
    },
    {
      "id": "2275120298",
      "type": "author",
      "name": "Chelsea Carlson"
    },
    {
      "id": "144114446",
      "type": "author",
      "name": "Rory Carmichael"
    },
    {
      "id": "1466431052",
      "type": "author",
      "name": "Brooke Chan"
    },
    {
      "id": "2275545855",
      "type": "author",
      "name": "Che Chang"
    },
    {
      "id": "2057091285",
      "type": "author",
      "name": "Fotis Chantzis"
    },
    {
      "id": "2253841704",
      "type": "author",
      "name": "Derek Chen"
    },
    {
      "id": "2275188918",
      "type": "author",
      "name": "Sully Chen"
    },
    {
      "id": "2275179180",
      "type": "author",
      "name": "Ruby Chen"
    },
    {
      "id": "2275289833",
      "type": "author",
      "name": "Jason Chen"
    },
    {
      "id": "2108828435",
      "type": "author",
      "name": "Mark Chen"
    },
    {
      "id": "1490681878",
      "type": "author",
      "name": "B. Chess"
    },
    {
      "id": "2275251158",
      "type": "author",
      "name": "Chester Cho"
    },
    {
      "id": "2276186593",
      "type": "author",
      "name": "Casey Chu"
    },
    {
      "id": "2275839391",
      "type": "author",
      "name": "Hyung Won Chung"
    },
    {
      "id": "2275231534",
      "type": "author",
      "name": "Dave Cummings"
    },
    {
      "id": "49645091",
      "type": "author",
      "name": "Jeremiah Currier"
    },
    {
      "id": "2276187456",
      "type": "author",
      "name": "Yunxing Dai"
    },
    {
      "id": "2275251205",
      "type": "author",
      "name": "C. Decareaux"
    },
    {
      "id": "2275244920",
      "type": "author",
      "name": "Thomas Degry"
    },
    {
      "id": "2275247090",
      "type": "author",
      "name": "Noah Deutsch"
    },
    {
      "id": "2275251200",
      "type": "author",
      "name": "Damien Deville"
    },
    {
      "id": "2275244298",
      "type": "author",
      "name": "Arka Dhar"
    },
    {
      "id": "35363891",
      "type": "author",
      "name": "David Dohan"
    },
    {
      "id": "2275252295",
      "type": "author",
      "name": "Steve Dowling"
    },
    {
      "id": "2275245491",
      "type": "author",
      "name": "Sheila Dunning"
    },
    {
      "id": "66821245",
      "type": "author",
      "name": "Adrien Ecoffet"
    },
    {
      "id": "2275245457",
      "type": "author",
      "name": "Atty Eleti"
    },
    {
      "id": "2146257131",
      "type": "author",
      "name": "Tyna Eloundou"
    },
    {
      "id": "2065430571",
      "type": "author",
      "name": "David Farhi"
    },
    {
      "id": "2096916416",
      "type": "author",
      "name": "L. Fedus"
    },
    {
      "id": "2275249996",
      "type": "author",
      "name": "Niko Felix"
    },
    {
      "id": "2275245820",
      "type": "author",
      "name": "Sim'on Posada Fishman"
    },
    {
      "id": "2275244914",
      "type": "author",
      "name": "Juston Forte"
    },
    {
      "id": "2275251173",
      "type": "author",
      "name": "Is-abella Fulford"
    },
    {
      "id": "2027599537",
      "type": "author",
      "name": "Leo Gao"
    },
    {
      "id": "2275200811",
      "type": "author",
      "name": "Elie Georges"
    },
    {
      "id": "2275254804",
      "type": "author",
      "name": "C. Gibson"
    },
    {
      "id": "2275144649",
      "type": "author",
      "name": "Vik Goel"
    },
    {
      "id": "2325028819",
      "type": "author",
      "name": "Tarun Gogineni"
    },
    {
      "id": "2261041177",
      "type": "author",
      "name": "Gabriel Goh"
    },
    {
      "id": "2158366935",
      "type": "author",
      "name": "Raphael Gontijo-Lopes"
    },
    {
      "id": "2265066144",
      "type": "author",
      "name": "Jonathan Gordon"
    },
    {
      "id": "2275250003",
      "type": "author",
      "name": "Morgan Grafstein"
    },
    {
      "id": "145565184",
      "type": "author",
      "name": "Scott Gray"
    },
    {
      "id": "2275247307",
      "type": "author",
      "name": "Ryan Greene"
    },
    {
      "id": "2275137274",
      "type": "author",
      "name": "Joshua Gross"
    },
    {
      "id": "2253699903",
      "type": "author",
      "name": "S. Gu"
    },
    {
      "id": "2276101257",
      "type": "author",
      "name": "Yufei Guo"
    },
    {
      "id": "2004021329",
      "type": "author",
      "name": "Chris Hallacy"
    },
    {
      "id": "2275540338",
      "type": "author",
      "name": "Jesse Han"
    },
    {
      "id": "2275295848",
      "type": "author",
      "name": "Jeff Harris"
    },
    {
      "id": "2275226809",
      "type": "author",
      "name": "Yuchen He"
    },
    {
      "id": "2275245527",
      "type": "author",
      "name": "Mike Heaton"
    },
    {
      "id": "2151087994",
      "type": "author",
      "name": "Jo-hannes Heidecke"
    },
    {
      "id": "2242286342",
      "type": "author",
      "name": "Chris Hesse"
    },
    {
      "id": "2226452668",
      "type": "author",
      "name": "Alan Hickey"
    },
    {
      "id": "2275246148",
      "type": "author",
      "name": "Wade Hickey"
    },
    {
      "id": "2275245339",
      "type": "author",
      "name": "Peter Hoeschele"
    },
    {
      "id": "103681415",
      "type": "author",
      "name": "Brandon Houghton"
    },
    {
      "id": "2275214107",
      "type": "author",
      "name": "Kenny Hsu"
    },
    {
      "id": "2275210604",
      "type": "author",
      "name": "Shengli Hu"
    },
    {
      "id": "2275777049",
      "type": "author",
      "name": "Xin Hu"
    },
    {
      "id": "39378983",
      "type": "author",
      "name": "Joost Huizinga"
    },
    {
      "id": "2276187117",
      "type": "author",
      "name": "Shantanu Jain"
    },
    {
      "id": "2171110177",
      "type": "author",
      "name": "Shawn Jain"
    },
    {
      "id": "2151094350",
      "type": "author",
      "name": "Joanne Jang"
    },
    {
      "id": "2253471334",
      "type": "author",
      "name": "Angela Jiang"
    },
    {
      "id": "2275172062",
      "type": "author",
      "name": "Roger Jiang"
    },
    {
      "id": "2275752035",
      "type": "author",
      "name": "Haozhun Jin"
    },
    {
      "id": "2275203081",
      "type": "author",
      "name": "Denny Jin"
    },
    {
      "id": "2275250083",
      "type": "author",
      "name": "Shino Jomoto"
    },
    {
      "id": "2275247096",
      "type": "author",
      "name": "B. Jonn"
    },
    {
      "id": "35450887",
      "type": "author",
      "name": "Heewoo Jun"
    },
    {
      "id": "2403754",
      "type": "author",
      "name": "Tomer Kaftan"
    },
    {
      "id": "2275230678",
      "type": "author",
      "name": "Lukasz Kaiser"
    },
    {
      "id": "2275169038",
      "type": "author",
      "name": "Ali Kamali"
    },
    {
      "id": "3151440",
      "type": "author",
      "name": "I. Kanitscheider"
    },
    {
      "id": "2844898",
      "type": "author",
      "name": "N. Keskar"
    },
    {
      "id": "2152264064",
      "type": "author",
      "name": "Tabarak Khan"
    },
    {
      "id": "2275246102",
      "type": "author",
      "name": "Logan Kilpatrick"
    },
    {
      "id": "2260346092",
      "type": "author",
      "name": "Jong Wook Kim"
    },
    {
      "id": "2149054292",
      "type": "author",
      "name": "Christina Kim"
    },
    {
      "id": "2275296777",
      "type": "author",
      "name": "Yongjik Kim"
    },
    {
      "id": "2275112980",
      "type": "author",
      "name": "Hendrik Kirchner"
    },
    {
      "id": "51131802",
      "type": "author",
      "name": "J. Kiros"
    },
    {
      "id": "2146257375",
      "type": "author",
      "name": "Matthew Knight"
    },
    {
      "id": "1485556711",
      "type": "author",
      "name": "Daniel Kokotajlo"
    },
    {
      "id": "2275246094",
      "type": "author",
      "name": "Lukasz Kondraciuk"
    },
    {
      "id": "1666171360",
      "type": "author",
      "name": "A. Kondrich"
    },
    {
      "id": "2275252322",
      "type": "author",
      "name": "Aris Konstantinidis"
    },
    {
      "id": "2275245594",
      "type": "author",
      "name": "Kyle Kosic"
    },
    {
      "id": "2064404342",
      "type": "author",
      "name": "Gretchen Krueger"
    },
    {
      "id": "2275229877",
      "type": "author",
      "name": "Vishal Kuo"
    },
    {
      "id": "2275247085",
      "type": "author",
      "name": "Michael Lampe"
    },
    {
      "id": "2275246287",
      "type": "author",
      "name": "Ikai Lan"
    },
    {
      "id": "2274915115",
      "type": "author",
      "name": "Teddy Lee"
    },
    {
      "id": "2990741",
      "type": "author",
      "name": "J. Leike"
    },
    {
      "id": "52152632",
      "type": "author",
      "name": "Jade Leung"
    },
    {
      "id": "2275256930",
      "type": "author",
      "name": "Daniel Levy"
    },
    {
      "id": "2275285124",
      "type": "author",
      "name": "C. Li"
    },
    {
      "id": "2275176375",
      "type": "author",
      "name": "Rachel Lim"
    },
    {
      "id": "2275759230",
      "type": "author",
      "name": "Molly Lin"
    },
    {
      "id": "2253840098",
      "type": "author",
      "name": "Stephanie Lin"
    },
    {
      "id": "1380985420",
      "type": "author",
      "name": "Ma-teusz Litwin"
    },
    {
      "id": "2275248327",
      "type": "author",
      "name": "Theresa Lopez"
    },
    {
      "id": "2257272397",
      "type": "author",
      "name": "Ryan Lowe"
    },
    {
      "id": "2275245628",
      "type": "author",
      "name": "Patricia Lue"
    },
    {
      "id": "119341078",
      "type": "author",
      "name": "A. Makanju"
    },
    {
      "id": "2275245649",
      "type": "author",
      "name": "Kim Malfacini"
    },
    {
      "id": "46430291",
      "type": "author",
      "name": "Sam Manning"
    },
    {
      "id": "14113256",
      "type": "author",
      "name": "Todor Markov"
    },
    {
      "id": "2275245336",
      "type": "author",
      "name": "Yaniv Markovski"
    },
    {
      "id": "2114362965",
      "type": "author",
      "name": "Bianca Martin"
    },
    {
      "id": "2275231822",
      "type": "author",
      "name": "Katie Mayer"
    },
    {
      "id": "2275247045",
      "type": "author",
      "name": "Andrew Mayne"
    },
    {
      "id": "39593364",
      "type": "author",
      "name": "Bob McGrew"
    },
    {
      "id": "2047820455",
      "type": "author",
      "name": "S. McKinney"
    },
    {
      "id": "3028785",
      "type": "author",
      "name": "C. McLeavey"
    },
    {
      "id": "2274772421",
      "type": "author",
      "name": "Paul McMillan"
    },
    {
      "id": "2275234856",
      "type": "author",
      "name": "Jake McNeil"
    },
    {
      "id": "2275210659",
      "type": "author",
      "name": "David Medina"
    },
    {
      "id": "2275132306",
      "type": "author",
      "name": "Aalok Mehta"
    },
    {
      "id": "10698483",
      "type": "author",
      "name": "Jacob Menick"
    },
    {
      "id": "2275246330",
      "type": "author",
      "name": "Luke Metz"
    },
    {
      "id": "2275252694",
      "type": "author",
      "name": "Andrey Mishchenko"
    },
    {
      "id": "2051714782",
      "type": "author",
      "name": "Pamela Mishkin"
    },
    {
      "id": "2275245453",
      "type": "author",
      "name": "Vinnie Monaco"
    },
    {
      "id": "1404556973",
      "type": "author",
      "name": "Evan Morikawa"
    },
    {
      "id": "3407880",
      "type": "author",
      "name": "Daniel P. Mossing"
    },
    {
      "id": "2275154456",
      "type": "author",
      "name": "Tong Mu"
    },
    {
      "id": "2117715631",
      "type": "author",
      "name": "Mira Murati"
    },
    {
      "id": "147746767",
      "type": "author",
      "name": "O. Murk"
    },
    {
      "id": "2275246116",
      "type": "author",
      "name": "David M'ely"
    },
    {
      "id": "3422774",
      "type": "author",
      "name": "Ashvin Nair"
    },
    {
      "id": "7406311",
      "type": "author",
      "name": "Reiichiro Nakano"
    },
    {
      "id": "2057426488",
      "type": "author",
      "name": "Rajeev Nayak"
    },
    {
      "id": "2072676",
      "type": "author",
      "name": "Arvind Neelakantan"
    },
    {
      "id": "2273886618",
      "type": "author",
      "name": "Richard Ngo"
    },
    {
      "id": "2275115983",
      "type": "author",
      "name": "Hyeonwoo Noh"
    },
    {
      "id": "2228518120",
      "type": "author",
      "name": "Ouyang Long"
    },
    {
      "id": "1435765036",
      "type": "author",
      "name": "Cullen O'Keefe"
    },
    {
      "id": "2713380",
      "type": "author",
      "name": "J. Pachocki"
    },
    {
      "id": "34800652",
      "type": "author",
      "name": "Alex Paino"
    },
    {
      "id": "2275244652",
      "type": "author",
      "name": "Joe Palermo"
    },
    {
      "id": "2275246178",
      "type": "author",
      "name": "Ashley Pantuliano"
    },
    {
      "id": "50213542",
      "type": "author",
      "name": "Giambattista Parascandolo"
    },
    {
      "id": "2275245818",
      "type": "author",
      "name": "Joel Parish"
    },
    {
      "id": "2275245435",
      "type": "author",
      "name": "Emy Parparita"
    },
    {
      "id": "2274774915",
      "type": "author",
      "name": "Alexandre Passos"
    },
    {
      "id": "2068123790",
      "type": "author",
      "name": "Mikhail Pavlov"
    },
    {
      "id": "2275125663",
      "type": "author",
      "name": "Andrew Peng"
    },
    {
      "id": "2275245529",
      "type": "author",
      "name": "Adam Perelman"
    },
    {
      "id": "2275250075",
      "type": "author",
      "name": "Filipe de Avila Belbute Peres"
    },
    {
      "id": "2136008481",
      "type": "author",
      "name": "Michael Petrov"
    },
    {
      "id": "1463773776",
      "type": "author",
      "name": "Henrique Pondé de Oliveira Pinto"
    },
    {
      "id": "2275246346",
      "type": "author",
      "name": "Michael Pokorny"
    },
    {
      "id": "2275246814",
      "type": "author",
      "name": "Michelle Pokrass"
    },
    {
      "id": "144401061",
      "type": "author",
      "name": "Vitchyr H. Pong"
    },
    {
      "id": "2275150061",
      "type": "author",
      "name": "Tolly Powell"
    },
    {
      "id": "146162186",
      "type": "author",
      "name": "Alethea Power"
    },
    {
      "id": "2151088845",
      "type": "author",
      "name": "Boris Power"
    },
    {
      "id": "2275243930",
      "type": "author",
      "name": "Elizabeth Proehl"
    },
    {
      "id": "2285654208",
      "type": "author",
      "name": "Raul Puri"
    },
    {
      "id": "38909097",
      "type": "author",
      "name": "Alec Radford"
    },
    {
      "id": "2275178294",
      "type": "author",
      "name": "Jack W. Rae"
    },
    {
      "id": "2261024614",
      "type": "author",
      "name": "Aditya Ramesh"
    },
    {
      "id": "2275225165",
      "type": "author",
      "name": "Cameron Raymond"
    },
    {
      "id": "2275252438",
      "type": "author",
      "name": "Francis Real"
    },
    {
      "id": "2275252095",
      "type": "author",
      "name": "Kendra Rimbach"
    },
    {
      "id": "2275207240",
      "type": "author",
      "name": "Carl Ross"
    },
    {
      "id": "11150265",
      "type": "author",
      "name": "Bob Rotsted"
    },
    {
      "id": "2275250007",
      "type": "author",
      "name": "Henri Roussez"
    },
    {
      "id": "2260406867",
      "type": "author",
      "name": "Nick Ryder"
    },
    {
      "id": "47204843",
      "type": "author",
      "name": "M. Saltarelli"
    },
    {
      "id": "2275246803",
      "type": "author",
      "name": "Ted Sanders"
    },
    {
      "id": "2852106",
      "type": "author",
      "name": "Shibani Santurkar"
    },
    {
      "id": "144864359",
      "type": "author",
      "name": "Girish Sastry"
    },
    {
      "id": "2275265666",
      "type": "author",
      "name": "Heather Schmidt"
    },
    {
      "id": "2252874293",
      "type": "author",
      "name": "David Schnurr"
    },
    {
      "id": "47971768",
      "type": "author",
      "name": "John Schulman"
    },
    {
      "id": "2196579",
      "type": "author",
      "name": "Daniel Selsam"
    },
    {
      "id": "2275244711",
      "type": "author",
      "name": "Kyla Sheppard"
    },
    {
      "id": "102475503",
      "type": "author",
      "name": "Toki Sherbakov"
    },
    {
      "id": "2275246834",
      "type": "author",
      "name": "Jessica Shieh"
    },
    {
      "id": "118335789",
      "type": "author",
      "name": "Sarah Shoker"
    },
    {
      "id": "2700360",
      "type": "author",
      "name": "Szymon Sidor"
    },
    {
      "id": "2064673055",
      "type": "author",
      "name": "Eric Sigler"
    },
    {
      "id": "2151735251",
      "type": "author",
      "name": "Maddie Simens"
    },
    {
      "id": "2275252299",
      "type": "author",
      "name": "Jordan Sitkin"
    },
    {
      "id": "2117680841",
      "type": "author",
      "name": "Katarina Slama"
    },
    {
      "id": "103422608",
      "type": "author",
      "name": "Ian Sohl"
    },
    {
      "id": "2901424",
      "type": "author",
      "name": "Benjamin Sokolowsky"
    },
    {
      "id": "2307592658",
      "type": "author",
      "name": "Yang Song"
    },
    {
      "id": "2275245668",
      "type": "author",
      "name": "Natalie Staudacher"
    },
    {
      "id": "9927844",
      "type": "author",
      "name": "F. Such"
    },
    {
      "id": "2275252251",
      "type": "author",
      "name": "Natalie Summers"
    },
    {
      "id": "1701686",
      "type": "author",
      "name": "I. Sutskever"
    },
    {
      "id": "2275750817",
      "type": "author",
      "name": "Jie Tang"
    },
    {
      "id": "145950540",
      "type": "author",
      "name": "N. Tezak"
    },
    {
      "id": "2151289331",
      "type": "author",
      "name": "Madeleine Thompson"
    },
    {
      "id": "2275252092",
      "type": "author",
      "name": "Phil Tillet"
    },
    {
      "id": "2267339677",
      "type": "author",
      "name": "Amin Tootoonchian"
    },
    {
      "id": "2275249879",
      "type": "author",
      "name": "Elizabeth Tseng"
    },
    {
      "id": "2275249709",
      "type": "author",
      "name": "Preston Tuggle"
    },
    {
      "id": "2275244171",
      "type": "author",
      "name": "Nick Turley"
    },
    {
      "id": "2065005836",
      "type": "author",
      "name": "Jerry Tworek"
    },
    {
      "id": "2275203310",
      "type": "author",
      "name": "Juan Felipe Cer'on Uribe"
    },
    {
      "id": "2275244586",
      "type": "author",
      "name": "Andrea Vallone"
    },
    {
      "id": "2275245661",
      "type": "author",
      "name": "Arun Vijayvergiya"
    },
    {
      "id": "153387869",
      "type": "author",
      "name": "Chelsea Voss"
    },
    {
      "id": "2275245962",
      "type": "author",
      "name": "Carroll L. Wainwright"
    },
    {
      "id": "2275528432",
      "type": "author",
      "name": "Justin Jay Wang"
    },
    {
      "id": "2275540420",
      "type": "author",
      "name": "Alvin Wang"
    },
    {
      "id": "2275189326",
      "type": "author",
      "name": "Ben Wang"
    },
    {
      "id": "2170081200",
      "type": "author",
      "name": "Jonathan Ward"
    },
    {
      "id": "2253952872",
      "type": "author",
      "name": "Jason Wei"
    },
    {
      "id": "2275244218",
      "type": "author",
      "name": "CJ Weinmann"
    },
    {
      "id": "2275245663",
      "type": "author",
      "name": "Akila Welihinda"
    },
    {
      "id": "2930640",
      "type": "author",
      "name": "P. Welinder"
    },
    {
      "id": "2275139180",
      "type": "author",
      "name": "Jiayi Weng"
    },
    {
      "id": "2065741038",
      "type": "author",
      "name": "Lilian Weng"
    },
    {
      "id": "2275249733",
      "type": "author",
      "name": "Dave Willner"
    },
    {
      "id": "2059411355",
      "type": "author",
      "name": "Clemens Winter"
    },
    {
      "id": "2275244177",
      "type": "author",
      "name": "Samuel Wolrich"
    },
    {
      "id": "2275225207",
      "type": "author",
      "name": "Hannah Wong"
    },
    {
      "id": "2275245771",
      "type": "author",
      "name": "Lauren Workman"
    },
    {
      "id": "2275299848",
      "type": "author",
      "name": "Sherwin Wu"
    },
    {
      "id": "2274911253",
      "type": "author",
      "name": "Jeff Wu"
    },
    {
      "id": "2307456650",
      "type": "author",
      "name": "Michael Wu"
    },
    {
      "id": "2275190169",
      "type": "author",
      "name": "Kai Xiao"
    },
    {
      "id": "2275452480",
      "type": "author",
      "name": "Tao Xu"
    },
    {
      "id": "2275310096",
      "type": "author",
      "name": "Sarah Yoo"
    },
    {
      "id": "2275593618",
      "type": "author",
      "name": "Kevin Yu"
    },
    {
      "id": "2275194186",
      "type": "author",
      "name": "Qim-ing Yuan"
    },
    {
      "id": "2563432",
      "type": "author",
      "name": "Wojciech Zaremba"
    },
    {
      "id": "49629836",
      "type": "author",
      "name": "Rowan Zellers"
    },
    {
      "id": "2262080679",
      "type": "author",
      "name": "Chong Zhang"
    },
    {
      "id": "2275288889",
      "type": "author",
      "name": "Marvin Zhang"
    },
    {
      "id": "2275545682",
      "type": "author",
      "name": "Shengjia Zhao"
    },
    {
      "id": "2275257857",
      "type": "author",
      "name": "Tianhao Zheng"
    },
    {
      "id": "2275201537",
      "type": "author",
      "name": "Juntang Zhuang"
    },
    {
      "id": "2275245715",
      "type": "author",
      "name": "William Zhuk"
    },
    {
      "id": "2368067",
      "type": "author",
      "name": "Barret Zoph"
    },
    {
      "id": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "paper",
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
      "year": 2023,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/57e849d0de13ed5f91d086936296721d4ff75a75",
      "citation_count": 10283,
      "reference_count": 80,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "LLaMA, a collection of foundation language models ranging from 7B to 65B parameters, is introduced and it is shown that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets.",
      "external_id_dblp": "journals/corr/abs-2302-13971",
      "external_id_arxiv": "2302.13971",
      "external_id_corpusid": 257219404
    },
    {
      "id": "2113243762",
      "type": "author",
      "name": "Hugo Touvron"
    },
    {
      "id": "46183616",
      "type": "author",
      "name": "Thibaut Lavril"
    },
    {
      "id": "1410231361",
      "type": "author",
      "name": "Gautier Izacard"
    },
    {
      "id": "1490887583",
      "type": "author",
      "name": "Xavier Martinet"
    },
    {
      "id": "114952298",
      "type": "author",
      "name": "M. Lachaux"
    },
    {
      "id": "47733973",
      "type": "author",
      "name": "Timothée Lacroix"
    },
    {
      "id": "39589154",
      "type": "author",
      "name": "Naman Goyal"
    },
    {
      "id": "2072738644",
      "type": "author",
      "name": "Eric Hambro"
    },
    {
      "id": "2209986197",
      "type": "author",
      "name": "Faisal Azhar"
    },
    {
      "id": "2166043087",
      "type": "author",
      "name": "Aurélien Rodriguez"
    },
    {
      "id": "2319608",
      "type": "author",
      "name": "Armand Joulin"
    },
    {
      "id": "3024698",
      "type": "author",
      "name": "Edouard Grave"
    },
    {
      "id": "1830914",
      "type": "author",
      "name": "Guillaume Lample"
    },
    {
      "id": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "paper",
      "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints",
      "abstract": "Training large, deep neural networks to convergence can be prohibitively expensive. As a result, often only a small selection of popular, dense models are reused across different contexts and tasks. Increasingly, sparsely activated models, which seek to decouple model size from computation costs, are becoming an attractive alternative to dense models. Although more efficient in terms of quality and computation cost, sparse models remain data-hungry and costly to train from scratch in the large scale regime. In this work, we propose sparse upcycling -- a simple way to reuse sunk training costs by initializing a sparsely activated Mixture-of-Experts model from a dense checkpoint. We show that sparsely upcycled T5 Base, Large, and XL language models and Vision Transformer Base and Large models, respectively, significantly outperform their dense counterparts on SuperGLUE and ImageNet, using only ~50% of the initial dense pretraining sunk cost. The upcycled models also outperform sparse models trained from scratch on 100% of the initial dense pretraining computation budget.",
      "year": 2022,
      "venue": "International Conference on Learning Representations",
      "url": "https://www.semanticscholar.org/paper/397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "citation_count": 91,
      "reference_count": 58,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "This work proposes sparse upcycling -- a simple way to reuse sunk training costs by initializing a sparsely activated Mixture-of-Experts model from a dense checkpoint, and shows that sparsely upcycled T5 Base, Large, and XL language models and Vision Transformer Base and Large models, respectively, significantly outperform their dense counterparts on SuperGLUE and ImageNet.",
      "open_access_pdf_url": "http://arxiv.org/pdf/2212.05055",
      "open_access_status": "GREEN",
      "external_id_dblp": "journals/corr/abs-2212-05055",
      "external_id_arxiv": "2212.05055",
      "external_id_doi": "10.48550/arXiv.2212.05055",
      "external_id_corpusid": 254535822
    },
    {
      "id": "51891020",
      "type": "author",
      "name": "Aran Komatsuzaki"
    },
    {
      "id": "1794202",
      "type": "author",
      "name": "J. Puigcerver"
    },
    {
      "id": "1405626394",
      "type": "author",
      "name": "J. Lee-Thorp"
    },
    {
      "id": "2135570886",
      "type": "author",
      "name": "Carlos Riquelme Ruiz"
    },
    {
      "id": "1643737606",
      "type": "author",
      "name": "J. Ainslie"
    },
    {
      "id": "97947517",
      "type": "author",
      "name": "Yi Tay"
    },
    {
      "id": "3226635",
      "type": "author",
      "name": "Mostafa Dehghani"
    },
    {
      "id": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "type": "paper",
      "title": "GLU Variants Improve Transformer",
      "abstract": "Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.",
      "year": 2020,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "citation_count": 757,
      "reference_count": 12,
      "fields_of_study": "[\"Computer Science\", \"Mathematics\"]",
      "is_open_access": false,
      "tldr": "Gated Linear Units (GLU) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function, and it is found that some of them yield quality improvements over the typically-used ReLU or GELU activations.",
      "external_id_mag": "3006439205",
      "external_id_dblp": "journals/corr/abs-2002-05202",
      "external_id_arxiv": "2002.05202",
      "external_id_corpusid": 211096588
    },
    {
      "id": "1846258",
      "type": "author",
      "name": "Noam M. Shazeer"
    },
    {
      "id": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "paper",
      "title": "Upcycling Large Language Models into Mixture of Experts",
      "abstract": "Upcycling pre-trained dense language models into sparse mixture-of-experts (MoE) models is an efficient approach to increase the model capacity of already trained models. However, optimal techniques for upcycling at scale remain unclear. In this work, we conduct an extensive study of upcycling methods and hyperparameters for billion-parameter scale language models. We propose a novel\"virtual group\"initialization scheme and weight scaling approach to enable upcycling into fine-grained MoE architectures. Through ablations, we find that upcycling outperforms continued dense model training. In addition, we show that softmax-then-topK expert routing improves over topK-then-softmax approach and higher granularity MoEs can help improve accuracy. Finally, we upcycled Nemotron-4 15B on 1T tokens and compared it to a continuously trained version of the same model on the same 1T tokens: the continuous trained model achieved 65.3% MMLU, whereas the upcycled model achieved 67.6%. Our results offer insights and best practices to effectively leverage upcycling for building MoE language models.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "citation_count": 4,
      "reference_count": 41,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "An extensive study of upcycling methods and hyperparameters for billion-parameter scale language models is conducted and a novel\"virtual group\"initialization scheme and weight scaling approach to enable upcycling into fine-grained MoE architectures is proposed.",
      "external_id_dblp": "journals/corr/abs-2410-07524",
      "external_id_arxiv": "2410.07524",
      "external_id_doi": "10.48550/arXiv.2410.07524",
      "external_id_corpusid": 273233217
    },
    {
      "id": "2325149690",
      "type": "author",
      "name": "Ethan He"
    },
    {
      "id": "31452163",
      "type": "author",
      "name": "Abhinav Khattar"
    },
    {
      "id": "3283879",
      "type": "author",
      "name": "R. Prenger"
    },
    {
      "id": "3111334",
      "type": "author",
      "name": "V. Korthikanti"
    },
    {
      "id": "2325199465",
      "type": "author",
      "name": "Zijie Yan"
    },
    {
      "id": "2325200012",
      "type": "author",
      "name": "Tong Liu"
    },
    {
      "id": "2325206494",
      "type": "author",
      "name": "Shiqing Fan"
    },
    {
      "id": "1491319211",
      "type": "author",
      "name": "Ashwath Aithal"
    },
    {
      "id": "1911755",
      "type": "author",
      "name": "M. Shoeybi"
    },
    {
      "id": "2264406909",
      "type": "author",
      "name": "Bryan Catanzaro"
    },
    {
      "id": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "paper",
      "title": "OLMoE: Open Mixture-of-Experts Language Models",
      "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "citation_count": 28,
      "reference_count": 0,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work introduces OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE), and presents various experiments on MoE training, analyzes routing in the model showing high specialization, and open-source all aspects of the work: model weights, training data, code, and logs.",
      "external_id_arxiv": "2409.02060",
      "external_id_dblp": "journals/corr/abs-2409-02060",
      "external_id_doi": "10.48550/arXiv.2409.02060",
      "external_id_corpusid": 272366674
    },
    {
      "id": "2037383772",
      "type": "author",
      "name": "Niklas Muennighoff"
    },
    {
      "id": "2280666145",
      "type": "author",
      "name": "Luca Soldaini"
    },
    {
      "id": "3458736",
      "type": "author",
      "name": "Dirk Groeneveld"
    },
    {
      "id": "2315302377",
      "type": "author",
      "name": "Kyle Lo"
    },
    {
      "id": "2146964035",
      "type": "author",
      "name": "Jacob Daniel Morrison"
    },
    {
      "id": "48872685",
      "type": "author",
      "name": "Sewon Min"
    },
    {
      "id": "2257597409",
      "type": "author",
      "name": "Weijia Shi"
    },
    {
      "id": "2158819969",
      "type": "author",
      "name": "Pete Walsh"
    },
    {
      "id": "3385516",
      "type": "author",
      "name": "Oyvind Tafjord"
    },
    {
      "id": "2052363815",
      "type": "author",
      "name": "Nathan Lambert"
    },
    {
      "id": "2261456046",
      "type": "author",
      "name": "Yuling Gu"
    },
    {
      "id": "2259924223",
      "type": "author",
      "name": "Shane Arora"
    },
    {
      "id": "2166136235",
      "type": "author",
      "name": "Akshita Bhagia"
    },
    {
      "id": "2264248042",
      "type": "author",
      "name": "Dustin Schwenk"
    },
    {
      "id": "30051202",
      "type": "author",
      "name": "David Wadden"
    },
    {
      "id": "2127066887",
      "type": "author",
      "name": "Alexander Wettig"
    },
    {
      "id": "2257002316",
      "type": "author",
      "name": "Binyuan Hui"
    },
    {
      "id": "2288469507",
      "type": "author",
      "name": "Tim Dettmers"
    },
    {
      "id": "2111313627",
      "type": "author",
      "name": "Douwe Kiela"
    },
    {
      "id": "2257020375",
      "type": "author",
      "name": "Ali Farhadi"
    },
    {
      "id": "2290483619",
      "type": "author",
      "name": "Noah A. Smith"
    },
    {
      "id": "2303396379",
      "type": "author",
      "name": "Pang Wei Koh"
    },
    {
      "id": "2284268811",
      "type": "author",
      "name": "Amanpreet Singh"
    },
    {
      "id": "2264251662",
      "type": "author",
      "name": "Hanna Hajishirzi"
    },
    {
      "id": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "paper",
      "title": "Qwen2 Technical Report",
      "abstract": "This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "citation_count": 521,
      "reference_count": 77,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model, which surpasses most prior open-weight models and demonstrates robust multilingual capabilities.",
      "external_id_dblp": "journals/corr/abs-2407-10671",
      "external_id_arxiv": "2407.10671",
      "external_id_doi": "10.48550/arXiv.2407.10671",
      "external_id_corpusid": 271212307
    },
    {
      "id": "2311633047",
      "type": "author",
      "name": "An Yang"
    },
    {
      "id": "2257101724",
      "type": "author",
      "name": "Baosong Yang"
    },
    {
      "id": "151471590",
      "type": "author",
      "name": "Binyuan Hui"
    },
    {
      "id": "2312091718",
      "type": "author",
      "name": "Bo Zheng"
    },
    {
      "id": "2249451832",
      "type": "author",
      "name": "Bowen Yu"
    },
    {
      "id": "2302295311",
      "type": "author",
      "name": "Chang Zhou"
    },
    {
      "id": "2257039734",
      "type": "author",
      "name": "Chengpeng Li"
    },
    {
      "id": "2311714296",
      "type": "author",
      "name": "Chengyuan Li"
    },
    {
      "id": "2248487202",
      "type": "author",
      "name": "Dayiheng Liu"
    },
    {
      "id": "2304136322",
      "type": "author",
      "name": "Fei Huang"
    },
    {
      "id": "51490462",
      "type": "author",
      "name": "Guanting Dong"
    },
    {
      "id": "2312183452",
      "type": "author",
      "name": "Haoran Wei"
    },
    {
      "id": "2314068968",
      "type": "author",
      "name": "Huan Lin"
    },
    {
      "id": "2299550823",
      "type": "author",
      "name": "Jialong Tang"
    },
    {
      "id": "2182966132",
      "type": "author",
      "name": "Jialin Wang"
    },
    {
      "id": "2243424858",
      "type": "author",
      "name": "Jian Yang"
    },
    {
      "id": "49365463",
      "type": "author",
      "name": "Jianhong Tu"
    },
    {
      "id": "2258670763",
      "type": "author",
      "name": "Jianwei Zhang"
    },
    {
      "id": "47793076",
      "type": "author",
      "name": "Jianxin Ma"
    },
    {
      "id": "2257108556",
      "type": "author",
      "name": "Jin Xu"
    },
    {
      "id": "2237981776",
      "type": "author",
      "name": "Jingren Zhou"
    },
    {
      "id": "41211611",
      "type": "author",
      "name": "Jinze Bai"
    },
    {
      "id": "2312070548",
      "type": "author",
      "name": "Jinzheng He"
    },
    {
      "id": "35996608",
      "type": "author",
      "name": "Junyang Lin"
    },
    {
      "id": "2247877609",
      "type": "author",
      "name": "Kai Dang"
    },
    {
      "id": "2257001403",
      "type": "author",
      "name": "Keming Lu"
    },
    {
      "id": "2293321194",
      "type": "author",
      "name": "Ke-Yang Chen"
    },
    {
      "id": "2303430522",
      "type": "author",
      "name": "Kexin Yang"
    },
    {
      "id": "2223106060",
      "type": "author",
      "name": "Mei Li"
    },
    {
      "id": "2301649171",
      "type": "author",
      "name": "Min Xue"
    },
    {
      "id": "2311440332",
      "type": "author",
      "name": "Na Ni"
    },
    {
      "id": "2288896179",
      "type": "author",
      "name": "Pei Zhang"
    },
    {
      "id": "2298005769",
      "type": "author",
      "name": "Peng Wang"
    },
    {
      "id": "2310233042",
      "type": "author",
      "name": "Ru Peng"
    },
    {
      "id": "47447639",
      "type": "author",
      "name": "Rui Men"
    },
    {
      "id": "2311453186",
      "type": "author",
      "name": "Ruize Gao"
    },
    {
      "id": "2248039532",
      "type": "author",
      "name": "Runji Lin"
    },
    {
      "id": "2311456728",
      "type": "author",
      "name": "Shijie Wang"
    },
    {
      "id": "2247821453",
      "type": "author",
      "name": "Shuai Bai"
    },
    {
      "id": "2110171536",
      "type": "author",
      "name": "Sinan Tan"
    },
    {
      "id": "2248127832",
      "type": "author",
      "name": "Tianhang Zhu"
    },
    {
      "id": "2309297259",
      "type": "author",
      "name": "Tianhao Li"
    },
    {
      "id": "2311842337",
      "type": "author",
      "name": "Tianyu Liu"
    },
    {
      "id": "2311391178",
      "type": "author",
      "name": "Wenbin Ge"
    },
    {
      "id": "2249717428",
      "type": "author",
      "name": "Xiaodong Deng"
    },
    {
      "id": "2141874108",
      "type": "author",
      "name": "Xiaohuan Zhou"
    },
    {
      "id": "2274095735",
      "type": "author",
      "name": "Xingzhang Ren"
    },
    {
      "id": "2294009097",
      "type": "author",
      "name": "Xinyu Zhang"
    },
    {
      "id": "2312041712",
      "type": "author",
      "name": "Xipin Wei"
    },
    {
      "id": "2312110505",
      "type": "author",
      "name": "Xuancheng Ren"
    },
    {
      "id": "2234000716",
      "type": "author",
      "name": "Yang Fan"
    },
    {
      "id": "2153951714",
      "type": "author",
      "name": "Yang Yao"
    },
    {
      "id": "29343468",
      "type": "author",
      "name": "Yichang Zhang"
    },
    {
      "id": "2244411465",
      "type": "author",
      "name": "Yunyang Wan"
    },
    {
      "id": "2256981628",
      "type": "author",
      "name": "Yunfei Chu"
    },
    {
      "id": "2248072386",
      "type": "author",
      "name": "Zeyu Cui"
    },
    {
      "id": "2116702333",
      "type": "author",
      "name": "Zhenru Zhang"
    },
    {
      "id": "2232106310",
      "type": "author",
      "name": "Zhi-Wei Fan"
    },
    {
      "id": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "paper",
      "title": "The Llama 3 Herd of Models",
      "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/40e8af970329135ec95057d73e239dab805ad128",
      "citation_count": 2350,
      "reference_count": 0,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "It is found that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks, and performs competitively with the state-of-the-art on image, video, and speech recognition tasks.",
      "external_id_arxiv": "2407.21783",
      "external_id_dblp": "journals/corr/abs-2407-21783",
      "external_id_doi": "10.48550/arXiv.2407.21783",
      "external_id_corpusid": 271571434
    },
    {
      "id": "2479521",
      "type": "author",
      "name": "Abhimanyu Dubey"
    },
    {
      "id": "2369482",
      "type": "author",
      "name": "Abhinav Jauhri"
    },
    {
      "id": "2299944289",
      "type": "author",
      "name": "Abhinav Pandey"
    },
    {
      "id": "89942851",
      "type": "author",
      "name": "Abhishek Kadian"
    },
    {
      "id": "2313916217",
      "type": "author",
      "name": "Ahmad Al-Dahle"
    },
    {
      "id": "2313924937",
      "type": "author",
      "name": "Aiesha Letman"
    },
    {
      "id": "2313975817",
      "type": "author",
      "name": "Akhil Mathur"
    },
    {
      "id": "14279694",
      "type": "author",
      "name": "Alan Schelten"
    },
    {
      "id": "2329138320",
      "type": "author",
      "name": "Amy Yang"
    },
    {
      "id": "2247818297",
      "type": "author",
      "name": "Angela Fan"
    },
    {
      "id": "2313918197",
      "type": "author",
      "name": "Anirudh Goyal"
    },
    {
      "id": "2129047988",
      "type": "author",
      "name": "Anthony S. Hartshorn"
    },
    {
      "id": "2269467670",
      "type": "author",
      "name": "Aobo Yang"
    },
    {
      "id": "2313926281",
      "type": "author",
      "name": "Archi Mitra"
    },
    {
      "id": "2313918952",
      "type": "author",
      "name": "Archie Sravankumar"
    },
    {
      "id": "2294453195",
      "type": "author",
      "name": "Artem Korenev"
    },
    {
      "id": "2279336258",
      "type": "author",
      "name": "Arthur Hinsvark"
    },
    {
      "id": "2314521400",
      "type": "author",
      "name": "Arun Rao"
    },
    {
      "id": "2313922587",
      "type": "author",
      "name": "Aston Zhang"
    },
    {
      "id": "2313910288",
      "type": "author",
      "name": "Austen Gregerson"
    },
    {
      "id": "2295667288",
      "type": "author",
      "name": "Ava Spataru"
    },
    {
      "id": "2313953240",
      "type": "author",
      "name": "Bap-tiste Roziere"
    },
    {
      "id": "2313916233",
      "type": "author",
      "name": "Bethany Biron"
    },
    {
      "id": "2079950350",
      "type": "author",
      "name": "Bobbie Chern"
    },
    {
      "id": "83928755",
      "type": "author",
      "name": "C. Caucheteux"
    },
    {
      "id": "2313917653",
      "type": "author",
      "name": "Chaya Nayak"
    },
    {
      "id": "2313909658",
      "type": "author",
      "name": "Chloe Bi"
    },
    {
      "id": "2313913576",
      "type": "author",
      "name": "Chris Marra"
    },
    {
      "id": "2217959550",
      "type": "author",
      "name": "Chris McConnell"
    },
    {
      "id": "2313909741",
      "type": "author",
      "name": "Christian Keller"
    },
    {
      "id": "103277778",
      "type": "author",
      "name": "Christophe Touret"
    },
    {
      "id": "2268428822",
      "type": "author",
      "name": "Chunyang Wu"
    },
    {
      "id": "2273700455",
      "type": "author",
      "name": "Corinne Wong"
    },
    {
      "id": "66286536",
      "type": "author",
      "name": "Cristian Cantón Ferrer"
    },
    {
      "id": "2273414632",
      "type": "author",
      "name": "Cyrus Nikolaidis"
    },
    {
      "id": "51882206",
      "type": "author",
      "name": "Damien Allonsius"
    },
    {
      "id": "2273006690",
      "type": "author",
      "name": "Daniel Song"
    },
    {
      "id": "2313909437",
      "type": "author",
      "name": "Danielle Pintz"
    },
    {
      "id": "2313918299",
      "type": "author",
      "name": "Danny Livshits"
    },
    {
      "id": "71039937",
      "type": "author",
      "name": "David Esiobu"
    },
    {
      "id": "2303390957",
      "type": "author",
      "name": "Dhruv Choudhary"
    },
    {
      "id": "2267338678",
      "type": "author",
      "name": "Dhruv Mahajan"
    },
    {
      "id": "2269456985",
      "type": "author",
      "name": "Diego Garcia-Olano"
    },
    {
      "id": "2306842160",
      "type": "author",
      "name": "Diego Perino"
    },
    {
      "id": "2343773325",
      "type": "author",
      "name": "Egor Lakomkin"
    },
    {
      "id": "1394834533",
      "type": "author",
      "name": "Ehab A. AlBadawy"
    },
    {
      "id": "2313918680",
      "type": "author",
      "name": "Elina Lobanova"
    },
    {
      "id": "31461304",
      "type": "author",
      "name": "Emily Dinan"
    },
    {
      "id": "2268821751",
      "type": "author",
      "name": "Eric Michael Smith"
    },
    {
      "id": "2708577",
      "type": "author",
      "name": "Filip Radenovic"
    },
    {
      "id": "2313967211",
      "type": "author",
      "name": "Frank Zhang"
    },
    {
      "id": "2282469774",
      "type": "author",
      "name": "Gabriele Synnaeve"
    },
    {
      "id": "2314074302",
      "type": "author",
      "name": "Gabrielle Lee"
    },
    {
      "id": "2313919767",
      "type": "author",
      "name": "Georgia Lewis Anderson"
    },
    {
      "id": "2268397654",
      "type": "author",
      "name": "Graeme Nail"
    },
    {
      "id": "51888120",
      "type": "author",
      "name": "Grégoire Mialon"
    },
    {
      "id": "2264339927",
      "type": "author",
      "name": "Guanglong Pang"
    },
    {
      "id": "2313924900",
      "type": "author",
      "name": "Guillem Cucurell"
    },
    {
      "id": "2314075528",
      "type": "author",
      "name": "Hailey Nguyen"
    },
    {
      "id": "103405110",
      "type": "author",
      "name": "Hannah Korevaar"
    },
    {
      "id": "2314125186",
      "type": "author",
      "name": "Hu Xu"
    },
    {
      "id": "2290402489",
      "type": "author",
      "name": "Hugo Touvron"
    },
    {
      "id": "121929334",
      "type": "author",
      "name": "Iliyan Zarov"
    },
    {
      "id": "34921162",
      "type": "author",
      "name": "Imanol Arrieta Ibarra"
    },
    {
      "id": "2207049",
      "type": "author",
      "name": "Isabel M. Kloumann"
    },
    {
      "id": "2267241285",
      "type": "author",
      "name": "Ishan Misra"
    },
    {
      "id": "2264288587",
      "type": "author",
      "name": "Ivan Evtimov"
    },
    {
      "id": "1805998294",
      "type": "author",
      "name": "Jade Copet"
    },
    {
      "id": "2314056661",
      "type": "author",
      "name": "Jaewon Lee"
    },
    {
      "id": "50825669",
      "type": "author",
      "name": "J. Geffert"
    },
    {
      "id": "2313917660",
      "type": "author",
      "name": "Jana Vranes"
    },
    {
      "id": "2314078634",
      "type": "author",
      "name": "Jason Park"
    },
    {
      "id": "3222225",
      "type": "author",
      "name": "Jay Mahadeokar"
    },
    {
      "id": "2313919733",
      "type": "author",
      "name": "Jeet Shah"
    },
    {
      "id": "35721567",
      "type": "author",
      "name": "J. V. D. Linde"
    },
    {
      "id": "2313909388",
      "type": "author",
      "name": "Jennifer Billock"
    },
    {
      "id": "2287049560",
      "type": "author",
      "name": "Jenny Hong"
    },
    {
      "id": "2223749565",
      "type": "author",
      "name": "Jenya Lee"
    },
    {
      "id": "2223974989",
      "type": "author",
      "name": "Jeremy Fu"
    },
    {
      "id": "31357678",
      "type": "author",
      "name": "Jianfeng Chi"
    },
    {
      "id": "2314428565",
      "type": "author",
      "name": "Jianyu Huang"
    },
    {
      "id": "2314080357",
      "type": "author",
      "name": "Jiawen Liu"
    },
    {
      "id": "2314602001",
      "type": "author",
      "name": "Jie Wang"
    },
    {
      "id": "2314078877",
      "type": "author",
      "name": "Jiecao Yu"
    },
    {
      "id": "1749686057",
      "type": "author",
      "name": "Joanna Bitton"
    },
    {
      "id": "90591458",
      "type": "author",
      "name": "Joe Spisak"
    },
    {
      "id": "2149161568",
      "type": "author",
      "name": "Jongsoo Park"
    },
    {
      "id": "2313925205",
      "type": "author",
      "name": "Joseph Rocca"
    },
    {
      "id": "2313912873",
      "type": "author",
      "name": "Joshua Johnstun"
    },
    {
      "id": "2273413914",
      "type": "author",
      "name": "Joshua Saxe"
    },
    {
      "id": "2211671694",
      "type": "author",
      "name": "Ju-Qing Jia"
    },
    {
      "id": "2313918589",
      "type": "author",
      "name": "Kalyan Vasuden Alwala"
    },
    {
      "id": "17097160",
      "type": "author",
      "name": "K. Upasani"
    },
    {
      "id": "2313918427",
      "type": "author",
      "name": "Kate Plawiak"
    },
    {
      "id": "2313920868",
      "type": "author",
      "name": "Keqian Li"
    },
    {
      "id": "2285859430",
      "type": "author",
      "name": "K. Heafield"
    },
    {
      "id": "2282542714",
      "type": "author",
      "name": "Kevin Stone"
    },
    {
      "id": "1405642252",
      "type": "author",
      "name": "Khalid El-Arini"
    },
    {
      "id": "2273645788",
      "type": "author",
      "name": "Krithika Iyer"
    },
    {
      "id": "2279924280",
      "type": "author",
      "name": "Kshitiz Malik"
    },
    {
      "id": "2313925316",
      "type": "author",
      "name": "Kuen-ley Chiu"
    },
    {
      "id": "2313913532",
      "type": "author",
      "name": "Kunal Bhalla"
    },
    {
      "id": "2313915935",
      "type": "author",
      "name": "Lauren Rantala-Yeary"
    },
    {
      "id": "1803520",
      "type": "author",
      "name": "L. Maaten"
    },
    {
      "id": "2314080073",
      "type": "author",
      "name": "Lawrence Chen"
    },
    {
      "id": "2313924605",
      "type": "author",
      "name": "Liang Tan"
    },
    {
      "id": "2313918409",
      "type": "author",
      "name": "Liz Jenkins"
    },
    {
      "id": "2249724552",
      "type": "author",
      "name": "Louis Martin"
    },
    {
      "id": "2313912794",
      "type": "author",
      "name": "Lubo Malo"
    },
    {
      "id": "2040305955",
      "type": "author",
      "name": "Lukas Blecher"
    },
    {
      "id": "2313925619",
      "type": "author",
      "name": "Lukas Landzaat"
    },
    {
      "id": "2314194315",
      "type": "author",
      "name": "Luke de Oliveira"
    },
    {
      "id": "2000839712",
      "type": "author",
      "name": "Madeline Muzzi"
    },
    {
      "id": "2047114741",
      "type": "author",
      "name": "M. Pasupuleti"
    },
    {
      "id": "152964870",
      "type": "author",
      "name": "Mannat Singh"
    },
    {
      "id": "2210374",
      "type": "author",
      "name": "Manohar Paluri"
    },
    {
      "id": "2059886128",
      "type": "author",
      "name": "Marcin Kardas"
    },
    {
      "id": "2313909379",
      "type": "author",
      "name": "Mathew Oldham"
    },
    {
      "id": "2313912870",
      "type": "author",
      "name": "Mathieu Rita"
    },
    {
      "id": "2313905186",
      "type": "author",
      "name": "Maya Pavlova"
    },
    {
      "id": "2165660870",
      "type": "author",
      "name": "M. Kambadur"
    },
    {
      "id": "2247796743",
      "type": "author",
      "name": "Mike Lewis"
    },
    {
      "id": "2310234768",
      "type": "author",
      "name": "Min Si"
    },
    {
      "id": "2247874378",
      "type": "author",
      "name": "Mitesh Kumar Singh"
    },
    {
      "id": "2314434867",
      "type": "author",
      "name": "Mona Hassan"
    },
    {
      "id": "2911626",
      "type": "author",
      "name": "Narjes Torabi"
    },
    {
      "id": "2223756247",
      "type": "author",
      "name": "Nikolay Bashlykov"
    },
    {
      "id": "3444222",
      "type": "author",
      "name": "Nikolay Bogoychev"
    },
    {
      "id": "2096643450",
      "type": "author",
      "name": "Olivier Duchenne"
    },
    {
      "id": "2166310112",
      "type": "author",
      "name": "Onur cCelebi"
    },
    {
      "id": "2037772368",
      "type": "author",
      "name": "Patrick Alrassy"
    },
    {
      "id": "2257643167",
      "type": "author",
      "name": "Pengchuan Zhang"
    },
    {
      "id": "2273574279",
      "type": "author",
      "name": "Pengwei Li"
    },
    {
      "id": "2268221163",
      "type": "author",
      "name": "Petar Vasić"
    },
    {
      "id": "2313915931",
      "type": "author",
      "name": "Peter Weng"
    },
    {
      "id": "46175439",
      "type": "author",
      "name": "Pratik Dubal"
    },
    {
      "id": "2304450089",
      "type": "author",
      "name": "Praveen Krishnan"
    },
    {
      "id": "2214843767",
      "type": "author",
      "name": "Puxin Xu"
    },
    {
      "id": "2314186827",
      "type": "author",
      "name": "Qing He"
    },
    {
      "id": "2313912601",
      "type": "author",
      "name": "Qingxiao Dong"
    },
    {
      "id": "2313910187",
      "type": "author",
      "name": "Ragavan Srinivasan"
    },
    {
      "id": "2313925467",
      "type": "author",
      "name": "Raj Ganapathy"
    },
    {
      "id": "98804036",
      "type": "author",
      "name": "Ramon Calderer"
    },
    {
      "id": "2313909428",
      "type": "author",
      "name": "Ricardo Silveira Cabral"
    },
    {
      "id": "1962768",
      "type": "author",
      "name": "Robert Stojnic"
    },
    {
      "id": "48647153",
      "type": "author",
      "name": "Roberta Raileanu"
    },
    {
      "id": "3102850",
      "type": "author",
      "name": "Rohit Girdhar"
    },
    {
      "id": "2313913363",
      "type": "author",
      "name": "Rohit Patel"
    },
    {
      "id": "2007285239",
      "type": "author",
      "name": "R. Sauvestre"
    },
    {
      "id": "2313925210",
      "type": "author",
      "name": "Ronnie Polidoro"
    },
    {
      "id": "1722889",
      "type": "author",
      "name": "Roshan Sumbaly"
    },
    {
      "id": "2110697298",
      "type": "author",
      "name": "Ross Taylor"
    },
    {
      "id": "2214818043",
      "type": "author",
      "name": "Ruan Silva"
    },
    {
      "id": "2266467782",
      "type": "author",
      "name": "Rui Hou"
    },
    {
      "id": "2248766592",
      "type": "author",
      "name": "Rui Wang"
    },
    {
      "id": "2268759462",
      "type": "author",
      "name": "S. Hosseini"
    },
    {
      "id": "2273416143",
      "type": "author",
      "name": "Sahana Chennabasappa"
    },
    {
      "id": "2313985129",
      "type": "author",
      "name": "Sanjay Singh"
    },
    {
      "id": "2277511475",
      "type": "author",
      "name": "Sean Bell"
    },
    {
      "id": "2281792543",
      "type": "author",
      "name": "Seohyun Sonia Kim"
    },
    {
      "id": "2068070",
      "type": "author",
      "name": "Sergey Edunov"
    },
    {
      "id": "35557488",
      "type": "author",
      "name": "Shaoliang Nie"
    },
    {
      "id": "1498636613",
      "type": "author",
      "name": "S. Raparthy"
    },
    {
      "id": "2191455",
      "type": "author",
      "name": "Sheng Shen"
    },
    {
      "id": "2272846244",
      "type": "author",
      "name": "Shengye Wan"
    },
    {
      "id": "2116473",
      "type": "author",
      "name": "Shruti Bhosale"
    },
    {
      "id": "2314071119",
      "type": "author",
      "name": "Shun Zhang"
    },
    {
      "id": "83754395",
      "type": "author",
      "name": "Simon Vandenhende"
    },
    {
      "id": "47505161",
      "type": "author",
      "name": "Soumya Batra"
    },
    {
      "id": "2273415395",
      "type": "author",
      "name": "Spencer Whitman"
    },
    {
      "id": "31460313",
      "type": "author",
      "name": "Sten Sootla"
    },
    {
      "id": "2313909594",
      "type": "author",
      "name": "Stephane Collot"
    },
    {
      "id": "40895369",
      "type": "author",
      "name": "Suchin Gururangan"
    },
    {
      "id": "148016419",
      "type": "author",
      "name": "S. Borodinsky"
    },
    {
      "id": "2313925454",
      "type": "author",
      "name": "Tamar Herman"
    },
    {
      "id": "2313918585",
      "type": "author",
      "name": "Tara Fowler"
    },
    {
      "id": "2313917558",
      "type": "author",
      "name": "Tarek Sheasha"
    },
    {
      "id": "2313910328",
      "type": "author",
      "name": "Thomas Georgiou"
    },
    {
      "id": "2073456043",
      "type": "author",
      "name": "Thomas Scialom"
    },
    {
      "id": "2313915815",
      "type": "author",
      "name": "Tobias Speckbacher"
    },
    {
      "id": "2313914277",
      "type": "author",
      "name": "Tong Xiao"
    },
    {
      "id": "46907106",
      "type": "author",
      "name": "Ujjwal Karn"
    },
    {
      "id": "2314332514",
      "type": "author",
      "name": "Vibhor Gupta"
    },
    {
      "id": "34066479",
      "type": "author",
      "name": "Vignesh Ramanathan"
    },
    {
      "id": "2190957318",
      "type": "author",
      "name": "Viktor Kerkez"
    },
    {
      "id": "2313913380",
      "type": "author",
      "name": "Vincent Gonguet"
    },
    {
      "id": "2313918349",
      "type": "author",
      "name": "Virginie Do"
    },
    {
      "id": "2232955561",
      "type": "author",
      "name": "Vish Vogeti"
    },
    {
      "id": "2162195471",
      "type": "author",
      "name": "Vladan Petrovic"
    },
    {
      "id": "2266751414",
      "type": "author",
      "name": "Weiwei Chu"
    },
    {
      "id": "2290750668",
      "type": "author",
      "name": "Wenhan Xiong"
    },
    {
      "id": "2223742000",
      "type": "author",
      "name": "Wenyin Fu"
    },
    {
      "id": "2313913371",
      "type": "author",
      "name": "Whit-ney Meers"
    },
    {
      "id": "2297930724",
      "type": "author",
      "name": "Xiaodong Wang"
    },
    {
      "id": "2249851858",
      "type": "author",
      "name": "Xiaoqing Ellen Tan"
    },
    {
      "id": "2285798957",
      "type": "author",
      "name": "Xinfeng Xie"
    },
    {
      "id": "2313910170",
      "type": "author",
      "name": "Xuchao Jia"
    },
    {
      "id": "2314067352",
      "type": "author",
      "name": "Xuewei Wang"
    },
    {
      "id": "1404341450",
      "type": "author",
      "name": "Yaelle Goldschlag"
    },
    {
      "id": "2286511206",
      "type": "author",
      "name": "Yashesh Gaur"
    },
    {
      "id": "2223764353",
      "type": "author",
      "name": "Yasmine Babaei"
    },
    {
      "id": "148416622",
      "type": "author",
      "name": "Yiqian Wen"
    },
    {
      "id": "2314381758",
      "type": "author",
      "name": "Yiwen Song"
    },
    {
      "id": "2108473229",
      "type": "author",
      "name": "Yuchen Zhang"
    },
    {
      "id": "2297839847",
      "type": "author",
      "name": "Yue Li"
    },
    {
      "id": "2272672481",
      "type": "author",
      "name": "Yuning Mao"
    },
    {
      "id": "2297187212",
      "type": "author",
      "name": "Zacharie Delpierre Coudert"
    },
    {
      "id": "2293992938",
      "type": "author",
      "name": "Zhengxu Yan"
    },
    {
      "id": "2266490735",
      "type": "author",
      "name": "Zhengxing Chen"
    },
    {
      "id": "51149919",
      "type": "author",
      "name": "Zoe Papakipos"
    },
    {
      "id": "2233294011",
      "type": "author",
      "name": "Aaron Grattafiori"
    },
    {
      "id": "2312019070",
      "type": "author",
      "name": "Abha Jain"
    },
    {
      "id": "2313915967",
      "type": "author",
      "name": "Adam Kelsey"
    },
    {
      "id": "116814432",
      "type": "author",
      "name": "Adam Shajnfeld"
    },
    {
      "id": "2077604116",
      "type": "author",
      "name": "Adi Gangidi"
    },
    {
      "id": "2313910256",
      "type": "author",
      "name": "Adolfo Victoria"
    },
    {
      "id": "2313913221",
      "type": "author",
      "name": "Ahuva Goldstand"
    },
    {
      "id": "2313925780",
      "type": "author",
      "name": "Ajay Menon"
    },
    {
      "id": "2314067298",
      "type": "author",
      "name": "Ajay Sharma"
    },
    {
      "id": "2313915843",
      "type": "author",
      "name": "Alex Boesenberg"
    },
    {
      "id": "2313910116",
      "type": "author",
      "name": "Alex Vaughan"
    },
    {
      "id": "14667698",
      "type": "author",
      "name": "Alexei Baevski"
    },
    {
      "id": "2313918472",
      "type": "author",
      "name": "Allie Feinstein"
    },
    {
      "id": "122882087",
      "type": "author",
      "name": "A. Kallet"
    },
    {
      "id": "2313918666",
      "type": "author",
      "name": "Amit Sangani"
    },
    {
      "id": "2313925473",
      "type": "author",
      "name": "Anam Yunus"
    },
    {
      "id": "2266838640",
      "type": "author",
      "name": "Andrei Lupu"
    },
    {
      "id": "2243192949",
      "type": "author",
      "name": "Andres Alvarado"
    },
    {
      "id": "2313925570",
      "type": "author",
      "name": "Andrew Caples"
    },
    {
      "id": "2313913152",
      "type": "author",
      "name": "Andrew Gu"
    },
    {
      "id": "2313919243",
      "type": "author",
      "name": "Andrew Ho"
    },
    {
      "id": "2282542314",
      "type": "author",
      "name": "Andrew Poulton"
    },
    {
      "id": "2313909933",
      "type": "author",
      "name": "Andrew Ryan"
    },
    {
      "id": "1453469113",
      "type": "author",
      "name": "Ankit Ramchandani"
    },
    {
      "id": "2313918528",
      "type": "author",
      "name": "Annie Franco"
    },
    {
      "id": "51912276",
      "type": "author",
      "name": "Aparajita Saraf"
    },
    {
      "id": "2313917455",
      "type": "author",
      "name": "Arkabandhu Chowdhury"
    },
    {
      "id": "2313925699",
      "type": "author",
      "name": "Ashley Gabriel"
    },
    {
      "id": "2313909987",
      "type": "author",
      "name": "Ashwin Bharambe"
    },
    {
      "id": "35198582",
      "type": "author",
      "name": "Assaf Eisenman"
    },
    {
      "id": "2313915928",
      "type": "author",
      "name": "Azadeh Yazdan"
    },
    {
      "id": "2313918606",
      "type": "author",
      "name": "Beau James"
    },
    {
      "id": "2313913272",
      "type": "author",
      "name": "Ben Maurer"
    },
    {
      "id": "2897362",
      "type": "author",
      "name": "Ben Leonhardi"
    },
    {
      "id": "2319973",
      "type": "author",
      "name": "Po-Yao (Bernie) Huang"
    },
    {
      "id": "2313918673",
      "type": "author",
      "name": "Beth Loyd"
    },
    {
      "id": "2313909983",
      "type": "author",
      "name": "Beto De Paola"
    },
    {
      "id": "8005713",
      "type": "author",
      "name": "Bhargavi Paranjape"
    },
    {
      "id": "2314014642",
      "type": "author",
      "name": "Bing Liu"
    },
    {
      "id": "2314069142",
      "type": "author",
      "name": "Bo Wu"
    },
    {
      "id": "2313909094",
      "type": "author",
      "name": "Boyu Ni"
    },
    {
      "id": "2313916282",
      "type": "author",
      "name": "Braden Hancock"
    },
    {
      "id": "46240090",
      "type": "author",
      "name": "Bram Wasti"
    },
    {
      "id": "2313918213",
      "type": "author",
      "name": "Brandon Spence"
    },
    {
      "id": "2313918219",
      "type": "author",
      "name": "Brani Stojkovic"
    },
    {
      "id": "2313925572",
      "type": "author",
      "name": "Brian Gamido"
    },
    {
      "id": "2313917587",
      "type": "author",
      "name": "Britt Montalvo"
    },
    {
      "id": "2313919256",
      "type": "author",
      "name": "Carl Parker"
    },
    {
      "id": "2313913658",
      "type": "author",
      "name": "Carly Burton"
    },
    {
      "id": "2313917281",
      "type": "author",
      "name": "Catalina Mejia"
    },
    {
      "id": "2313925537",
      "type": "author",
      "name": "Changhan Wang"
    },
    {
      "id": "2314071777",
      "type": "author",
      "name": "Changkyu Kim"
    },
    {
      "id": "2314072280",
      "type": "author",
      "name": "Chao Zhou"
    },
    {
      "id": "2313982652",
      "type": "author",
      "name": "Chester Hu"
    },
    {
      "id": "2290129157",
      "type": "author",
      "name": "Ching-Hsiang Chu"
    },
    {
      "id": "2263867885",
      "type": "author",
      "name": "Chris Cai"
    },
    {
      "id": "2313925773",
      "type": "author",
      "name": "Chris Tindal"
    },
    {
      "id": "2322150",
      "type": "author",
      "name": "Christoph Feichtenhofer"
    },
    {
      "id": "50986776",
      "type": "author",
      "name": "Damon Civin"
    },
    {
      "id": "2313913237",
      "type": "author",
      "name": "Dana Beaty"
    },
    {
      "id": "3046707",
      "type": "author",
      "name": "Daniel Kreymer"
    },
    {
      "id": "2313916159",
      "type": "author",
      "name": "Danny Wyatt"
    },
    {
      "id": "2161835643",
      "type": "author",
      "name": "David Adkins"
    },
    {
      "id": "2313915190",
      "type": "author",
      "name": "David Xu"
    },
    {
      "id": "2273657478",
      "type": "author",
      "name": "Davide Testuggine"
    },
    {
      "id": "2311498203",
      "type": "author",
      "name": "Delia David"
    },
    {
      "id": "2248278031",
      "type": "author",
      "name": "Devi Parikh"
    },
    {
      "id": "2145259939",
      "type": "author",
      "name": "Diana Liskovich"
    },
    {
      "id": "2313925798",
      "type": "author",
      "name": "Didem Foss"
    },
    {
      "id": "2283843884",
      "type": "author",
      "name": "Dingkang Wang"
    },
    {
      "id": "145267619",
      "type": "author",
      "name": "Duc Le"
    },
    {
      "id": "2313913567",
      "type": "author",
      "name": "Dustin Holland"
    },
    {
      "id": "2313916034",
      "type": "author",
      "name": "Edward Dowling"
    },
    {
      "id": "2313916009",
      "type": "author",
      "name": "Eissa Jamil"
    },
    {
      "id": "2313925401",
      "type": "author",
      "name": "Elaine Montgomery"
    },
    {
      "id": "6072807",
      "type": "author",
      "name": "Eleonora Presani"
    },
    {
      "id": "2313914699",
      "type": "author",
      "name": "Emily Hahn"
    },
    {
      "id": "2313913986",
      "type": "author",
      "name": "Emily Wood"
    },
    {
      "id": "2313913160",
      "type": "author",
      "name": "Erik Brinkman"
    },
    {
      "id": "2064373270",
      "type": "author",
      "name": "Esteban Arcaute"
    },
    {
      "id": "2313915853",
      "type": "author",
      "name": "Evan Dunbar"
    },
    {
      "id": "2313918562",
      "type": "author",
      "name": "Evan Smothers"
    },
    {
      "id": "2314197755",
      "type": "author",
      "name": "Fei Sun"
    },
    {
      "id": "32653170",
      "type": "author",
      "name": "Felix Kreuk"
    },
    {
      "id": "2313907929",
      "type": "author",
      "name": "Feng Tian"
    },
    {
      "id": "2160885118",
      "type": "author",
      "name": "Firat Ozgenel"
    },
    {
      "id": "31292058",
      "type": "author",
      "name": "Francesco Caggioni"
    },
    {
      "id": "2061585840",
      "type": "author",
      "name": "Francisco Guzm'an"
    },
    {
      "id": "3360115",
      "type": "author",
      "name": "Frank J. Kanayet"
    },
    {
      "id": "2243280567",
      "type": "author",
      "name": "Frank Seide"
    },
    {
      "id": "2313913137",
      "type": "author",
      "name": "Gabriela Medina Florez"
    },
    {
      "id": "2313925846",
      "type": "author",
      "name": "Gabriella Schwarz"
    },
    {
      "id": "2313918570",
      "type": "author",
      "name": "Gada Badeer"
    },
    {
      "id": "2313916006",
      "type": "author",
      "name": "Georgia Swee"
    },
    {
      "id": "2313925677",
      "type": "author",
      "name": "Gil Halpern"
    },
    {
      "id": "2028300167",
      "type": "author",
      "name": "G. Thattai"
    },
    {
      "id": "2313918648",
      "type": "author",
      "name": "Grant Herman"
    },
    {
      "id": "2266304177",
      "type": "author",
      "name": "G. Sizov"
    },
    {
      "id": "47776500",
      "type": "author",
      "name": "Guangyi Zhang"
    },
    {
      "id": "2289832027",
      "type": "author",
      "name": "Guna Lakshminarayanan"
    },
    {
      "id": "2343773236",
      "type": "author",
      "name": "Hamid Shojanazeri"
    },
    {
      "id": "2313908554",
      "type": "author",
      "name": "Han Zou"
    },
    {
      "id": "2314725059",
      "type": "author",
      "name": "Hannah Wang"
    },
    {
      "id": "2261827291",
      "type": "author",
      "name": "Han Zha"
    },
    {
      "id": "30279076",
      "type": "author",
      "name": "Haroun Habeeb"
    },
    {
      "id": "2313913215",
      "type": "author",
      "name": "Harrison Rudolph"
    },
    {
      "id": "2297942583",
      "type": "author",
      "name": "Helen Suk"
    },
    {
      "id": "87085411",
      "type": "author",
      "name": "Henry Aspegren"
    },
    {
      "id": "2313910892",
      "type": "author",
      "name": "Hunter Goldman"
    },
    {
      "id": "2322981055",
      "type": "author",
      "name": "Igor Molybog"
    },
    {
      "id": "2032201719",
      "type": "author",
      "name": "Igor Tufanov"
    },
    {
      "id": "2127473751",
      "type": "author",
      "name": "Irina-Elena Veliche"
    },
    {
      "id": "2064713742",
      "type": "author",
      "name": "Itai Gat"
    },
    {
      "id": "2313913125",
      "type": "author",
      "name": "Jake Weissman"
    },
    {
      "id": "2313913133",
      "type": "author",
      "name": "James Geboski"
    },
    {
      "id": "2313917982",
      "type": "author",
      "name": "James Kohli"
    },
    {
      "id": "2313909751",
      "type": "author",
      "name": "Japhet Asher"
    },
    {
      "id": "2131867883",
      "type": "author",
      "name": "Jean-Baptiste Gaya"
    },
    {
      "id": "2313917933",
      "type": "author",
      "name": "Jeff Marcus"
    },
    {
      "id": "2314079720",
      "type": "author",
      "name": "Jeff Tang"
    },
    {
      "id": "2313924089",
      "type": "author",
      "name": "Jennifer Chan"
    },
    {
      "id": "2313906372",
      "type": "author",
      "name": "Jenny Zhen"
    },
    {
      "id": "39906022",
      "type": "author",
      "name": "Jeremy Reizenstein"
    },
    {
      "id": "2313925697",
      "type": "author",
      "name": "Jeremy Teboul"
    },
    {
      "id": "2314712137",
      "type": "author",
      "name": "Jessica Zhong"
    },
    {
      "id": "2314696266",
      "type": "author",
      "name": "Jian Jin"
    },
    {
      "id": "2314170063",
      "type": "author",
      "name": "Jingyi Yang"
    },
    {
      "id": "2313921009",
      "type": "author",
      "name": "Joe Cummings"
    },
    {
      "id": "2313916920",
      "type": "author",
      "name": "Jon Carvill"
    },
    {
      "id": "2313918017",
      "type": "author",
      "name": "Jon Shepard"
    },
    {
      "id": "2313925667",
      "type": "author",
      "name": "Jonathan McPhie"
    },
    {
      "id": "2314554743",
      "type": "author",
      "name": "Jonathan Torres"
    },
    {
      "id": "2313925786",
      "type": "author",
      "name": "Josh Ginsburg"
    },
    {
      "id": "2314072216",
      "type": "author",
      "name": "Junjie Wang"
    },
    {
      "id": "2115598555",
      "type": "author",
      "name": "Kaixing(Kai) Wu"
    },
    {
      "id": "2313913073",
      "type": "author",
      "name": "U. KamHou"
    },
    {
      "id": "2313917036",
      "type": "author",
      "name": "Karan Saxena"
    },
    {
      "id": "2313913208",
      "type": "author",
      "name": "Karthik Prasad"
    },
    {
      "id": "40267343",
      "type": "author",
      "name": "Kartikay Khandelwal"
    },
    {
      "id": "2158995926",
      "type": "author",
      "name": "Katayoun Zand"
    },
    {
      "id": "2313926373",
      "type": "author",
      "name": "Kathy Matosich"
    },
    {
      "id": "2262920209",
      "type": "author",
      "name": "K. Veeraraghavan"
    },
    {
      "id": "2313925800",
      "type": "author",
      "name": "Kelly Michelena"
    },
    {
      "id": "2314883034",
      "type": "author",
      "name": "Kun Huang"
    },
    {
      "id": "2313918835",
      "type": "author",
      "name": "Kunal Chawla"
    },
    {
      "id": "1410624139",
      "type": "author",
      "name": "Kushal Lakhotia"
    },
    {
      "id": "2314883036",
      "type": "author",
      "name": "Kyle Huang"
    },
    {
      "id": "2197533966",
      "type": "author",
      "name": "Lailin Chen"
    },
    {
      "id": "2313913092",
      "type": "author",
      "name": "Lakshya Garg"
    },
    {
      "id": "2313926370",
      "type": "author",
      "name": "A. Lavender"
    },
    {
      "id": "2314073913",
      "type": "author",
      "name": "Leandro Silva"
    },
    {
      "id": "2313926731",
      "type": "author",
      "name": "Lee Bell"
    },
    {
      "id": "2313951052",
      "type": "author",
      "name": "Lei Zhang"
    },
    {
      "id": "2314460078",
      "type": "author",
      "name": "Liangpeng Guo"
    },
    {
      "id": "2269696579",
      "type": "author",
      "name": "Licheng Yu"
    },
    {
      "id": "2313918301",
      "type": "author",
      "name": "Liron Moshkovich"
    },
    {
      "id": "2331511165",
      "type": "author",
      "name": "Luca Wehrstedt"
    },
    {
      "id": "2072010",
      "type": "author",
      "name": "Madian Khabsa"
    },
    {
      "id": "2313918577",
      "type": "author",
      "name": "Manav Avalani"
    },
    {
      "id": "2273002871",
      "type": "author",
      "name": "Manish Bhatt"
    },
    {
      "id": "2010057",
      "type": "author",
      "name": "M. Tsimpoukelli"
    },
    {
      "id": "98800979",
      "type": "author",
      "name": "Martynas Mankus"
    },
    {
      "id": "2093466943",
      "type": "author",
      "name": "Matan Hasson"
    },
    {
      "id": "83174287",
      "type": "author",
      "name": "M. Lennie"
    },
    {
      "id": "2248340971",
      "type": "author",
      "name": "Matthias Reso"
    },
    {
      "id": "2313918566",
      "type": "author",
      "name": "Maxim Groshev"
    },
    {
      "id": "2290016941",
      "type": "author",
      "name": "Maxim Naumov"
    },
    {
      "id": "52097509",
      "type": "author",
      "name": "Maya Lathi"
    },
    {
      "id": "2313926376",
      "type": "author",
      "name": "Meghan Keneally"
    },
    {
      "id": "1727524",
      "type": "author",
      "name": "M. Seltzer"
    },
    {
      "id": "2259912893",
      "type": "author",
      "name": "Michal Valko"
    },
    {
      "id": "2313917797",
      "type": "author",
      "name": "Michelle Restrepo"
    },
    {
      "id": "2314105463",
      "type": "author",
      "name": "Mihir Patel"
    },
    {
      "id": "2313909990",
      "type": "author",
      "name": "Mik Vyatskov"
    },
    {
      "id": "49089678",
      "type": "author",
      "name": "Mikayel Samvelyan"
    },
    {
      "id": "2314111844",
      "type": "author",
      "name": "Mike Clark"
    },
    {
      "id": "2313910746",
      "type": "author",
      "name": "Mike Macey"
    },
    {
      "id": "2314078208",
      "type": "author",
      "name": "Mike Wang"
    },
    {
      "id": "147487949",
      "type": "author",
      "name": "Miquel Jubert Hermoso"
    },
    {
      "id": "2313913313",
      "type": "author",
      "name": "Mo Metanat"
    },
    {
      "id": "32371083",
      "type": "author",
      "name": "Mohammad Rastegari"
    },
    {
      "id": "2313910172",
      "type": "author",
      "name": "Munish Bansal"
    },
    {
      "id": "2265554054",
      "type": "author",
      "name": "Nandhini Santhanam"
    },
    {
      "id": "2313916856",
      "type": "author",
      "name": "Natascha Parks"
    },
    {
      "id": "2313910535",
      "type": "author",
      "name": "Natasha White"
    },
    {
      "id": "2313918859",
      "type": "author",
      "name": "Navyata Bawa"
    },
    {
      "id": "40943290",
      "type": "author",
      "name": "Nayan Singhal"
    },
    {
      "id": "2313909893",
      "type": "author",
      "name": "Nick Egebo"
    },
    {
      "id": "1746841",
      "type": "author",
      "name": "Nicolas Usunier"
    },
    {
      "id": "2285597551",
      "type": "author",
      "name": "Nikolay Pavlovich Laptev"
    },
    {
      "id": "2313910396",
      "type": "author",
      "name": "Ning Dong"
    },
    {
      "id": "2314687456",
      "type": "author",
      "name": "Ning Zhang"
    },
    {
      "id": "2313916655",
      "type": "author",
      "name": "Norman Cheng"
    },
    {
      "id": "1405690366",
      "type": "author",
      "name": "Oleg Chernoguz"
    },
    {
      "id": "2313918980",
      "type": "author",
      "name": "Olivia Hart"
    },
    {
      "id": "1778909324",
      "type": "author",
      "name": "Omkar Salpekar"
    },
    {
      "id": "1729960",
      "type": "author",
      "name": "Ozlem Kalinli"
    },
    {
      "id": "2313916098",
      "type": "author",
      "name": "Parkin Kent"
    },
    {
      "id": "2313909999",
      "type": "author",
      "name": "Parth Parekh"
    },
    {
      "id": "2313915995",
      "type": "author",
      "name": "Paul Saab"
    },
    {
      "id": "2307470796",
      "type": "author",
      "name": "Pavan Balaji"
    },
    {
      "id": "31915793",
      "type": "author",
      "name": "Pedro Rittner"
    },
    {
      "id": "14171685",
      "type": "author",
      "name": "Philip Bontrager"
    },
    {
      "id": "2313919367",
      "type": "author",
      "name": "Pierre Roux"
    },
    {
      "id": "2257254817",
      "type": "author",
      "name": "Piotr Dollár"
    },
    {
      "id": "2163709899",
      "type": "author",
      "name": "Polina Zvyagina"
    },
    {
      "id": "2459737",
      "type": "author",
      "name": "Prashant Ratanchandani"
    },
    {
      "id": "41020300",
      "type": "author",
      "name": "Pritish Yuvraj"
    },
    {
      "id": "2313916229",
      "type": "author",
      "name": "Qian Liang"
    },
    {
      "id": "2313909804",
      "type": "author",
      "name": "Rachad Alao"
    },
    {
      "id": "2313934481",
      "type": "author",
      "name": "Rachel Rodriguez"
    },
    {
      "id": "14714641",
      "type": "author",
      "name": "Rafi Ayub"
    },
    {
      "id": "2313909891",
      "type": "author",
      "name": "Raghotham Murthy"
    },
    {
      "id": "2313918294",
      "type": "author",
      "name": "Raghu Nayani"
    },
    {
      "id": "2264459586",
      "type": "author",
      "name": "Rahul Mitra"
    },
    {
      "id": "2313919671",
      "type": "author",
      "name": "Raymond Li"
    },
    {
      "id": "2313918488",
      "type": "author",
      "name": "Rebekkah Hogan"
    },
    {
      "id": "2313913081",
      "type": "author",
      "name": "Robin Battey"
    },
    {
      "id": "46886013",
      "type": "author",
      "name": "Rocky Wang"
    },
    {
      "id": "2313918943",
      "type": "author",
      "name": "Rohan Maheswari"
    },
    {
      "id": "1410913697",
      "type": "author",
      "name": "Russ Howes"
    },
    {
      "id": "1905713",
      "type": "author",
      "name": "Ruty Rinott"
    },
    {
      "id": "2313916859",
      "type": "author",
      "name": "Sai Jayesh Bondu"
    },
    {
      "id": "19200118",
      "type": "author",
      "name": "Samyak Datta"
    },
    {
      "id": "2313913104",
      "type": "author",
      "name": "Sara Chugh"
    },
    {
      "id": "2313916525",
      "type": "author",
      "name": "Sara Hunt"
    },
    {
      "id": "2313919311",
      "type": "author",
      "name": "Sargun Dhillon"
    },
    {
      "id": "2313918976",
      "type": "author",
      "name": "Sasha Sidorov"
    },
    {
      "id": "2314108295",
      "type": "author",
      "name": "Satadru Pan"
    },
    {
      "id": "2314005184",
      "type": "author",
      "name": "Saurabh Verma"
    },
    {
      "id": "2314406059",
      "type": "author",
      "name": "Seiji Yamamoto"
    },
    {
      "id": "48347720",
      "type": "author",
      "name": "Sharadh Ramaswamy"
    },
    {
      "id": "2313926214",
      "type": "author",
      "name": "Shaun Lindsay"
    },
    {
      "id": "2314693028",
      "type": "author",
      "name": "Sheng Feng"
    },
    {
      "id": "2311565327",
      "type": "author",
      "name": "Shenghao Lin"
    },
    {
      "id": "2268757277",
      "type": "author",
      "name": "S. Zha"
    },
    {
      "id": "2313916766",
      "type": "author",
      "name": "Shiva Shankar"
    },
    {
      "id": "2237076180",
      "type": "author",
      "name": "Shuqiang Zhang"
    },
    {
      "id": "2237101143",
      "type": "author",
      "name": "Sinong Wang"
    },
    {
      "id": "50230355",
      "type": "author",
      "name": "Sneha Agarwal"
    },
    {
      "id": "2423558",
      "type": "author",
      "name": "S. Sajuyigbe"
    },
    {
      "id": "2127604",
      "type": "author",
      "name": "Soumith Chintala"
    },
    {
      "id": "2313919129",
      "type": "author",
      "name": "Stephanie Max"
    },
    {
      "id": "2125208891",
      "type": "author",
      "name": "Stephen Chen"
    },
    {
      "id": "2313926357",
      "type": "author",
      "name": "Steve Kehoe"
    },
    {
      "id": "2313909787",
      "type": "author",
      "name": "Steve Satterfield"
    },
    {
      "id": "2313918283",
      "type": "author",
      "name": "Sudarshan Govindaprasad"
    },
    {
      "id": "2157683980",
      "type": "author",
      "name": "Sumit Gupta"
    },
    {
      "id": "2286537482",
      "type": "author",
      "name": "Sung-Bae Cho"
    },
    {
      "id": "2313919117",
      "type": "author",
      "name": "Sunny Virk"
    },
    {
      "id": "2313914940",
      "type": "author",
      "name": "Suraj Subramanian"
    },
    {
      "id": "89754631",
      "type": "author",
      "name": "Sy Choudhury"
    },
    {
      "id": "2313908725",
      "type": "author",
      "name": "Sydney Goldman"
    },
    {
      "id": "2211633",
      "type": "author",
      "name": "T. Remez"
    },
    {
      "id": "2071335303",
      "type": "author",
      "name": "Tamar Glaser"
    },
    {
      "id": "2313910221",
      "type": "author",
      "name": "Tamara Best"
    },
    {
      "id": "2189305275",
      "type": "author",
      "name": "Thilo Kohler"
    },
    {
      "id": "2313919760",
      "type": "author",
      "name": "Thomas Robinson"
    },
    {
      "id": "2314332791",
      "type": "author",
      "name": "Tianhe Li"
    },
    {
      "id": "1993655237",
      "type": "author",
      "name": "Tianjun Zhang"
    },
    {
      "id": "2313919132",
      "type": "author",
      "name": "Tim Matthews"
    },
    {
      "id": "2313919289",
      "type": "author",
      "name": "Timothy Chou"
    },
    {
      "id": "2313919174",
      "type": "author",
      "name": "Tzook Shaked"
    },
    {
      "id": "2273415095",
      "type": "author",
      "name": "Varun Vontimitta"
    },
    {
      "id": "2313918541",
      "type": "author",
      "name": "Victoria Ajayi"
    },
    {
      "id": "2313910202",
      "type": "author",
      "name": "Victoria Montanez"
    },
    {
      "id": "2313916470",
      "type": "author",
      "name": "Vijai Mohan"
    },
    {
      "id": "2314056846",
      "type": "author",
      "name": "Vinay Satish Kumar"
    },
    {
      "id": "71203676",
      "type": "author",
      "name": "Vishal Mangla"
    },
    {
      "id": "2313685593",
      "type": "author",
      "name": "Vlad Ionescu"
    },
    {
      "id": "144386035",
      "type": "author",
      "name": "V. Poenaru"
    },
    {
      "id": "2051654054",
      "type": "author",
      "name": "Vlad T. Mihailescu"
    },
    {
      "id": "2313920446",
      "type": "author",
      "name": "Vladimir Ivanov"
    },
    {
      "id": "2293767405",
      "type": "author",
      "name": "Wei Li"
    },
    {
      "id": "2314069334",
      "type": "author",
      "name": "Wenchen Wang"
    },
    {
      "id": "375b65ae876ccdd6b909ce6bed7d58549c0da216",
      "type": "paper",
      "title": "Image Memorability Prediction with Vision Transformers",
      "abstract": "Behavioral studies have shown that the memorability of images is similar across groups of people, suggesting that memorability is a function of the intrinsic properties of images, and is unrelated to people's individual experiences and traits. Deep learning networks can be trained on such properties and be used to predict memorability in new data sets. Convolutional neural networks (CNN) have pioneered image memorability prediction, but more recently developed vision transformer (ViT) models may have the potential to yield even better predictions. In this paper, we present the ViTMem, a new memorability model based on ViT, and evaluate memorability predictions obtained by it with state-of-the-art CNN-derived models. Results showed that ViTMem performed equal to or better than state-of-the-art models on all data sets. Additional semantic level analyses revealed that ViTMem is particularly sensitive to the semantic content that drives memorability in images. We conclude that ViTMem provides a new step forward, and propose that ViT-derived models can replace CNNs for computational prediction of image memorability. Researchers, educators, advertisers, visual designers and other interested parties can leverage the model to improve the memorability of their image material.",
      "year": 2023,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/375b65ae876ccdd6b909ce6bed7d58549c0da216",
      "citation_count": 5,
      "reference_count": 39,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "It is proposed that ViT-derived models can replace CNNs for computational prediction of image memorability, and researchers, educators, advertisers, visual designers and other interested parties can leverage the model to improve the memorability of their image material.",
      "open_access_pdf_url": "http://arxiv.org/pdf/2301.08647",
      "open_access_status": "CLOSED",
      "external_id_dblp": "journals/corr/abs-2301-08647",
      "external_id_arxiv": "2301.08647",
      "external_id_doi": "10.48550/arXiv.2301.08647",
      "external_id_corpusid": 256080388
    },
    {
      "id": "46985523",
      "type": "author",
      "name": "Thomas Hagen"
    },
    {
      "id": "2874733",
      "type": "author",
      "name": "T. Espeseth"
    },
    {
      "id": "203bfa5e488a9c3100e3d9b9af5ea34537068612",
      "type": "paper",
      "title": "Multimodal Memorability: Modeling Effects of Semantics and Decay on Video Memorability",
      "abstract": "",
      "year": 2020,
      "venue": "European Conference on Computer Vision",
      "url": "https://www.semanticscholar.org/paper/203bfa5e488a9c3100e3d9b9af5ea34537068612",
      "citation_count": 45,
      "reference_count": 52,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A predictive model of human visual event memory and how those memories decay over time is developed, resulting in a model that is able to produce the first quantitative estimation of how a video decays in memory over time.",
      "external_id_arxiv": "2009.02568",
      "external_id_dblp": "conf/eccv/NewmanFCLMO20",
      "external_id_mag": "3083405459",
      "external_id_doi": "10.1007/978-3-030-58517-4_14",
      "external_id_corpusid": 221375489
    },
    {
      "id": "117232498",
      "type": "author",
      "name": "Anelise Newman"
    },
    {
      "id": "1482544048",
      "type": "author",
      "name": "Camilo Luciano Fosco"
    },
    {
      "id": "24026083",
      "type": "author",
      "name": "Vincent Casser"
    },
    {
      "id": "2132497979",
      "type": "author",
      "name": "Allen Lee"
    },
    {
      "id": "2064049828",
      "type": "author",
      "name": "Barry"
    },
    {
      "id": "2065288337",
      "type": "author",
      "name": "Mcnamara"
    },
    {
      "id": "143868587",
      "type": "author",
      "name": "A. Oliva"
    },
    {
      "id": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "paper",
      "title": "A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks",
      "abstract": "In an era defined by the explosive growth of data and rapid technological advancements, Multimodal Large Language Models (MLLMs) stand at the forefront of artificial intelligence (AI) systems. Designed to seamlessly integrate diverse data types-including text, images, videos, audio, and physiological sequences-MLLMs address the complexities of real-world applications far beyond the capabilities of single-modality systems. In this paper, we systematically sort out the applications of MLLM in multimodal tasks such as natural language, vision, and audio. We also provide a comparative analysis of the focus of different MLLMs in the tasks, and provide insights into the shortcomings of current MLLMs, and suggest potential directions for future research. Through these discussions, this paper hopes to provide valuable insights for the further development and application of MLLM.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "citation_count": 12,
      "reference_count": 113,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper systematically sort out the applications of MLLM in multimodal tasks such as natural language, vision, and audio, and provides a comparative analysis of the focus of different MLLMs in the tasks, and provides insights into the shortcomings of current MLLMs.",
      "external_id_arxiv": "2408.01319",
      "external_id_dblp": "journals/corr/abs-2408-01319",
      "external_id_doi": "10.48550/arXiv.2408.01319",
      "external_id_corpusid": 271693411
    },
    {
      "id": "2136025369",
      "type": "author",
      "name": "Jiaqi Wang"
    },
    {
      "id": "2273631049",
      "type": "author",
      "name": "Hanqi Jiang"
    },
    {
      "id": "2116426849",
      "type": "author",
      "name": "Yi-Hsueh Liu"
    },
    {
      "id": "120688117",
      "type": "author",
      "name": "Chong-Yi Ma"
    },
    {
      "id": "2273584640",
      "type": "author",
      "name": "Xu Zhang"
    },
    {
      "id": "2221032216",
      "type": "author",
      "name": "Yi Pan"
    },
    {
      "id": "2210636248",
      "type": "author",
      "name": "Mengyuan Liu"
    },
    {
      "id": "2314692435",
      "type": "author",
      "name": "Peiran Gu"
    },
    {
      "id": "2314692233",
      "type": "author",
      "name": "Sichen Xia"
    },
    {
      "id": "2284031962",
      "type": "author",
      "name": "Wenjun Li"
    },
    {
      "id": "2257095790",
      "type": "author",
      "name": "Yutong Zhang"
    },
    {
      "id": "2238905102",
      "type": "author",
      "name": "Zihao Wu"
    },
    {
      "id": "2145977326",
      "type": "author",
      "name": "Zheng Liu"
    },
    {
      "id": "2215167446",
      "type": "author",
      "name": "Tianyang Zhong"
    },
    {
      "id": "2257302793",
      "type": "author",
      "name": "Bao Ge"
    },
    {
      "id": "2269508672",
      "type": "author",
      "name": "Tuo Zhang"
    },
    {
      "id": "2251076040",
      "type": "author",
      "name": "Ning Qiang"
    },
    {
      "id": "1742535",
      "type": "author",
      "name": "Xintao Hu"
    },
    {
      "id": "2300922460",
      "type": "author",
      "name": "Xi Jiang"
    },
    {
      "id": "2290203999",
      "type": "author",
      "name": "Xin Zhang"
    },
    {
      "id": "2157449203",
      "type": "author",
      "name": "Wei Zhang"
    },
    {
      "id": "2272672300",
      "type": "author",
      "name": "Dinggang Shen"
    },
    {
      "id": "2301126629",
      "type": "author",
      "name": "Tianming Liu"
    },
    {
      "id": "2277750447",
      "type": "author",
      "name": "Shu Zhang"
    },
    {
      "id": "d081584960c42f7793502bb496e46f682e3e43b3",
      "type": "paper",
      "title": "Long Context Transfer from Language to Vision",
      "abstract": "Video sequences offer valuable temporal information, but existing large multimodal models (LMMs) fall short in understanding extremely long videos. Many works address this by reducing the number of visual tokens using visual resamplers. Alternatively, in this paper, we approach this problem from the perspective of the language model. By simply extrapolating the context length of the language backbone, we enable LMMs to comprehend orders of magnitude more visual tokens without any video training. We call this phenomenon long context transfer and carefully ablate its properties. To effectively measure LMMs' ability to generalize to long contexts in the vision modality, we develop V-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark inspired by the language model's NIAH test. Our proposed Long Video Assistant (LongVA) can process 2000 frames or over 200K visual tokens without additional complexities. With its extended context length, LongVA achieves state-of-the-art performance on Video-MME among 7B-scale models by densely sampling more input frames. Our work is open-sourced at https://github.com/EvolvingLMMs-Lab/LongVA.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/d081584960c42f7793502bb496e46f682e3e43b3",
      "citation_count": 65,
      "reference_count": 61,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "To effectively measure LMMs' ability to generalize to long contexts in the vision modality, V-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark inspired by the language model's NIAH test is developed.",
      "external_id_arxiv": "2406.16852",
      "external_id_dblp": "journals/corr/abs-2406-16852",
      "external_id_doi": "10.48550/arXiv.2406.16852",
      "external_id_corpusid": 270703489
    },
    {
      "id": "2265621323",
      "type": "author",
      "name": "Peiyuan Zhang"
    },
    {
      "id": "2300086932",
      "type": "author",
      "name": "Kaichen Zhang"
    },
    {
      "id": "2165247100",
      "type": "author",
      "name": "Bo Li"
    },
    {
      "id": "2308040513",
      "type": "author",
      "name": "Guangtao Zeng"
    },
    {
      "id": "2295601",
      "type": "author",
      "name": "Jingkang Yang"
    },
    {
      "id": "2145784327",
      "type": "author",
      "name": "Yuanhan Zhang"
    },
    {
      "id": "2257550096",
      "type": "author",
      "name": "Ziyue Wang"
    },
    {
      "id": "2258308833",
      "type": "author",
      "name": "Haoran Tan"
    },
    {
      "id": "2264692022",
      "type": "author",
      "name": "Chunyuan Li"
    },
    {
      "id": "2279869111",
      "type": "author",
      "name": "Ziwei Liu"
    },
    {
      "id": "51f5d1ae89c29d70426bed77680ffab41e1eb945",
      "type": "paper",
      "title": "Modular Memorability: Tiered Representations for Video Memorability Prediction",
      "abstract": "The question of how to best estimate the memorability of visual content is currently a source of debate in the memorability community. In this paper, we propose to explore how different key properties of images and videos affect their consolidation into memory. We analyze the impact of several features and develop a model that emulates the most important parts of a proposed “pathway to memory”: a simple but effective way of representing the different hurdles that new visual content needs to surpass to stay in memory. This framework leads to the construction of our M3-S model, a novel memorability network that processes input videos in a modular fashion. Each module of the network emulates one of the four key steps of the pathway to memory: raw encoding, scene understanding, event understanding and memory consolidation. We find that the different representations learned by our modules are non-trivial and substantially different from each other. Additionally, we observe that certain representations tend to perform better at the task of memorability prediction than others, and we introduce an in-depth ablation study to support our results. Our proposed approach surpasses the state of the art on the two largest video memorability datasets and opens the door to new applications in the field. Our code is available at https://github.com/tekal-ai/modular-memorability.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "url": "https://www.semanticscholar.org/paper/51f5d1ae89c29d70426bed77680ffab41e1eb945",
      "citation_count": 5,
      "reference_count": 54,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper analyzes the impact of several features and develops a model that emulates the most important parts of a proposed “pathway to memory”: a simple but effective way of representing the different hurdles that new visual content needs to surpass to stay in memory.",
      "external_id_dblp": "conf/cvpr/DumontHF23",
      "external_id_doi": "10.1109/CVPR52729.2023.01035",
      "external_id_corpusid": 261081291
    },
    {
      "id": "2111511268",
      "type": "author",
      "name": "Théo Dumont"
    },
    {
      "id": "2233132826",
      "type": "author",
      "name": "Juan Segundo Hevia"
    },
    {
      "id": "727df25246b5156fd2abaf52ba1b47abaeae9d72",
      "type": "paper",
      "title": "Learning in Double Time: The Effect of Lecture Video Speed on Immediate and Delayed Comprehension",
      "abstract": "We presented participants with lecture videos at different speeds and tested immediate and delayed (1 week) comprehension. Results revealed minimal costs incurred by increasing video speed from 1x to 1.5x, or 2x speed, but performance declined beyond 2x speed. We also compared learning outcomes after watching videos once at 1x or twice at 2x speed. There was not an advantage to watching twice at 2x speed but if participants watched the video again at 2x speed immediately before the test, compared with watching once at 1x a week before the test, comprehension improved. Thus, increasing the speed of videos (up to 2x) may be an efficient strategy, especially if students use the time saved for additional studying or rewatching the videos, but learners should do this additional studying shortly before an exam. However, these trends may differ for videos with different speech rates, complexity or difficulty, and audiovisual overlap.",
      "year": 2021,
      "venue": "Applied Cognitive Psychology",
      "url": "https://www.semanticscholar.org/paper/727df25246b5156fd2abaf52ba1b47abaeae9d72",
      "citation_count": 39,
      "reference_count": 62,
      "fields_of_study": "",
      "is_open_access": false,
      "tldr": "",
      "external_id_doi": "10.1002/acp.3899",
      "external_id_corpusid": 244122478
    },
    {
      "id": "118237536",
      "type": "author",
      "name": "Dillon H. Murphy"
    },
    {
      "id": "2140535196",
      "type": "author",
      "name": "Kara M. Hoover"
    },
    {
      "id": "2087438361",
      "type": "author",
      "name": "K. Agadzhanyan"
    },
    {
      "id": "2140530383",
      "type": "author",
      "name": "Jesse C. Kuehn"
    },
    {
      "id": "3484063",
      "type": "author",
      "name": "A. Castel"
    },
    {
      "id": "9e20f3155d16e13a16f3614a638207d67a7f4a5f",
      "type": "paper",
      "title": "Video‐triggered EEG‐emotion public databases and current methods: A survey",
      "abstract": "Emotions, formed in the process of perceiving external environment, directly affect human daily life, such as social interaction, work efficiency, physical wellness, and mental health. In recent decades, emotion recognition has become a promising research direction with significant application values. Taking the advantages of electroencephalogram (EEG) signals (i.e., high time resolution) and video‐based external emotion evoking (i.e., rich media information), video‐triggered emotion recognition with EEG signals has been proven as a useful tool to conduct emotion‐related studies in a laboratory environment, which provides constructive technical supports for establishing real‐time emotion interaction systems. In this paper, we will focus on video‐triggered EEG‐based emotion recognition and present a systematical introduction of the current available video‐triggered EEG‐based emotion databases with the corresponding analysis methods. First, current video‐triggered EEG databases for emotion recognition (e.g., DEAP, MAHNOB‐HCI, SEED series databases) will be presented with full details. Then, the commonly used EEG feature extraction, feature selection, and modeling methods in video‐triggered EEG‐based emotion recognition will be systematically summarized and a brief review of current situation about video‐triggered EEG‐based emotion studies will be provided. Finally, the limitations and possible prospects of the existing video‐triggered EEG‐emotion databases will be fully discussed.",
      "year": 2020,
      "venue": "Brain Science Advances",
      "url": "https://www.semanticscholar.org/paper/9e20f3155d16e13a16f3614a638207d67a7f4a5f",
      "citation_count": 34,
      "reference_count": 132,
      "fields_of_study": "",
      "is_open_access": true,
      "tldr": "This paper presents a systematical introduction of the current available video‐triggered EEG‐based emotion recognition databases with the corresponding analysis methods and systematically summarized the commonly used EEG feature extraction, feature selection, and modeling methods.",
      "open_access_pdf_url": "https://journals.sagepub.com/doi/pdf/10.26599/BSA.2020.9050026",
      "open_access_status": "GOLD",
      "external_id_doi": "10.26599/BSA.2020.9050026",
      "external_id_corpusid": 231821748
    },
    {
      "id": "2026684421",
      "type": "author",
      "name": "Wanrou Hu"
    },
    {
      "id": "50775285",
      "type": "author",
      "name": "G. Huang"
    },
    {
      "id": "3402485",
      "type": "author",
      "name": "Linling Li"
    },
    {
      "id": "2152827533",
      "type": "author",
      "name": "Li Zhang"
    },
    {
      "id": "2116219842",
      "type": "author",
      "name": "Zhiguo Zhang"
    },
    {
      "id": "2087124567",
      "type": "author",
      "name": "Zhen Liang"
    },
    {
      "id": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "paper",
      "title": "Mistral 7B",
      "abstract": "We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",
      "year": 2023,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "citation_count": 1590,
      "reference_count": 29,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "This work introduces Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency, which leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost.",
      "open_access_pdf_url": "https://arxiv.org/pdf/2310.06825",
      "open_access_status": "CLOSED",
      "external_id_arxiv": "2310.06825",
      "external_id_dblp": "journals/corr/abs-2310-06825",
      "external_id_doi": "10.48550/arXiv.2310.06825",
      "external_id_corpusid": 263830494
    },
    {
      "id": "2063969818",
      "type": "author",
      "name": "Albert Qiaochu Jiang"
    },
    {
      "id": "2256994781",
      "type": "author",
      "name": "Alexandre Sablayrolles"
    },
    {
      "id": "1697879",
      "type": "author",
      "name": "A. Mensch"
    },
    {
      "id": "2256994975",
      "type": "author",
      "name": "Chris Bamford"
    },
    {
      "id": "2328602",
      "type": "author",
      "name": "Devendra Singh Chaplot"
    },
    {
      "id": "40550616",
      "type": "author",
      "name": "Diego de Las Casas"
    },
    {
      "id": "2256995640",
      "type": "author",
      "name": "Florian Bressand"
    },
    {
      "id": "2256993163",
      "type": "author",
      "name": "Gianna Lengyel"
    },
    {
      "id": "2113836860",
      "type": "author",
      "name": "Lucile Saulnier"
    },
    {
      "id": "2256995632",
      "type": "author",
      "name": "L'elio Renard Lavaud"
    },
    {
      "id": "2256994779",
      "type": "author",
      "name": "Pierre Stock"
    },
    {
      "id": "1379806208",
      "type": "author",
      "name": "Teven Le Scao"
    },
    {
      "id": "2135734748",
      "type": "author",
      "name": "Thomas Wang"
    },
    {
      "id": "2256992826",
      "type": "author",
      "name": "William El Sayed"
    },
    {
      "id": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "paper",
      "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
      "year": 2023,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "citation_count": 9374,
      "reference_count": 131,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work develops and releases Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters, which may be a suitable substitute for closed-source models.",
      "external_id_arxiv": "2307.09288",
      "external_id_dblp": "journals/corr/abs-2307-09288",
      "external_id_corpusid": 259950998
    },
    {
      "id": "143792623",
      "type": "author",
      "name": "Louis Martin"
    },
    {
      "id": "2059203763",
      "type": "author",
      "name": "Kevin R. Stone"
    },
    {
      "id": "2214809450",
      "type": "author",
      "name": "Peter Albert"
    },
    {
      "id": "2634674",
      "type": "author",
      "name": "Amjad Almahairi"
    },
    {
      "id": "2023469",
      "type": "author",
      "name": "D. Bikel"
    },
    {
      "id": "2108267192",
      "type": "author",
      "name": "Moya Chen"
    },
    {
      "id": "7153363",
      "type": "author",
      "name": "Guillem Cucurull"
    },
    {
      "id": "2166312768",
      "type": "author",
      "name": "Jude Fernandes"
    },
    {
      "id": "2223748737",
      "type": "author",
      "name": "Brian Fuller"
    },
    {
      "id": "2107063269",
      "type": "author",
      "name": "Cynthia Gao"
    },
    {
      "id": "4305645",
      "type": "author",
      "name": "A. Hartshorn"
    },
    {
      "id": "2195458",
      "type": "author",
      "name": "Saghar Hosseini"
    },
    {
      "id": "2132302721",
      "type": "author",
      "name": "Rui Hou"
    },
    {
      "id": "2065277797",
      "type": "author",
      "name": "Hakan Inan"
    },
    {
      "id": "2135297476",
      "type": "author",
      "name": "A. Korenev"
    },
    {
      "id": "1768032",
      "type": "author",
      "name": "Yinghai Lu"
    },
    {
      "id": "3375249",
      "type": "author",
      "name": "Yuning Mao"
    },
    {
      "id": "3047561",
      "type": "author",
      "name": "Pushkar Mishra"
    },
    {
      "id": "40383658",
      "type": "author",
      "name": "Yixin Nie"
    },
    {
      "id": "38579672",
      "type": "author",
      "name": "Andrew Poulton"
    },
    {
      "id": "150282885",
      "type": "author",
      "name": "Rashi Rungta"
    },
    {
      "id": "1859294",
      "type": "author",
      "name": "Kalyan Saladi"
    },
    {
      "id": "51324296",
      "type": "author",
      "name": "Eric Michael Smith"
    },
    {
      "id": "2066074360",
      "type": "author",
      "name": "R. Subramanian"
    },
    {
      "id": "2112782199",
      "type": "author",
      "name": "Xia Tan"
    },
    {
      "id": "71292072",
      "type": "author",
      "name": "Binh Tang"
    },
    {
      "id": "2110032535",
      "type": "author",
      "name": "Adina Williams"
    },
    {
      "id": "2223770369",
      "type": "author",
      "name": "Jian Xiang Kuan"
    },
    {
      "id": "14701107",
      "type": "author",
      "name": "Zhengxu Yan"
    },
    {
      "id": "144270981",
      "type": "author",
      "name": "Angela Fan"
    },
    {
      "id": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
      "type": "paper",
      "title": "Training language models to follow instructions with human feedback",
      "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
      "year": 2022,
      "venue": "Neural Information Processing Systems",
      "url": "https://www.semanticscholar.org/paper/d766bffc357127e0dc86dd69561d5aeb520d6f4c",
      "citation_count": 10391,
      "reference_count": 83,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent and showing improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets.",
      "external_id_dblp": "conf/nips/Ouyang0JAWMZASR22",
      "external_id_arxiv": "2203.02155",
      "external_id_corpusid": 246426909
    },
    {
      "id": "31793034",
      "type": "author",
      "name": "Long Ouyang"
    },
    {
      "id": "49387725",
      "type": "author",
      "name": "Jeff Wu"
    },
    {
      "id": "2115903168",
      "type": "author",
      "name": "Xu Jiang"
    },
    {
      "id": "2061137049",
      "type": "author",
      "name": "Diogo Almeida"
    },
    {
      "id": "2064084601",
      "type": "author",
      "name": "Carroll L. Wainwright"
    },
    {
      "id": "814a4f680b9ba6baba23b93499f4b48af1a27678",
      "type": "paper",
      "title": "Measuring Massive Multitask Language Understanding",
      "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
      "year": 2020,
      "venue": "International Conference on Learning Representations",
      "url": "https://www.semanticscholar.org/paper/814a4f680b9ba6baba23b93499f4b48af1a27678",
      "citation_count": 3121,
      "reference_count": 35,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "While most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average, however, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy.",
      "external_id_dblp": "conf/iclr/HendrycksBBZMSS21",
      "external_id_arxiv": "2009.03300",
      "external_id_mag": "3083410900",
      "external_id_corpusid": 221516475
    },
    {
      "id": "3422872",
      "type": "author",
      "name": "Dan Hendrycks"
    },
    {
      "id": "90909974",
      "type": "author",
      "name": "Collin Burns"
    },
    {
      "id": "104444594",
      "type": "author",
      "name": "Steven Basart"
    },
    {
      "id": "1380103052",
      "type": "author",
      "name": "Andy Zou"
    },
    {
      "id": "16787428",
      "type": "author",
      "name": "Mantas Mazeika"
    },
    {
      "id": "143711382",
      "type": "author",
      "name": "D. Song"
    },
    {
      "id": "5164568",
      "type": "author",
      "name": "J. Steinhardt"
    },
    {
      "id": "295065d942abca0711300b2b4c39829551060578",
      "type": "paper",
      "title": "BERTScore: Evaluating Text Generation with BERT",
      "abstract": "We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.",
      "year": 2019,
      "venue": "International Conference on Learning Representations",
      "url": "https://www.semanticscholar.org/paper/295065d942abca0711300b2b4c39829551060578",
      "citation_count": 4929,
      "reference_count": 104,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work proposes BERTScore, an automatic evaluation metric for text generation that correlates better with human judgments and provides stronger model selection performance than existing metrics.",
      "external_id_mag": "2936695845",
      "external_id_arxiv": "1904.09675",
      "external_id_dblp": "journals/corr/abs-1904-09675",
      "external_id_corpusid": 127986044
    },
    {
      "id": "123437034",
      "type": "author",
      "name": "Tianyi Zhang"
    },
    {
      "id": "145461044",
      "type": "author",
      "name": "Varsha Kishore"
    },
    {
      "id": "24277779",
      "type": "author",
      "name": "Felix Wu"
    },
    {
      "id": "7446832",
      "type": "author",
      "name": "Kilian Q. Weinberger"
    },
    {
      "id": "3167681",
      "type": "author",
      "name": "Yoav Artzi"
    },
    {
      "id": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
      "type": "paper",
      "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
      "abstract": "We present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1326 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topic—in the context of common knowledge—and the language it is expressed in. Human performance on OpenBookQA is close to 92%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.",
      "year": 2018,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "url": "https://www.semanticscholar.org/paper/1536e8958697c5364f68b2e2448905dbbeb3a0ca",
      "citation_count": 1223,
      "reference_count": 53,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "A new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject, and oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts.",
      "open_access_pdf_url": "https://www.aclweb.org/anthology/D18-1260.pdf",
      "open_access_status": "HYBRID",
      "external_id_dblp": "journals/corr/abs-1809-02789",
      "external_id_mag": "2952396187",
      "external_id_acl": "D18-1260",
      "external_id_arxiv": "1809.02789",
      "external_id_doi": "10.18653/v1/D18-1260",
      "external_id_corpusid": 52183757
    },
    {
      "id": "48323507",
      "type": "author",
      "name": "Peter Clark"
    },
    {
      "id": "2236429",
      "type": "author",
      "name": "Tushar Khot"
    },
    {
      "id": "48229640",
      "type": "author",
      "name": "Ashish Sabharwal"
    },
    {
      "id": "751b04d346fc9a70781bbfea23953f424ff7deec",
      "type": "paper",
      "title": "The proof and measurement of association between two things.",
      "abstract": "All knowledge—beyond that of bare isolated occurrence—deals with uniformities. Of the latter, some few have a claim to be considered absolute, such as mathematical implications and mechanical laws. But the vast majority are only partial; medicine does not teach that smallpox is inevitably escaped by vaccination, but that it is so generally; biology has not shown that all animals require organic food, but that nearly all do so; in daily life, a dark sky is no proof that it will rain, but merely a warning; even in morality, the sole categorical imperative alleged by Kant was the sinfulness of telling a lie, and few thinkers since have admitted so much as this to be valid universally. In psychology, more perhaps than in any other science, it is hard to find absolutely inflexible coincidences; occasionally, indeed, there appear uniformities sufficiently regular to be practically treated as laws, but infinitely the greater part of the observations hitherto recorded concern only more or less pronounced tendencies of one event or attribute to accompany another. Under these circumstances, one might well have expected that the evidential evaluation and precise mensuration of tendencies had long been the subject of exhaustive investigation and now formed one of the earliest sections in a beginner’s psychological course. Instead, we find only a general naı̈ve ignorance that there is anything about it requiring to be learnt. One after another, laborious series of experiments are executed and published with the purpose of demonstrating some connection between two events, wherein the otherwise learned psychologist reveals that his art of proving and measuring correspondence has not advanced beyond that of lay persons. The consequence has been that the significance of the experiments is not at all rightly understood, nor have any definite facts been elicited that may be either confirmed or refuted. The present article is a commencement at attempting to remedy this deficiency of scientific correlation. With this view, it will be strictly confined to the needs of practical workers, and all theoretical mathematical demonstrations will be omitted; it may, however, be said that the relations stated have already received a large amount of empirical verification. Great thanks are due from me to Professor Haussdorff and to Dr. G. Lipps, each of whom have supplied a useful theorem in polynomial probability; the former has also very kindly given valuable advice concerning the proof of the important formulæ for elimination of ‘‘systematic deviations.’’ At the same time, and for the same reason, the meaning and working of the various formulæ have been explained sufficiently, it is hoped, to render them readily usable even by those whose knowledge of mathematics is elementary. The fundamental procedure is accompanied by simple imaginary examples, while the more advanced parts are illustrated by cases that have actually occurred in my personal experience. For more abundant and positive exemplification, the reader is requested to refer to the under cited research, which is entirely built upon the principles and mathematical relations here laid down. In conclusion, the general value of the methodics recommended is emphasized by a brief criticism of the best correlational work hitherto made public, and also the important question is discussed as to the number of ‘‘cases’’ required for an experimental series.",
      "year": 2015,
      "venue": "International Journal of Epidemiology",
      "url": "https://www.semanticscholar.org/paper/751b04d346fc9a70781bbfea23953f424ff7deec",
      "citation_count": 4835,
      "reference_count": 1,
      "fields_of_study": "[\"Psychology\", \"Medicine\"]",
      "is_open_access": true,
      "tldr": "The present article is a commencement at attempting to remedy this deficiency of scientific correlation, and the meaning and working of the various formulæ have been explained sufficiently, it is hoped, to render them readily usable even by those whose knowledge of mathematics is elementary.",
      "open_access_pdf_url": "https://archive.org/download/proofmeasurement00speauoft/proofmeasurement00speauoft_bw.pdf",
      "open_access_status": "BRONZE",
      "external_id_mag": "2151967815",
      "external_id_doi": "10.1093/ije/dyq191",
      "external_id_corpusid": 14780428,
      "external_id_pubmed": "21051364"
    },
    {
      "id": "72898556",
      "type": "author",
      "name": "C. Spearman"
    },
    {
      "id": "d7da009f457917aa381619facfa5ffae9329a6e9",
      "type": "paper",
      "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
      "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.",
      "year": 2002,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "url": "https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9",
      "citation_count": 26465,
      "reference_count": 6,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.",
      "open_access_pdf_url": "https://dl.acm.org/doi/pdf/10.3115/1073083.1073135",
      "open_access_status": "BRONZE",
      "external_id_dblp": "conf/acl/PapineniRWZ02",
      "external_id_mag": "2101105183",
      "external_id_acl": "P02-1040",
      "external_id_doi": "10.3115/1073083.1073135",
      "external_id_corpusid": 11080756
    },
    {
      "id": "3323275",
      "type": "author",
      "name": "Kishore Papineni"
    },
    {
      "id": "46924970",
      "type": "author",
      "name": "Salim Roukos"
    },
    {
      "id": "144582029",
      "type": "author",
      "name": "T. Ward"
    },
    {
      "id": "2587983",
      "type": "author",
      "name": "Wei-Jing Zhu"
    },
    {
      "id": "7235707434df77b0469e6da21e93a27f250870eb",
      "type": "paper",
      "title": "Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks",
      "abstract": "The potential of using Large Language Models (LLMs) themselves to evaluate LLM outputs offers a promising method for assessing model performance across various contexts. Previous research indicates that LLM-as-a-judge exhibits a strong correlation with human judges in the context of general instruction following. However, for instructions that require specialized knowledge, the validity of using LLMs as judges remains uncertain. In our study, we applied a mixed-methods approach, conducting pairwise comparisons in which both subject matter experts (SMEs) and LLMs evaluated outputs from domain-specific tasks. We focused on two distinct fields: dietetics, with registered dietitian experts, and mental health, with clinical psychologist experts. Our results showed that SMEs agreed with LLM judges 68% of the time in the dietetics domain and 64% in mental health when evaluating overall preference. Additionally, the results indicated variations in SME-LLM agreement across domain-specific aspect questions. Our findings emphasize the importance of keeping human experts in the evaluation process, as LLMs alone may not provide the depth of understanding required for complex, knowledge specific tasks. We also explore the implications of LLM evaluations across different domains and discuss how these insights can inform the design of evaluation workflows that ensure better alignment between human experts and LLMs in interactive systems.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/7235707434df77b0469e6da21e93a27f250870eb",
      "citation_count": 2,
      "reference_count": 56,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The results showed that SMEs agreed with LLM judges 68% of the time in the dietetics domain and 64% in mental health when evaluating overall preference, and variations in SME-LLM agreement across domain-specific aspect questions indicated variations in SME-LLM agreement across domain-specific aspect questions.",
      "external_id_arxiv": "2410.20266",
      "external_id_dblp": "journals/corr/abs-2410-20266",
      "external_id_doi": "10.48550/arXiv.2410.20266",
      "external_id_corpusid": 273654506
    },
    {
      "id": "2301055584",
      "type": "author",
      "name": "Annalisa Szymanski"
    },
    {
      "id": "2264184691",
      "type": "author",
      "name": "Noah Ziems"
    },
    {
      "id": "1402296287",
      "type": "author",
      "name": "H. Eicher-Miller"
    },
    {
      "id": "2261394820",
      "type": "author",
      "name": "Toby Jia-Jun Li"
    },
    {
      "id": "2325501478",
      "type": "author",
      "name": "Meng Jiang"
    },
    {
      "id": "2273093960",
      "type": "author",
      "name": "Ronald A. Metoyer"
    },
    {
      "id": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "paper",
      "title": "GPT-4o System Card",
      "abstract": "GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/7943ec4a67151a559b25cd34369e661c9a7924c8",
      "citation_count": 128,
      "reference_count": 56,
      "fields_of_study": "[\"Computer Science\", \"Engineering\"]",
      "is_open_access": false,
      "tldr": "This System Card provides a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures the authors've implemented to ensure the model is safe and aligned.",
      "external_id_arxiv": "2410.21276",
      "external_id_dblp": "journals/corr/abs-2410-21276",
      "external_id_doi": "10.48550/arXiv.2410.21276",
      "external_id_corpusid": 273662196
    },
    {
      "id": "2328092058",
      "type": "author",
      "name": "OpenAI Aaron Hurst"
    },
    {
      "id": "1977806",
      "type": "author",
      "name": "Adam Lerer"
    },
    {
      "id": "1964382",
      "type": "author",
      "name": "Adam P. Goucher"
    },
    {
      "id": "2293772267",
      "type": "author",
      "name": "Aidan Clark"
    },
    {
      "id": "2328089416",
      "type": "author",
      "name": "AJ Ostrow"
    },
    {
      "id": "2276823919",
      "type": "author",
      "name": "Alan Hayes"
    },
    {
      "id": "1557541414",
      "type": "author",
      "name": "Aleksander Mkadry"
    },
    {
      "id": "2328088295",
      "type": "author",
      "name": "Alex Baker-Whitcomb"
    },
    {
      "id": "2297773170",
      "type": "author",
      "name": "Alex Beutel"
    },
    {
      "id": "2328092088",
      "type": "author",
      "name": "Alex Borzunov"
    },
    {
      "id": "2328088996",
      "type": "author",
      "name": "Alex Carney"
    },
    {
      "id": "2328088630",
      "type": "author",
      "name": "Alex Chow"
    },
    {
      "id": "2328088972",
      "type": "author",
      "name": "Alexander Kirillov"
    },
    {
      "id": "38967461",
      "type": "author",
      "name": "Alex Nichol"
    },
    {
      "id": "2328089001",
      "type": "author",
      "name": "Alex Renzin"
    },
    {
      "id": "2328086314",
      "type": "author",
      "name": "Alexi Christakis"
    },
    {
      "id": "2288154316",
      "type": "author",
      "name": "Alexis Conneau"
    },
    {
      "id": "2304796773",
      "type": "author",
      "name": "Allan Jabri"
    },
    {
      "id": "2328086873",
      "type": "author",
      "name": "Allison Moyer"
    },
    {
      "id": "2328088190",
      "type": "author",
      "name": "Allison Tam"
    },
    {
      "id": "2328077794",
      "type": "author",
      "name": "Amadou Crookes"
    },
    {
      "id": "2328088272",
      "type": "author",
      "name": "Amin Tootoochian"
    },
    {
      "id": "2286810186",
      "type": "author",
      "name": "Ananya Kumar"
    },
    {
      "id": "2354728",
      "type": "author",
      "name": "A. Karpathy"
    },
    {
      "id": "2328089401",
      "type": "author",
      "name": "Andrew Braunstein"
    },
    {
      "id": "2328086274",
      "type": "author",
      "name": "Andrew Codispoti"
    },
    {
      "id": "2328086861",
      "type": "author",
      "name": "Andrew Galu"
    },
    {
      "id": "3609856",
      "type": "author",
      "name": "Andrew Tulloch"
    },
    {
      "id": "2328086268",
      "type": "author",
      "name": "Angela Baek"
    },
    {
      "id": "2328088910",
      "type": "author",
      "name": "Antoine Pelisse"
    },
    {
      "id": "2328091197",
      "type": "author",
      "name": "Antonia Woodford"
    },
    {
      "id": "2328087875",
      "type": "author",
      "name": "Anuj Gosalia"
    },
    {
      "id": "2328085045",
      "type": "author",
      "name": "Avi Nayak"
    },
    {
      "id": "2328088314",
      "type": "author",
      "name": "Avital Oliver"
    },
    {
      "id": "47475538",
      "type": "author",
      "name": "B. Ghorbani"
    },
    {
      "id": "2328086263",
      "type": "author",
      "name": "Ben Leimberger"
    },
    {
      "id": "2328087747",
      "type": "author",
      "name": "Ben Rossen"
    },
    {
      "id": "2328086866",
      "type": "author",
      "name": "Benjamin Zweig"
    },
    {
      "id": "2328088700",
      "type": "author",
      "name": "Beth Hoover"
    },
    {
      "id": "100549595",
      "type": "author",
      "name": "B. Samic"
    },
    {
      "id": "2328088963",
      "type": "author",
      "name": "Bobby Spero"
    },
    {
      "id": "2328087787",
      "type": "author",
      "name": "Bogo Giertler"
    },
    {
      "id": "2328306274",
      "type": "author",
      "name": "Bowen Cheng"
    },
    {
      "id": "2328088277",
      "type": "author",
      "name": "Brad Lightcap"
    },
    {
      "id": "2328088135",
      "type": "author",
      "name": "Brandon Walkin"
    },
    {
      "id": "2328089502",
      "type": "author",
      "name": "Brendan Quinn"
    },
    {
      "id": "3077501",
      "type": "author",
      "name": "Brian Guarraci"
    },
    {
      "id": "2328086363",
      "type": "author",
      "name": "Brian Hsu"
    },
    {
      "id": "2328087880",
      "type": "author",
      "name": "Bright Kellogg"
    },
    {
      "id": "2328086892",
      "type": "author",
      "name": "Brydon Eastman"
    },
    {
      "id": "47757859",
      "type": "author",
      "name": "Camillo Lugaresi"
    },
    {
      "id": "2328086506",
      "type": "author",
      "name": "Cary Bassin"
    },
    {
      "id": "2328089025",
      "type": "author",
      "name": "Cary Hudson"
    },
    {
      "id": "2328082349",
      "type": "author",
      "name": "Chad Nelson"
    },
    {
      "id": "2162465097",
      "type": "author",
      "name": "Chan Jun Shern"
    },
    {
      "id": "2328088805",
      "type": "author",
      "name": "Channing Conger"
    },
    {
      "id": "2328088258",
      "type": "author",
      "name": "Charlotte Barette"
    },
    {
      "id": "2328262416",
      "type": "author",
      "name": "Chen Ding"
    },
    {
      "id": "2328601582",
      "type": "author",
      "name": "Cheng Lu"
    },
    {
      "id": "2328087829",
      "type": "author",
      "name": "Chris Beaumont"
    },
    {
      "id": "2328928508",
      "type": "author",
      "name": "Chris Koch"
    },
    {
      "id": "2328273156",
      "type": "author",
      "name": "Christine Choi"
    },
    {
      "id": "2328272761",
      "type": "author",
      "name": "Claudia Fischer"
    },
    {
      "id": "2328086878",
      "type": "author",
      "name": "Coley Czarnecki"
    },
    {
      "id": "2328110341",
      "type": "author",
      "name": "Colin Jarvis"
    },
    {
      "id": "2328340618",
      "type": "author",
      "name": "Colin Wei"
    },
    {
      "id": "2328088908",
      "type": "author",
      "name": "Constantin Koumouzelis"
    },
    {
      "id": "2231409794",
      "type": "author",
      "name": "Dane Sherburn"
    },
    {
      "id": "2328089763",
      "type": "author",
      "name": "Daniel Kappler"
    },
    {
      "id": "2328080408",
      "type": "author",
      "name": "Daniel Levin"
    },
    {
      "id": "2328090011",
      "type": "author",
      "name": "David Carr"
    },
    {
      "id": "2319226404",
      "type": "author",
      "name": "David Mély"
    },
    {
      "id": "2328312366",
      "type": "author",
      "name": "David Robinson"
    },
    {
      "id": "2328088056",
      "type": "author",
      "name": "David Sasaki"
    },
    {
      "id": "2328088261",
      "type": "author",
      "name": "Dev Valladares"
    },
    {
      "id": "2754804",
      "type": "author",
      "name": "Dimitris Tsipras"
    },
    {
      "id": "2328108199",
      "type": "author",
      "name": "Doug Li"
    },
    {
      "id": "152842363",
      "type": "author",
      "name": "Phong Duc Nguyen"
    },
    {
      "id": "2328086940",
      "type": "author",
      "name": "Duncan Findlay"
    },
    {
      "id": "2328087781",
      "type": "author",
      "name": "Edede Oiwoh"
    },
    {
      "id": "2328091414",
      "type": "author",
      "name": "Edmund Wong"
    },
    {
      "id": "1659172809",
      "type": "author",
      "name": "Ehsan Asdar"
    },
    {
      "id": "2328550909",
      "type": "author",
      "name": "Elizabeth Yang"
    },
    {
      "id": "2328086947",
      "type": "author",
      "name": "Eric Antonow"
    },
    {
      "id": "2328088981",
      "type": "author",
      "name": "Eric Kramer"
    },
    {
      "id": "2328386420",
      "type": "author",
      "name": "Eric Peterson"
    },
    {
      "id": "2297774080",
      "type": "author",
      "name": "Eric Wallace"
    },
    {
      "id": "2165941083",
      "type": "author",
      "name": "E. Brevdo"
    },
    {
      "id": "2325158061",
      "type": "author",
      "name": "Evan Mays"
    },
    {
      "id": "2328088811",
      "type": "author",
      "name": "Farzad Khorasani"
    },
    {
      "id": "2328088712",
      "type": "author",
      "name": "Filippo Raso"
    },
    {
      "id": "2328106205",
      "type": "author",
      "name": "Francis Zhang"
    },
    {
      "id": "2285927942",
      "type": "author",
      "name": "Fred von Lohmann"
    },
    {
      "id": "11948938",
      "type": "author",
      "name": "Freddie Sulit"
    },
    {
      "id": "2328107429",
      "type": "author",
      "name": "Gene Oden"
    },
    {
      "id": "2328088679",
      "type": "author",
      "name": "Geoff Salmon"
    },
    {
      "id": "2168285763",
      "type": "author",
      "name": "Giulio Starace"
    },
    {
      "id": "2328086469",
      "type": "author",
      "name": "Hadi Salman"
    },
    {
      "id": "2290040262",
      "type": "author",
      "name": "Hai-Biao Bao"
    },
    {
      "id": "2328104851",
      "type": "author",
      "name": "Haitang Hu"
    },
    {
      "id": "2328340186",
      "type": "author",
      "name": "Haoyu Wang"
    },
    {
      "id": "2328088790",
      "type": "author",
      "name": "Heather Whitney"
    },
    {
      "id": "2307453621",
      "type": "author",
      "name": "Heewoo Jun"
    },
    {
      "id": "2282935403",
      "type": "author",
      "name": "Hongyu Ren"
    },
    {
      "id": "2328338413",
      "type": "author",
      "name": "Huiwen Chang"
    },
    {
      "id": "24643287",
      "type": "author",
      "name": "I. Kivlichan"
    },
    {
      "id": "2328088813",
      "type": "author",
      "name": "Ian O’Connell"
    },
    {
      "id": "2561924",
      "type": "author",
      "name": "Ian Osband"
    },
    {
      "id": "2328089393",
      "type": "author",
      "name": "Ian Silber"
    },
    {
      "id": "102707868",
      "type": "author",
      "name": "İ. Okuyucu"
    },
    {
      "id": "2000906",
      "type": "author",
      "name": "Ilya Kostrikov"
    },
    {
      "id": "2708454",
      "type": "author",
      "name": "Ishaan Gulrajani"
    },
    {
      "id": "2328088993",
      "type": "author",
      "name": "Jacob Coxon"
    },
    {
      "id": "2325157923",
      "type": "author",
      "name": "James Aung"
    },
    {
      "id": "2187579768",
      "type": "author",
      "name": "James Betker"
    },
    {
      "id": "2328088092",
      "type": "author",
      "name": "James Crooks"
    },
    {
      "id": "2328088895",
      "type": "author",
      "name": "James Lennon"
    },
    {
      "id": "2309210464",
      "type": "author",
      "name": "Jan Leike"
    },
    {
      "id": "2275788749",
      "type": "author",
      "name": "Jane Park"
    },
    {
      "id": "2328244583",
      "type": "author",
      "name": "Jason Kwon"
    },
    {
      "id": "2241609611",
      "type": "author",
      "name": "Jason Phang"
    },
    {
      "id": "2328088965",
      "type": "author",
      "name": "Jason Teplitz"
    },
    {
      "id": "2256302782",
      "type": "author",
      "name": "Jason Wei"
    },
    {
      "id": "2328089547",
      "type": "author",
      "name": "Jason Wolfe"
    },
    {
      "id": "2328106487",
      "type": "author",
      "name": "Jay Chen"
    },
    {
      "id": "2328088802",
      "type": "author",
      "name": "Jenia Varavva"
    },
    {
      "id": "2328319492",
      "type": "author",
      "name": "Jessica Gan Lee"
    },
    {
      "id": "2328261167",
      "type": "author",
      "name": "Ji Lin"
    },
    {
      "id": "2329018321",
      "type": "author",
      "name": "Jiahui Yu"
    },
    {
      "id": "2328247147",
      "type": "author",
      "name": "Jieqi Yu"
    },
    {
      "id": "3177568",
      "type": "author",
      "name": "J. Q. Candela"
    },
    {
      "id": "2328086498",
      "type": "author",
      "name": "Joe Beutler"
    },
    {
      "id": "2328089222",
      "type": "author",
      "name": "Joe Landers"
    },
    {
      "id": "2297873691",
      "type": "author",
      "name": "John Schulman"
    },
    {
      "id": "2328088675",
      "type": "author",
      "name": "Jonathan Lachman"
    },
    {
      "id": "2328087850",
      "type": "author",
      "name": "Jonathan McKay"
    },
    {
      "id": "9960452",
      "type": "author",
      "name": "Jonathan Uesato"
    },
    {
      "id": "2328087765",
      "type": "author",
      "name": "Jos Kraaijeveld"
    },
    {
      "id": "2328081248",
      "type": "author",
      "name": "Josh Kaplan"
    },
    {
      "id": "2328089934",
      "type": "author",
      "name": "Josh Snyder"
    },
    {
      "id": "2318703608",
      "type": "author",
      "name": "Josh Achiam"
    },
    {
      "id": "2328078075",
      "type": "author",
      "name": "Joy Jiao"
    },
    {
      "id": "2328108630",
      "type": "author",
      "name": "Joyce Lee"
    },
    {
      "id": "2328089008",
      "type": "author",
      "name": "Justyn Harriman"
    },
    {
      "id": "2328086333",
      "type": "author",
      "name": "Kai Fricke"
    },
    {
      "id": "2328370280",
      "type": "author",
      "name": "Kai Hayashi"
    },
    {
      "id": "2328110126",
      "type": "author",
      "name": "Karan Singhal"
    },
    {
      "id": "2328088657",
      "type": "author",
      "name": "Katy Shi"
    },
    {
      "id": "2328088401",
      "type": "author",
      "name": "Kavin Karthik"
    },
    {
      "id": "2328090052",
      "type": "author",
      "name": "Kayla Wood"
    },
    {
      "id": "2328346829",
      "type": "author",
      "name": "Kenny Nguyen"
    },
    {
      "id": "2275176580",
      "type": "author",
      "name": "Keren Gu-Lemberg"
    },
    {
      "id": "2325207065",
      "type": "author",
      "name": "Kevin Liu"
    },
    {
      "id": "2328087131",
      "type": "author",
      "name": "Kiel Howe"
    },
    {
      "id": "2254898448",
      "type": "author",
      "name": "K. Muthukumar"
    },
    {
      "id": "2328086322",
      "type": "author",
      "name": "Kyle Luther"
    },
    {
      "id": "2328089844",
      "type": "author",
      "name": "Larry Kai"
    },
    {
      "id": "2328089080",
      "type": "author",
      "name": "Lauren Itow"
    },
    {
      "id": "2328086398",
      "type": "author",
      "name": "Leher Pathak"
    },
    {
      "id": "2328092675",
      "type": "author",
      "name": "Leo Chen"
    },
    {
      "id": "2260978400",
      "type": "author",
      "name": "Li Jing"
    },
    {
      "id": "2328087145",
      "type": "author",
      "name": "Lia Guy"
    },
    {
      "id": "2328113403",
      "type": "author",
      "name": "Liang Zhou"
    },
    {
      "id": "2328086391",
      "type": "author",
      "name": "Lien Mamitsuka"
    },
    {
      "id": "2325162875",
      "type": "author",
      "name": "Lilian Weng"
    },
    {
      "id": "2328089100",
      "type": "author",
      "name": "Lindsay McCallum"
    },
    {
      "id": "2328111423",
      "type": "author",
      "name": "Lindsey Held"
    },
    {
      "id": "2328088792",
      "type": "author",
      "name": "Louis Feuvrier"
    },
    {
      "id": "2328457059",
      "type": "author",
      "name": "Lu Zhang"
    },
    {
      "id": "2320728017",
      "type": "author",
      "name": "Luke Hewitt"
    },
    {
      "id": "2099682093",
      "type": "author",
      "name": "Lyric Doshi"
    },
    {
      "id": "2328107693",
      "type": "author",
      "name": "Mada Aflak"
    },
    {
      "id": "3193064",
      "type": "author",
      "name": "Marat Dukhan"
    },
    {
      "id": "2328328041",
      "type": "author",
      "name": "Mark Gray"
    },
    {
      "id": "2272599912",
      "type": "author",
      "name": "M. Hudnall"
    },
    {
      "id": "2328090291",
      "type": "author",
      "name": "Marwan Aljubeh"
    },
    {
      "id": "2328090151",
      "type": "author",
      "name": "Matthew Zeng"
    },
    {
      "id": "2328280032",
      "type": "author",
      "name": "Max Johnson"
    },
    {
      "id": "2328089159",
      "type": "author",
      "name": "Maya Shetty"
    },
    {
      "id": "2328258887",
      "type": "author",
      "name": "Mayank Gupta"
    },
    {
      "id": "2328959571",
      "type": "author",
      "name": "Meghan Shah"
    },
    {
      "id": "3308772",
      "type": "author",
      "name": "M. Yatbaz"
    },
    {
      "id": "2305476965",
      "type": "author",
      "name": "Mengxue Yang"
    },
    {
      "id": "2328090097",
      "type": "author",
      "name": "Mengchao Zhong"
    },
    {
      "id": "2275287759",
      "type": "author",
      "name": "Mianna Chen"
    },
    {
      "id": "35163402",
      "type": "author",
      "name": "Michael Janner"
    },
    {
      "id": "2328092228",
      "type": "author",
      "name": "Michele Wang"
    },
    {
      "id": "2328089085",
      "type": "author",
      "name": "Michelle Fradin"
    },
    {
      "id": "2328824976",
      "type": "author",
      "name": "Miguel Castro"
    },
    {
      "id": "2265097787",
      "type": "author",
      "name": "M. Brundage"
    },
    {
      "id": "2328091362",
      "type": "author",
      "name": "Miles Wang"
    },
    {
      "id": "2328086997",
      "type": "author",
      "name": "Murat Yesildal"
    },
    {
      "id": "2328089777",
      "type": "author",
      "name": "Nacho Soto"
    },
    {
      "id": "3365851",
      "type": "author",
      "name": "N. Gimelshein"
    },
    {
      "id": "2328086984",
      "type": "author",
      "name": "Natalie Cone"
    },
    {
      "id": "2328089359",
      "type": "author",
      "name": "Natan LaFontaine"
    },
    {
      "id": "2325154459",
      "type": "author",
      "name": "Neil Chowdhury"
    },
    {
      "id": "2133298112",
      "type": "author",
      "name": "Nickolas Stathas"
    },
    {
      "id": "2328086444",
      "type": "author",
      "name": "Nithanth Kudige"
    },
    {
      "id": "2328086448",
      "type": "author",
      "name": "Noel Bundick"
    },
    {
      "id": "2328087770",
      "type": "author",
      "name": "Nora Puckett"
    },
    {
      "id": "7624658",
      "type": "author",
      "name": "Ofir Nachum"
    },
    {
      "id": "2328086961",
      "type": "author",
      "name": "Ola Okelola"
    },
    {
      "id": "2325151487",
      "type": "author",
      "name": "Oliver Jaffe"
    },
    {
      "id": "2328111449",
      "type": "author",
      "name": "Olivia Watkins"
    },
    {
      "id": "2328086281",
      "type": "author",
      "name": "Olivier Godement"
    },
    {
      "id": "2328113242",
      "type": "author",
      "name": "Owen Campbell-Moore"
    },
    {
      "id": "2328087286",
      "type": "author",
      "name": "Patrick Chao"
    },
    {
      "id": "2328088204",
      "type": "author",
      "name": "Pavel Belov"
    },
    {
      "id": "2328367058",
      "type": "author",
      "name": "Peng Su"
    },
    {
      "id": "2328089338",
      "type": "author",
      "name": "Peter Bak"
    },
    {
      "id": "2328086386",
      "type": "author",
      "name": "Peter Bakkum"
    },
    {
      "id": "2328089793",
      "type": "author",
      "name": "Peter Deng"
    },
    {
      "id": "2328087257",
      "type": "author",
      "name": "Peter Dolan"
    },
    {
      "id": "2328088078",
      "type": "author",
      "name": "Philip Pronin"
    },
    {
      "id": "2316290883",
      "type": "author",
      "name": "Prafulla Dhariwal"
    },
    {
      "id": "2328288671",
      "type": "author",
      "name": "Rachel Dias"
    },
    {
      "id": "2328114240",
      "type": "author",
      "name": "Rahul Arora"
    },
    {
      "id": "2304955112",
      "type": "author",
      "name": "Rajan Troll"
    },
    {
      "id": "2329125040",
      "type": "author",
      "name": "Randall Lin"
    },
    {
      "id": "2314850132",
      "type": "author",
      "name": "Raphael Gontijo Lopes"
    },
    {
      "id": "2105843952",
      "type": "author",
      "name": "Reah Miyara"
    },
    {
      "id": "7987799",
      "type": "author",
      "name": "R. Leike"
    },
    {
      "id": "2328089012",
      "type": "author",
      "name": "Renaud Gaubert"
    },
    {
      "id": "2328089937",
      "type": "author",
      "name": "Reza Zamani"
    },
    {
      "id": "2328260600",
      "type": "author",
      "name": "Ricky Wang"
    },
    {
      "id": "2328087260",
      "type": "author",
      "name": "Rob Donnelly"
    },
    {
      "id": "2328088979",
      "type": "author",
      "name": "Rob Honsby"
    },
    {
      "id": "2328377365",
      "type": "author",
      "name": "Rocky Smith"
    },
    {
      "id": "2328089004",
      "type": "author",
      "name": "Rohan Sahai"
    },
    {
      "id": "2073442003",
      "type": "author",
      "name": "Rohit Ramchandani"
    },
    {
      "id": "2328086978",
      "type": "author",
      "name": "Romain Huet"
    },
    {
      "id": "2328158635",
      "type": "author",
      "name": "Roy Chen"
    },
    {
      "id": "2093346446",
      "type": "author",
      "name": "R. Nigmatullin"
    },
    {
      "id": "2062757640",
      "type": "author",
      "name": "Ryan Cheu"
    },
    {
      "id": "2329885104",
      "type": "author",
      "name": "Saachi Jain"
    },
    {
      "id": "2328086378",
      "type": "author",
      "name": "Sam S. Schoenholz"
    },
    {
      "id": "2328088524",
      "type": "author",
      "name": "Sam Toizer"
    },
    {
      "id": "2328088976",
      "type": "author",
      "name": "Samuel Miserendino"
    },
    {
      "id": "2328086279",
      "type": "author",
      "name": "Sara Culver"
    },
    {
      "id": "2328088881",
      "type": "author",
      "name": "Scott Ethersmith"
    },
    {
      "id": "2328089201",
      "type": "author",
      "name": "Sean Grove"
    },
    {
      "id": "2328087278",
      "type": "author",
      "name": "Sean Metzger"
    },
    {
      "id": "2328088626",
      "type": "author",
      "name": "Shamez Hermani"
    },
    {
      "id": "2328337411",
      "type": "author",
      "name": "Shirong Wu"
    },
    {
      "id": "2328078049",
      "type": "author",
      "name": "Shuaiqi Xia"
    },
    {
      "id": "52254851",
      "type": "author",
      "name": "Sonia Phene"
    },
    {
      "id": "2328088969",
      "type": "author",
      "name": "Spencer Papay"
    },
    {
      "id": "2328282767",
      "type": "author",
      "name": "Srinivas Narayanan"
    },
    {
      "id": "2328087320",
      "type": "author",
      "name": "Steve Coffey"
    },
    {
      "id": "2328107017",
      "type": "author",
      "name": "Steve Lee"
    },
    {
      "id": "2328268724",
      "type": "author",
      "name": "Stewart Hall"
    },
    {
      "id": "52010041",
      "type": "author",
      "name": "Tal Broda"
    },
    {
      "id": "14973749",
      "type": "author",
      "name": "Tal Stramer"
    },
    {
      "id": "2328087115",
      "type": "author",
      "name": "Taya Christianson"
    },
    {
      "id": "90169542",
      "type": "author",
      "name": "Tejal A. Patwardhan"
    },
    {
      "id": "2328087230",
      "type": "author",
      "name": "Thomas Cunninghman"
    },
    {
      "id": "2379991",
      "type": "author",
      "name": "Thomas Dimson"
    },
    {
      "id": "2259739",
      "type": "author",
      "name": "Thomas Raoux"
    },
    {
      "id": "2328087314",
      "type": "author",
      "name": "Thomas Shadwell"
    },
    {
      "id": "2328089683",
      "type": "author",
      "name": "Todd Underwood"
    },
    {
      "id": "2266396476",
      "type": "author",
      "name": "Tom Rubin"
    },
    {
      "id": "2328087307",
      "type": "author",
      "name": "Tom Stasi"
    },
    {
      "id": "2328088645",
      "type": "author",
      "name": "Tristan Heywood"
    },
    {
      "id": "2328983284",
      "type": "author",
      "name": "Troy Peterson"
    },
    {
      "id": "2328088989",
      "type": "author",
      "name": "Tyce Walters"
    },
    {
      "id": "2328087310",
      "type": "author",
      "name": "Valerie Qi"
    },
    {
      "id": "2328089157",
      "type": "author",
      "name": "Veit Moeller"
    },
    {
      "id": "2295667407",
      "type": "author",
      "name": "Vlad Fomenko"
    },
    {
      "id": "2328252793",
      "type": "author",
      "name": "Wayne Chang"
    },
    {
      "id": "2329138264",
      "type": "author",
      "name": "Weiyi Zheng"
    },
    {
      "id": "2328485051",
      "type": "author",
      "name": "Wenda Zhou"
    },
    {
      "id": "100528996",
      "type": "author",
      "name": "Wesam Manassra"
    },
    {
      "id": "2328088471",
      "type": "author",
      "name": "Will Sheu"
    },
    {
      "id": "2307452791",
      "type": "author",
      "name": "Wojciech Zaremba"
    },
    {
      "id": "2328086527",
      "type": "author",
      "name": "Yash Patil"
    },
    {
      "id": "2330153952",
      "type": "author",
      "name": "Yilei Qian"
    },
    {
      "id": "2328255074",
      "type": "author",
      "name": "Youlong Cheng"
    },
    {
      "id": "2328092537",
      "type": "author",
      "name": "Yu Zhang"
    },
    {
      "id": "2328256460",
      "type": "author",
      "name": "Yuchen Zhang"
    },
    {
      "id": "2328616992",
      "type": "author",
      "name": "Yujia Jin"
    },
    {
      "id": "2328086974",
      "type": "author",
      "name": "Yury Malkov"
    },
    {
      "id": "deb3fccb5b68041ea0f5c724f08b2d840fa51eaa",
      "type": "paper",
      "title": "WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs",
      "abstract": "We introduce WildGuard -- an open, light-weight moderation tool for LLM safety that achieves three goals: (1) identifying malicious intent in user prompts, (2) detecting safety risks of model responses, and (3) determining model refusal rate. Together, WildGuard serves the increasing needs for automatic safety moderation and evaluation of LLM interactions, providing a one-stop tool with enhanced accuracy and broad coverage across 13 risk categories. While existing open moderation tools such as Llama-Guard2 score reasonably well in classifying straightforward model interactions, they lag far behind a prompted GPT-4, especially in identifying adversarial jailbreaks and in evaluating models' refusals, a key measure for evaluating safety behaviors in model responses. To address these challenges, we construct WildGuardMix, a large-scale and carefully balanced multi-task safety moderation dataset with 92K labeled examples that cover vanilla (direct) prompts and adversarial jailbreaks, paired with various refusal and compliance responses. WildGuardMix is a combination of WildGuardTrain, the training data of WildGuard, and WildGuardTest, a high-quality human-annotated moderation test set with 5K labeled items covering broad risk scenarios. Through extensive evaluations on WildGuardTest and ten existing public benchmarks, we show that WildGuard establishes state-of-the-art performance in open-source safety moderation across all the three tasks compared to ten strong existing open-source moderation models (e.g., up to 26.4% improvement on refusal detection). Importantly, WildGuard matches and sometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt harmfulness identification). WildGuard serves as a highly effective safety moderator in an LLM interface, reducing the success rate of jailbreak attacks from 79.8% to 2.4%.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "url": "https://www.semanticscholar.org/paper/deb3fccb5b68041ea0f5c724f08b2d840fa51eaa",
      "citation_count": 32,
      "reference_count": 46,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "WildGuard establishes state-of-the-art performance in open-source safety moderation across all the three tasks compared to ten strong existing open-source moderation models (e.g., up to 26.4% improvement on refusal detection).",
      "external_id_dblp": "journals/corr/abs-2406-18495",
      "external_id_arxiv": "2406.18495",
      "external_id_doi": "10.48550/arXiv.2406.18495",
      "external_id_corpusid": 270737916
    },
    {
      "id": "2423429",
      "type": "author",
      "name": "Seungju Han"
    },
    {
      "id": "2237944445",
      "type": "author",
      "name": "Kavel Rao"
    },
    {
      "id": "2262217080",
      "type": "author",
      "name": "Allyson Ettinger"
    },
    {
      "id": "2112504145",
      "type": "author",
      "name": "Liwei Jiang"
    },
    {
      "id": "2273918810",
      "type": "author",
      "name": "Bill Yuchen Lin"
    },
    {
      "id": "2257385142",
      "type": "author",
      "name": "Yejin Choi"
    },
    {
      "id": "46217681",
      "type": "author",
      "name": "Nouha Dziri"
    },
    {
      "id": "03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f",
      "type": "paper",
      "title": "Describing Differences in Image Sets with Natural Language",
      "abstract": "How do two sets of images differ? Discerning set-level differences is crucial for understanding model behaviors and analyzing datasets, yet manually sifting through thousands of images is impractical. To aid in this discovery process, we explore the task of automatically describing the differences between two sets of images, which we term Set Difference Captioning. This task takes in image sets $\\mathcal{D}_{A}$ and $\\mathcal{D}_{B}$, and outputs a description that is more often true on $\\mathcal{D}_{A}$ than $\\mathcal{D}_{B}$. We outline a two-stage approach that first proposes candidate difference descriptions from image sets and then re-ranks the candidates by checking how well they can differentiate the two sets. We introduce VisDiff, which first captions the images and prompts a language model to propose candidate descriptions, then re-ranks these descriptions using CLIP. To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image sets with ground truth difference descriptions. We apply VisDiff to various domains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing classification models (e.g., zero-shot CLIP vs. supervised ResNet), characterizing differences between generative models (e.g., StableDiffusionV1 and V2), and discovering what makes images memorable. Using VisDiff, we are able to find interesting and previously unknown differences in datasets and models, demonstrating its utility in revealing nuanced insights.11Project page available at https:/understanding-visual-datasets.github.io/VisDiff-website/.",
      "year": 2023,
      "venue": "Computer Vision and Pattern Recognition",
      "url": "https://www.semanticscholar.org/paper/03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f",
      "citation_count": 20,
      "reference_count": 54,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "Using VisDiff, which first captions the images and prompts a language model to propose candidate descriptions, then re-ranks these descriptions using CLIP, it is able to find interesting and previously unknown differences in datasets and models, demonstrating its utility in revealing nuanced insights.",
      "open_access_pdf_url": "https://arxiv.org/pdf/2312.02974",
      "open_access_status": "GREEN",
      "external_id_arxiv": "2312.02974",
      "external_id_dblp": "conf/cvpr/DunlapZWZDSGY24",
      "external_id_doi": "10.1109/CVPR52733.2024.02284",
      "external_id_corpusid": 265658938
    },
    {
      "id": "151088535",
      "type": "author",
      "name": "Lisa Dunlap"
    },
    {
      "id": "49889860",
      "type": "author",
      "name": "Yuhui Zhang"
    },
    {
      "id": "2269778420",
      "type": "author",
      "name": "Xiaohan Wang"
    },
    {
      "id": "51011000",
      "type": "author",
      "name": "Ruiqi Zhong"
    },
    {
      "id": "2267488244",
      "type": "author",
      "name": "Trevor Darrell"
    },
    {
      "id": "2269733338",
      "type": "author",
      "name": "Jacob Steinhardt"
    },
    {
      "id": "2260456251",
      "type": "author",
      "name": "Joseph Gonzalez"
    },
    {
      "id": "2275638250",
      "type": "author",
      "name": "S. Yeung-Levy"
    },
    {
      "id": "54e14b407d246a73d0af8f22ed67c363d3aa6f54",
      "type": "paper",
      "title": "Compositional preference models for aligning LMs",
      "abstract": "As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. Through these simple steps, CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using CPMs tend to be preferred over samples obtained using conventional PMs. Overall, our approach demonstrates the benefits of endowing PMs with priors about which features determine human preferences while relying on LM capabilities to extract those features in a scalable and robust way.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "url": "https://www.semanticscholar.org/paper/54e14b407d246a73d0af8f22ed67c363d3aa6f54",
      "citation_count": 13,
      "reference_count": 46,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Compositional Preference Models are proposed, a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier.",
      "external_id_dblp": "conf/iclr/GoKKRD24",
      "external_id_arxiv": "2310.13011",
      "external_id_doi": "10.48550/arXiv.2310.13011",
      "external_id_corpusid": 264405792
    },
    {
      "id": "2148631790",
      "type": "author",
      "name": "Dongyoung Go"
    },
    {
      "id": "2261082206",
      "type": "author",
      "name": "Tomasz Korbak"
    },
    {
      "id": "2067996",
      "type": "author",
      "name": "Germán Kruszewski"
    },
    {
      "id": "120419790",
      "type": "author",
      "name": "Jos Rozen"
    },
    {
      "id": "2257233215",
      "type": "author",
      "name": "Marc Dymetman"
    },
    {
      "id": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "paper",
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "abstract": "We apply preference modeling and reinforcement learning from human feedback (RLHF) to ﬁnetune language models to act as helpful and harmless assistants. We ﬁnd this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efﬁciently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work. Figure These plots show that PM accuracy decreases as we focus exclusively on comparisons between pairs of samples with high score. We have normalized all preference models to have the same mean score on a held-out dataset so that they’re directly comparable, and then plotted accuracy for the comparisons where both samples have scores above a speciﬁc threshold.",
      "year": 2022,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/0286b2736a114198b25fb5553c671c33aed5d477",
      "citation_count": 1941,
      "reference_count": 72,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "An iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, and a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization is identified.",
      "open_access_pdf_url": "http://arxiv.org/pdf/2204.05862",
      "open_access_status": "GREEN",
      "external_id_arxiv": "2204.05862",
      "external_id_dblp": "journals/corr/abs-2204-05862",
      "external_id_doi": "10.48550/arXiv.2204.05862",
      "external_id_corpusid": 248118878
    },
    {
      "id": "1486307451",
      "type": "author",
      "name": "Yuntao Bai"
    },
    {
      "id": "2149890773",
      "type": "author",
      "name": "Andy Jones"
    },
    {
      "id": "1978097132",
      "type": "author",
      "name": "Kamal Ndousse"
    },
    {
      "id": "119609682",
      "type": "author",
      "name": "Amanda Askell"
    },
    {
      "id": "2111073313",
      "type": "author",
      "name": "Anna Chen"
    },
    {
      "id": "2142833890",
      "type": "author",
      "name": "Nova Dassarma"
    },
    {
      "id": "1943097969",
      "type": "author",
      "name": "Dawn Drain"
    },
    {
      "id": "30176974",
      "type": "author",
      "name": "Stanislav Fort"
    },
    {
      "id": "2081806483",
      "type": "author",
      "name": "Deep Ganguli"
    },
    {
      "id": "103143311",
      "type": "author",
      "name": "T. Henighan"
    },
    {
      "id": "2117706920",
      "type": "author",
      "name": "Nicholas Joseph"
    },
    {
      "id": "148070327",
      "type": "author",
      "name": "Saurav Kadavath"
    },
    {
      "id": "1583434563",
      "type": "author",
      "name": "John Kernion"
    },
    {
      "id": "2154608209",
      "type": "author",
      "name": "Tom Conerly"
    },
    {
      "id": "1403602266",
      "type": "author",
      "name": "S. El-Showk"
    },
    {
      "id": "2866708",
      "type": "author",
      "name": "Nelson Elhage"
    },
    {
      "id": "1573482302",
      "type": "author",
      "name": "Zac Hatfield-Dodds"
    },
    {
      "id": "39182747",
      "type": "author",
      "name": "Danny Hernandez"
    },
    {
      "id": "2162194147",
      "type": "author",
      "name": "Tristan Hume"
    },
    {
      "id": "2154610174",
      "type": "author",
      "name": "Scott Johnston"
    },
    {
      "id": "49604482",
      "type": "author",
      "name": "Shauna Kravec"
    },
    {
      "id": "2154608229",
      "type": "author",
      "name": "Liane Lovitt"
    },
    {
      "id": "2051128902",
      "type": "author",
      "name": "Neel Nanda"
    },
    {
      "id": "2061321863",
      "type": "author",
      "name": "Catherine Olsson"
    },
    {
      "id": "2698777",
      "type": "author",
      "name": "Dario Amodei"
    },
    {
      "id": "31035595",
      "type": "author",
      "name": "Tom B. Brown"
    },
    {
      "id": "2115193883",
      "type": "author",
      "name": "Jack Clark"
    },
    {
      "id": "52238703",
      "type": "author",
      "name": "Sam McCandlish"
    },
    {
      "id": "37232298",
      "type": "author",
      "name": "C. Olah"
    },
    {
      "id": "2056658938",
      "type": "author",
      "name": "Benjamin Mann"
    },
    {
      "id": "2053807409",
      "type": "author",
      "name": "Jared Kaplan"
    },
    {
      "id": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
      "type": "paper",
      "title": "Deep Reinforcement Learning from Human Preferences",
      "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",
      "year": 2017,
      "venue": "Neural Information Processing Systems",
      "url": "https://www.semanticscholar.org/paper/5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
      "citation_count": 2703,
      "reference_count": 45,
      "fields_of_study": "[\"Computer Science\", \"Mathematics\"]",
      "is_open_access": false,
      "tldr": "This work explores goals defined in terms of (non-expert) human preferences between pairs of trajectory segments in order to effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion.",
      "external_id_dblp": "journals/corr/abs-1706-03741",
      "external_id_mag": "2626804490",
      "external_id_arxiv": "1706.03741",
      "external_id_corpusid": 4787508
    },
    {
      "id": "145791315",
      "type": "author",
      "name": "P. Christiano"
    },
    {
      "id": "26890260",
      "type": "author",
      "name": "Miljan Martic"
    },
    {
      "id": "34313265",
      "type": "author",
      "name": "S. Legg"
    },
    {
      "id": "2330246606",
      "type": "author",
      "name": "Dario Amodei"
    },
    {
      "id": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "paper",
      "title": "Clio: Privacy-Preserving Insights into Real-World AI Use",
      "abstract": "How are AI assistants being used in the real world? While model providers in theory have a window into this impact via their users' data, both privacy concerns and practical challenges have made analyzing this data difficult. To address these issues, we present Clio (Claude insights and observations), a privacy-preserving platform that uses AI assistants themselves to analyze and surface aggregated usage patterns across millions of conversations, without the need for human reviewers to read raw conversations. We validate this can be done with a high degree of accuracy and privacy by conducting extensive evaluations. We demonstrate Clio's usefulness in two broad ways. First, we share insights about how models are being used in the real world from one million Claude.ai Free and Pro conversations, ranging from providing advice on hairstyles to providing guidance on Git operations and concepts. We also identify the most common high-level use cases on Claude.ai (coding, writing, and research tasks) as well as patterns that differ across languages (e.g., conversations in Japanese discuss elder care and aging populations at higher-than-typical rates). Second, we use Clio to make our systems safer by identifying coordinated attempts to abuse our systems, monitoring for unknown unknowns during critical periods like launches of new capabilities or major world events, and improving our existing monitoring systems. We also discuss the limitations of our approach, as well as risks and ethical concerns. By enabling analysis of real-world AI usage, Clio provides a scalable platform for empirically grounded AI safety and governance.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "citation_count": 1,
      "reference_count": 103,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Clio (Claude insights and observations), a privacy-preserving platform that uses AI assistants themselves to analyze and surface aggregated usage patterns across millions of conversations, without the need for human reviewers to read raw conversations is presented.",
      "external_id_arxiv": "2412.13678",
      "external_id_dblp": "journals/corr/abs-2412-13678",
      "external_id_doi": "10.48550/arXiv.2412.13678",
      "external_id_corpusid": 274822379
    },
    {
      "id": "88726969",
      "type": "author",
      "name": "Alex Tamkin"
    },
    {
      "id": "2335860974",
      "type": "author",
      "name": "Miles McCain"
    },
    {
      "id": "2066900445",
      "type": "author",
      "name": "Kunal Handa"
    },
    {
      "id": "41152329",
      "type": "author",
      "name": "Esin Durmus"
    },
    {
      "id": "2335860956",
      "type": "author",
      "name": "Ankur Rathi"
    },
    {
      "id": "2336037417",
      "type": "author",
      "name": "Saffron Huang"
    },
    {
      "id": "2335857971",
      "type": "author",
      "name": "Alfred Mountfield"
    },
    {
      "id": "2336035162",
      "type": "author",
      "name": "Jerry Hong"
    },
    {
      "id": "2335860886",
      "type": "author",
      "name": "Stuart Ritchie"
    },
    {
      "id": "2335859860",
      "type": "author",
      "name": "Michael Stern"
    },
    {
      "id": "2335859695",
      "type": "author",
      "name": "Brian Clarke"
    },
    {
      "id": "2316350011",
      "type": "author",
      "name": "Landon Goldberg"
    },
    {
      "id": "1976174397",
      "type": "author",
      "name": "T. Sumers"
    },
    {
      "id": "2190111475",
      "type": "author",
      "name": "J. Mueller"
    },
    {
      "id": "98366006",
      "type": "author",
      "name": "William McEachen"
    },
    {
      "id": "2335857516",
      "type": "author",
      "name": "Wes Mitchell"
    },
    {
      "id": "2335858715",
      "type": "author",
      "name": "Shan Carter"
    },
    {
      "id": "2336243144",
      "type": "author",
      "name": "Jack Clark"
    },
    {
      "id": "deeb72a8582a68403082f2da4e7fa6a3c4da26b4",
      "type": "paper",
      "title": "Video Instruction Tuning With Synthetic Data",
      "abstract": "The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, namely LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA. By training on this dataset, in combination with existing visual instruction tuning data, we introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that LLaVA-Video achieves strong performance across various video benchmarks, highlighting the effectiveness of our dataset. We plan to release the dataset, its generation pipeline, and the model checkpoints.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/deeb72a8582a68403082f2da4e7fa6a3c4da26b4",
      "citation_count": 30,
      "reference_count": 66,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A high-quality synthetic dataset specifically for video instruction-following, namely LLaVA-Video-178K, which includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA, and introduces LLaVA-Video, a new video LMM.",
      "external_id_arxiv": "2410.02713",
      "external_id_dblp": "journals/corr/abs-2410-02713",
      "external_id_doi": "10.48550/arXiv.2410.02713",
      "external_id_corpusid": 273098427
    },
    {
      "id": "2324074296",
      "type": "author",
      "name": "Jinming Wu"
    },
    {
      "id": "2310658071",
      "type": "author",
      "name": "Wei Li"
    },
    {
      "id": "2310709478",
      "type": "author",
      "name": "Bo Li"
    },
    {
      "id": "2257135061",
      "type": "author",
      "name": "Zejun Ma"
    },
    {
      "id": "2315193840",
      "type": "author",
      "name": "Ziwei Liu"
    },
    {
      "id": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "paper",
      "title": "LLaVA-OneVision: Easy Visual Task Transfer",
      "abstract": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/1a71f7b216b710b936da666027014adb83af8e7a",
      "citation_count": 210,
      "reference_count": 156,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios.",
      "external_id_arxiv": "2408.03326",
      "external_id_dblp": "journals/corr/abs-2408-03326",
      "external_id_doi": "10.48550/arXiv.2408.03326",
      "external_id_corpusid": 271719914
    },
    {
      "id": "2325209062",
      "type": "author",
      "name": "Dong Guo"
    },
    {
      "id": "2310650738",
      "type": "author",
      "name": "Renrui Zhang"
    },
    {
      "id": "2310758205",
      "type": "author",
      "name": "Feng Li"
    },
    {
      "id": "2267467406",
      "type": "author",
      "name": "Hao Zhang"
    },
    {
      "id": "2315071527",
      "type": "author",
      "name": "Yanwei Li"
    },
    {
      "id": "81002626b9eeb896f2adcdc838a4451d9001cdb3",
      "type": "paper",
      "title": "Listen Then See: Video Alignment with Speaker Attention",
      "abstract": "Video-based Question Answering (Video QA) is a challenging task and becomes even more intricate when addressing Socially Intelligent Question Answering (SIQA). SIQA requires context understanding, temporal reasoning, and the integration of multimodal information, but in addition, it requires processing nuanced human behavior. Furthermore, the complexities involved are exacerbated by the dominance of the primary modality (text) over the others. Thus, there is a need to help the task’s secondary modalities to work in tandem with the primary modality. In this work, we introduce a cross-modal alignment and subsequent representation fusion approach that achieves state-of-the-art results (82.06% accuracy) on the Social IQ 2.0 dataset for SIQA. Our approach exhibits an improved ability to leverage the video modality by using the audio modality as a bridge with the language modality. This leads to enhanced performance by reducing the prevalent issue of language overfitting and resultant video modality bypassing encountered by current existing techniques. Our code and models are publicly available at [1].",
      "year": 2024,
      "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "url": "https://www.semanticscholar.org/paper/81002626b9eeb896f2adcdc838a4451d9001cdb3",
      "citation_count": 2,
      "reference_count": 45,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "This work introduces a cross-modal alignment and subsequent representation fusion approach that achieves state-of-the-art results on the Social IQ 2.0 dataset for SIQA and exhibits an improved ability to leverage the video modality by using the audio modality as a bridge with the language modality.",
      "open_access_pdf_url": "https://arxiv.org/pdf/2404.13530",
      "open_access_status": "GREEN",
      "external_id_arxiv": "2404.13530",
      "external_id_dblp": "conf/cvpr/AgrawalLHS22",
      "external_id_doi": "10.1109/CVPRW63382.2024.00207",
      "external_id_corpusid": 269293128
    },
    {
      "id": "2297768601",
      "type": "author",
      "name": "Aviral Agrawal"
    },
    {
      "id": "2297769955",
      "type": "author",
      "name": "Carlos Mateo Samudio Lezcano"
    },
    {
      "id": "2167332264",
      "type": "author",
      "name": "Iqui Balam Heredia-Marin"
    },
    {
      "id": "2688928",
      "type": "author",
      "name": "P. Sethi"
    },
    {
      "id": "aa66639895a7dfce1e229293e546686912ba8320",
      "type": "paper",
      "title": "LLMs Meet Long Video: Advancing Long Video Question Answering with An Interactive Visual Adapter in LLMs",
      "abstract": "Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector and a spatial feature interactor, within the internal blocks of LLMs to capture instruction-aware and fine-grained visual signals. Consequently, the proposed video-LLM facilitates a comprehensive understanding of long video content through appropriate long video modeling and precise visual interactions. We conducted extensive experiments on nine video understanding benchmarks and experimental results show that our interactive visual adapter significantly improves the performance of video LLMs on long video QA tasks. Ablation studies further verify the effectiveness of IVA in understanding long and short video.",
      "year": 2024,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/aa66639895a7dfce1e229293e546686912ba8320",
      "citation_count": 1,
      "reference_count": 48,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "An Interactive Visual Adapter (IVA) within LLMs is presented, designed to enhance interaction with fine-grained visual elements and significantly improves the performance of video LLMs on long video QA tasks.",
      "external_id_arxiv": "2402.13546",
      "external_id_corpusid": 271957622
    },
    {
      "id": "2118046679",
      "type": "author",
      "name": "Yunxin Li"
    },
    {
      "id": "2266425853",
      "type": "author",
      "name": "Xinyu Chen"
    },
    {
      "id": "2316960542",
      "type": "author",
      "name": "Baotain Hu"
    },
    {
      "id": "2284915404",
      "type": "author",
      "name": "Min Zhang"
    },
    {
      "id": "cf4fac3510b0e4fdab47fa7495c18ad32dfefde0",
      "type": "paper",
      "title": "DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding",
      "abstract": "Social intelligence is essential for understanding and reasoning about human expressions, intents and interactions. One representative benchmark for its study is Social Intelligence Queries (Social-IQ), a dataset of multiple-choice questions on videos of complex social interactions. We define a comprehensive methodology to study the soundness of Social-IQ, as the soundness of such benchmark datasets is crucial to the investigation of the underlying research problem. Our analysis reveals that Social-IQ contains substantial biases, which can be exploited by a moderately strong language model to learn spurious correlations to achieve perfect performance without being given the context or even the question. We introduce DeSIQ, a new challenging dataset, constructed by applying simple perturbations to Social-IQ. Our empirical analysis shows DeSIQ significantly reduces the biases in the original Social-IQ dataset. Furthermore, we examine and shed light on the effect of model size, model style, learning settings, commonsense knowledge, and multi-modality on the new benchmark performance. Our new dataset, observations and findings open up important research questions for the study of social intelligence.",
      "year": 2023,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "url": "https://www.semanticscholar.org/paper/cf4fac3510b0e4fdab47fa7495c18ad32dfefde0",
      "citation_count": 3,
      "reference_count": 24,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The analysis reveals that Social-IQ contains substantial biases, which can be exploited by a moderately strong language model to learn spurious correlations to achieve perfect performance without being given the context or even the question.",
      "external_id_arxiv": "2310.18359",
      "external_id_dblp": "journals/corr/abs-2310-18359",
      "external_id_doi": "10.48550/arXiv.2310.18359",
      "external_id_corpusid": 264590462
    },
    {
      "id": "2146373626",
      "type": "author",
      "name": "Xiao-Yu Guo"
    },
    {
      "id": "2256011160",
      "type": "author",
      "name": "Yuan-Fang Li"
    },
    {
      "id": "2561045",
      "type": "author",
      "name": "Gholamreza Haffari"
    },
    {
      "id": "21a6bd63a062acab5e738db4c593891060df6205",
      "type": "paper",
      "title": "Multi-Modal Correlated Network with Emotional Reasoning Knowledge for Social Intelligence Question-Answering",
      "abstract": "The capacity for social reasoning is essential to the development of social intelligence in humans, which we easily acquire through study and experience. The acquisition of such ability by machines, however, is still challenging, even with the diverse deep learning models that are currently available. Recent artificial social intelligence models have achieved state-of-the-art results in question-answering tasks by employing a variety of methods, including self-supervised setups, multi-modal inputs, and so on. However, there is still a gap in the literature regarding the introduction of commonsense knowledge when training the model in social intelligence tasks. In this paper, we propose a Multi-Modal Temporal Correlated Network with Emotional Social Cues (MMTC-ESC). In order to model cross-modal correlations, an attention-based mechanism is used, and contrastive learning is achieved using emotional social cues. Our findings indicate that combining multimodal inputs and using contrastive loss is advantageous for the performance of social intelligence learning.",
      "year": 2023,
      "venue": "2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "url": "https://www.semanticscholar.org/paper/21a6bd63a062acab5e738db4c593891060df6205",
      "citation_count": 5,
      "reference_count": 36,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper proposes a Multi-Modal Temporal Correlated Network with Emotional Social Cues (MMTC-ESC), and indicates that combining multimodal inputs and using contrastive loss is advantageous for the performance of social intelligence learning.",
      "external_id_dblp": "conf/iccvw/XieP23",
      "external_id_doi": "10.1109/ICCVW60793.2023.00331",
      "external_id_corpusid": 263770683
    },
    {
      "id": "1576157233",
      "type": "author",
      "name": "Baijun Xie"
    },
    {
      "id": "2256473222",
      "type": "author",
      "name": "Chung Hyuk Park"
    },
    {
      "id": "23c8d86963639daf04bc6518c1731eafc85bdef1",
      "type": "paper",
      "title": "Just Ask Plus: Using Transcripts for VideoQA",
      "abstract": "Social-IQ 2.0 challenge is designed to benchmark recent AI technologies' skills to reason about social interactions, which is referred as Artificial Social Intelligence in the form of a VideoQA task. In this work, we use Just Ask and SpeechT5 models as feature extractors, and reason by adding one attention layer and two transformer encoders. Our best configuration reaches 53.35% accuracy on the validation set. The code is publicly available on GitHub.",
      "year": 2023,
      "venue": "2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "url": "https://www.semanticscholar.org/paper/23c8d86963639daf04bc6518c1731eafc85bdef1",
      "citation_count": 3,
      "reference_count": 11,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work uses Just Ask and SpeechT5 models as feature extractors, and reason by adding one attention layer and two transformer encoders, and reaches 53.35% accuracy on the validation set.",
      "external_id_dblp": "conf/iccvw/PirhadiME23",
      "external_id_doi": "10.1109/ICCVW60793.2023.00332",
      "external_id_corpusid": 263745036
    },
    {
      "id": "2196915039",
      "type": "author",
      "name": "Mohammad Javad Pirhadi"
    },
    {
      "id": "2195848881",
      "type": "author",
      "name": "Motahhare Mirzaei"
    },
    {
      "id": "3054954",
      "type": "author",
      "name": "Sauleh Eetemadi"
    },
    {
      "id": "e17aac3bbe0aabda715cafad392f31a1e046c17c",
      "type": "paper",
      "title": "Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence",
      "abstract": "As intelligent systems increasingly blend into our everyday life, artificial social intelligence becomes a prominent area of research. Intelligent systems must be socially intelligent in order to comprehend human intents and maintain a rich level of interaction with humans. Human language offers a unique unconstrained approach to probe through questions and reason through answers about social situations. This unconstrained approach extends previous attempts to model social intelligence through numeric supervision (e.g. sentiment and emotions labels). In this paper, we introduce Social-IQ, a unconstrained benchmark specifically designed to train and evaluate socially intelligent technologies. By providing a rich source of open-ended questions and answers, Social-IQ opens the door to explainable social intelligence. The dataset contains rigorously annotated and validated videos, questions and answers, as well as annotations for the complexity level of each question and answer. Social-IQ contains 1,250 natural in-the-wild social situations, 7,500 questions and 52,500 correct and incorrect answers. Although humans can reason about social situations with very high accuracy (95.08%), existing state-of-the-art computational models struggle on this task. As a result, Social-IQ brings novel challenges that will spark future research in social intelligence modeling, visual reasoning, and multimodal question answering (QA).",
      "year": 2019,
      "venue": "Computer Vision and Pattern Recognition",
      "url": "https://www.semanticscholar.org/paper/e17aac3bbe0aabda715cafad392f31a1e046c17c",
      "citation_count": 119,
      "reference_count": 63,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper introduces Social-IQ, a unconstrained benchmark specifically designed to train and evaluate socially intelligent technologies and brings novel challenges that will spark future research in social intelligence modeling, visual reasoning, and multimodal question answering (QA).",
      "external_id_mag": "2964306921",
      "external_id_dblp": "conf/cvpr/0001CLTM19",
      "external_id_doi": "10.1109/CVPR.2019.00901",
      "external_id_corpusid": 198165809
    },
    {
      "id": "144802290",
      "type": "author",
      "name": "Amir Zadeh"
    },
    {
      "id": "2067821698",
      "type": "author",
      "name": "Michael Chan"
    },
    {
      "id": "31726556",
      "type": "author",
      "name": "Edmund Tong"
    },
    {
      "id": "fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "type": "paper",
      "title": "HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding",
      "abstract": "Multimodal large language models have become a popular topic in deep visual understanding due to many promising real-world applications. However, hour-long video understanding, spanning over one hour and containing tens of thousands of visual frames, remains under-explored because of 1) challenging long-term video analyses, 2) inefficient large-model approaches, and 3) lack of large-scale benchmark datasets. Among them, in this paper, we focus on building a large-scale hour-long long video benchmark, HLV-1K, designed to evaluate long video understanding models. HLV-1K comprises 1009 hour-long videos with 14,847 high-quality question answering (QA) and multi-choice question asnwering (MCQA) pairs with time-aware query and diverse annotations, covering frame-level, within-event-level, cross-event-level, and long-term reasoning tasks. We evaluate our benchmark using existing state-of-the-art methods and demonstrate its value for testing deep long video understanding capabilities at different levels and for various tasks. This includes promoting future long video understanding tasks at a granular level, such as deep understanding of long live videos, meeting recordings, and movies.",
      "year": 2025,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "citation_count": 1,
      "reference_count": 0,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper focuses on building a large-scale hour-long long video benchmark, HLV-1K, designed to evaluate long video understanding models, and evaluates its value for testing deep long video understanding capabilities at different levels and for various tasks.",
      "external_id_dblp": "journals/corr/abs-2501-01645",
      "external_id_arxiv": "2501.01645",
      "external_id_doi": "10.48550/arXiv.2501.01645",
      "external_id_corpusid": 275324269
    },
    {
      "id": "2323449745",
      "type": "author",
      "name": "Heqing Zou"
    },
    {
      "id": "2323372219",
      "type": "author",
      "name": "Tianze Luo"
    },
    {
      "id": "2323375681",
      "type": "author",
      "name": "Guiyang Xie"
    },
    {
      "id": "2323373523",
      "type": "author",
      "name": "Victor Zhang"
    },
    {
      "id": "2338693661",
      "type": "author",
      "name": "Fengmao Lv"
    },
    {
      "id": "2313186079",
      "type": "author",
      "name": "Guangcong Wang"
    },
    {
      "id": "2338786700",
      "type": "author",
      "name": "Junyang Chen"
    },
    {
      "id": "2323393220",
      "type": "author",
      "name": "Zhuochen Wang"
    },
    {
      "id": "2323387847",
      "type": "author",
      "name": "Hansheng Zhang"
    },
    {
      "id": "2323428668",
      "type": "author",
      "name": "Huaijian Zhang"
    },
    {
      "id": "078a9c3dcd0575dccc45c861618e2363caf47986",
      "type": "paper",
      "title": "NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?",
      "abstract": "In evaluating the long-context capabilities of large language models (LLMs), identifying content relevant to a user's query from original long documents is a crucial prerequisite for any LLM to answer questions based on long text. We present NeedleBench, a framework consisting of a series of progressively more challenging tasks for assessing bilingual long-context capabilities, spanning multiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and different depth ranges, allowing the strategic insertion of critical data points in different text depth zones to rigorously test the retrieval and reasoning capabilities of models in diverse contexts. We use the NeedleBench framework to assess how well the leading open-source models can identify key information relevant to the question and apply that information to reasoning in bilingual long texts. Furthermore, we propose the Ancestral Trace Challenge (ATC) to mimic the complexity of logical reasoning challenges that are likely to be present in real-world long-context tasks, providing a simple method for evaluating LLMs in dealing with complex long-context situations. Our results suggest that current LLMs have significant room for improvement in practical long-context applications, as they struggle with the complexity of logical reasoning challenges that are likely to be present in real-world long-context tasks. All codes and resources are available at OpenCompass: https://github.com/open-compass/opencompass.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/078a9c3dcd0575dccc45c861618e2363caf47986",
      "citation_count": 18,
      "reference_count": 12,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The NeedleBench framework is used to assess how well the leading open-source models can identify key information relevant to the question and apply that information to reasoning in bilingual long texts, and the Ancestral Trace Challenge (ATC) is proposed to mimic the complexity of logical reasoning challenges that are likely to be present in real-world long-context tasks.",
      "external_id_dblp": "journals/corr/abs-2407-11963",
      "external_id_arxiv": "2407.11963",
      "external_id_doi": "10.48550/arXiv.2407.11963",
      "external_id_corpusid": 271218188
    },
    {
      "id": "2291515374",
      "type": "author",
      "name": "Mo Li"
    },
    {
      "id": "2266356137",
      "type": "author",
      "name": "Songyang Zhang"
    },
    {
      "id": "2311864117",
      "type": "author",
      "name": "Yunxin Liu"
    },
    {
      "id": "2261273470",
      "type": "author",
      "name": "Kai Chen"
    },
    {
      "id": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "paper",
      "title": "Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?",
      "abstract": "Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs' ability to natively ingest and process entire corpora of information offers numerous advantages. It enhances user-friendliness by eliminating the need for specialized knowledge of tools, provides robust end-to-end modeling that minimizes cascading errors in complex pipelines, and allows for the application of sophisticated prompting techniques across the entire system. To assess this paradigm shift, we introduce LOFT, a benchmark of real-world tasks requiring context up to millions of tokens designed to evaluate LCLMs' performance on in-context retrieval and reasoning. Our findings reveal LCLMs' surprising ability to rival state-of-the-art retrieval and RAG systems, despite never having been explicitly trained for these tasks. However, LCLMs still face challenges in areas like compositional reasoning that are required in SQL-like tasks. Notably, prompting strategies significantly influence performance, emphasizing the need for continued research as context lengths grow. Overall, LOFT provides a rigorous testing ground for LCLMs, showcasing their potential to supplant existing paradigms and tackle novel tasks as model capabilities scale.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "citation_count": 17,
      "reference_count": 69,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "",
      "external_id_dblp": "journals/corr/abs-2406-13121",
      "external_id_arxiv": "2406.13121",
      "external_id_doi": "10.48550/arXiv.2406.13121",
      "external_id_corpusid": 270620072
    },
    {
      "id": "2275576296",
      "type": "author",
      "name": "Jinhyuk Lee"
    },
    {
      "id": "52151521",
      "type": "author",
      "name": "Anthony Chen"
    },
    {
      "id": "2287053658",
      "type": "author",
      "name": "Zhuyun Dai"
    },
    {
      "id": "33546336",
      "type": "author",
      "name": "Dheeru Dua"
    },
    {
      "id": "39670454",
      "type": "author",
      "name": "Devendra Singh Sachan"
    },
    {
      "id": "51020741",
      "type": "author",
      "name": "Michael Boratko"
    },
    {
      "id": "2287130440",
      "type": "author",
      "name": "Yi Luan"
    },
    {
      "id": "2307469987",
      "type": "author",
      "name": "Vincent Perot"
    },
    {
      "id": "2290487862",
      "type": "author",
      "name": "Sid Dalmia"
    },
    {
      "id": "2307548497",
      "type": "author",
      "name": "Hexiang Hu"
    },
    {
      "id": "2307556437",
      "type": "author",
      "name": "Xudong Lin"
    },
    {
      "id": "2616463",
      "type": "author",
      "name": "Panupong Pasupat"
    },
    {
      "id": "2288902682",
      "type": "author",
      "name": "Aida Amini"
    },
    {
      "id": "2294173946",
      "type": "author",
      "name": "Jeremy R. Cole"
    },
    {
      "id": "2257279869",
      "type": "author",
      "name": "Sebastian Riedel"
    },
    {
      "id": "2296971",
      "type": "author",
      "name": "Iftekhar Naim"
    },
    {
      "id": "2277809314",
      "type": "author",
      "name": "Ming-Wei Chang"
    },
    {
      "id": "2091768",
      "type": "author",
      "name": "Kelvin Guu"
    },
    {
      "id": "b47507f12a8b7d26f78a484e32a08d61d3f87358",
      "type": "paper",
      "title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack",
      "abstract": "In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20\\% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented Generation methods achieve a modest 60\\% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "url": "https://www.semanticscholar.org/paper/b47507f12a8b7d26f78a484e32a08d61d3f87358",
      "citation_count": 35,
      "reference_count": 85,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The BABILong benchmark is introduced, designed to test language models' ability to reason across facts distributed in extremely long documents, and is extendable to any length to support the evaluation of new upcoming models with increased capabilities.",
      "external_id_dblp": "journals/corr/abs-2406-10149",
      "external_id_arxiv": "2406.10149",
      "external_id_doi": "10.48550/arXiv.2406.10149",
      "external_id_corpusid": 270521583
    },
    {
      "id": "51114080",
      "type": "author",
      "name": "Yuri Kuratov"
    },
    {
      "id": "2176183932",
      "type": "author",
      "name": "Aydar Bulatov"
    },
    {
      "id": "2284590453",
      "type": "author",
      "name": "Petr Anokhin"
    },
    {
      "id": "2306782862",
      "type": "author",
      "name": "Ivan Rodkin"
    },
    {
      "id": "2284591109",
      "type": "author",
      "name": "Dmitry Sorokin"
    },
    {
      "id": "2284590611",
      "type": "author",
      "name": "Artyom Sorokin"
    },
    {
      "id": "2284592236",
      "type": "author",
      "name": "M. Burtsev"
    },
    {
      "id": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "paper",
      "title": "Many-Shot In-Context Learning",
      "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. We also find that inference cost increases linearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL to varying degrees. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "url": "https://www.semanticscholar.org/paper/603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "citation_count": 74,
      "reference_count": 74,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "It is demonstrated that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning.",
      "external_id_arxiv": "2404.11018",
      "external_id_dblp": "journals/corr/abs-2404-11018",
      "external_id_doi": "10.48550/arXiv.2404.11018",
      "external_id_corpusid": 269187943
    },
    {
      "id": "2258553001",
      "type": "author",
      "name": "Rishabh Agarwal"
    },
    {
      "id": "2258779676",
      "type": "author",
      "name": "Avi Singh"
    },
    {
      "id": "2297176886",
      "type": "author",
      "name": "Lei M. Zhang"
    },
    {
      "id": "2297673118",
      "type": "author",
      "name": "Stephanie Chan"
    },
    {
      "id": "2176782672",
      "type": "author",
      "name": "Azade Nova"
    },
    {
      "id": "2273515439",
      "type": "author",
      "name": "John D. Co-Reyes"
    },
    {
      "id": "2296992073",
      "type": "author",
      "name": "Eric Chu"
    },
    {
      "id": "145124447",
      "type": "author",
      "name": "Feryal M. P. Behbahani"
    },
    {
      "id": "2258552654",
      "type": "author",
      "name": "Aleksandra Faust"
    },
    {
      "id": "2268317474",
      "type": "author",
      "name": "Hugo Larochelle"
    },
    {
      "id": "344fa52471306a25133c9536992be15eecdd1c60",
      "type": "paper",
      "title": "One Thousand and One Pairs: A “novel” challenge for long-context language models",
      "abstract": "Synthetic long-context LLM benchmarks (e.g., “needle-in-the-haystack”) test only surface-level retrieval capabilities; but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest pair accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "url": "https://www.semanticscholar.org/paper/344fa52471306a25133c9536992be15eecdd1c60",
      "citation_count": 22,
      "reference_count": 66,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "NoCha is a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books, and reveals that on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning.",
      "external_id_dblp": "journals/corr/abs-2406-16264",
      "external_id_arxiv": "2406.16264",
      "external_id_acl": "2024.emnlp-main.948",
      "external_id_doi": "10.48550/arXiv.2406.16264",
      "external_id_corpusid": 270703648
    },
    {
      "id": "37796923",
      "type": "author",
      "name": "Marzena Karpinska"
    },
    {
      "id": "1972292021",
      "type": "author",
      "name": "Katherine Thai"
    },
    {
      "id": "2308032002",
      "type": "author",
      "name": "Kyle Lo"
    },
    {
      "id": "2253400779",
      "type": "author",
      "name": "Tanya Goyal"
    },
    {
      "id": "2136562",
      "type": "author",
      "name": "Mohit Iyyer"
    },
    {
      "id": "879905229dd629570165ab92cfa4045d7de0cbe5",
      "type": "paper",
      "title": "Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models",
      "abstract": "Multimodal Large Language Models (MLLMs) have shown significant promise in various applications, leading to broad interest from researchers and practitioners alike. However, a comprehensive evaluation of their long-context capabilities remains underexplored. To address these gaps, we introduce the MultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to assess the long-context capabilities of MLLMs. Besides multi-image input, we employ image stitching to further increase the input context length, and develop a protocol to automatically generate labels for sub-image level retrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their capability to locate a target sub-image (needle) within a set of images (haystack) based on textual instructions and descriptions of image contents. This setup necessitates an advanced understanding of extensive visual contexts and effective information retrieval within long-context image inputs. With this benchmark, we evaluate state-of-the-art MLLMs, encompassing both API-based and open-source models. The findings reveal that GPT-4o consistently surpasses other models in long-context scenarios, but suffers from hallucination problems in negative samples, i.e., when needles are not in the haystacks. Our comprehensive long-context evaluation of MLLMs also sheds lights on the considerable performance gap between API-based and open-source models. All the code, data, and instructions required to reproduce the main results are available at https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/879905229dd629570165ab92cfa4045d7de0cbe5",
      "citation_count": 8,
      "reference_count": 38,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A comprehensive long-context evaluation of MLLMs is introduced, revealing that GPT-4o consistently surpasses other models in long-context scenarios, but suffers from hallucination problems in negative samples, i.e., when needles are not in the haystacks.",
      "external_id_arxiv": "2406.11230",
      "external_id_dblp": "journals/corr/abs-2406-11230",
      "external_id_doi": "10.48550/arXiv.2406.11230",
      "external_id_corpusid": 270559255
    },
    {
      "id": "2298494612",
      "type": "author",
      "name": "Hengyi Wang"
    },
    {
      "id": "2260379784",
      "type": "author",
      "name": "Haizhou Shi"
    },
    {
      "id": "2307066146",
      "type": "author",
      "name": "Shiwei Tan"
    },
    {
      "id": "2298275515",
      "type": "author",
      "name": "Weiyi Qin"
    },
    {
      "id": "2298410609",
      "type": "author",
      "name": "Wenyuan Wang"
    },
    {
      "id": "2306947387",
      "type": "author",
      "name": "Tunyu Zhang"
    },
    {
      "id": "51464520",
      "type": "author",
      "name": "A. Nambi"
    },
    {
      "id": "1785978",
      "type": "author",
      "name": "T. Ganu"
    },
    {
      "id": "2307433841",
      "type": "author",
      "name": "Hao Wang"
    },
    {
      "id": "76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d",
      "type": "paper",
      "title": "NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens",
      "abstract": "The rapid advancement of Large Language Models (LLMs) has introduced a new frontier in natural language processing, particularly in understanding and processing long-context information. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically designed to test the capabilities of LLMs with extended texts. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper presents the design and construction of NovelQA, highlighting its manual annotation, and diverse question types. Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-oriented questions, and extremely long input with an average length more than 200,000 tokens. The results underscore the necessity for further advancements in LLMs to improve their long-context comprehension.",
      "year": 2024,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d",
      "citation_count": 2,
      "reference_count": 52,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The design and construction of NovelQA is presented, highlighting its manual annotation, and diverse question types, and significant insights into the models' performance are revealed, particularly emphasizing the challenges they face with multi-hop reasoning, detail-oriented questions, and extremely long input with an average length more than 200,000 tokens.",
      "external_id_arxiv": "2403.12766",
      "external_id_corpusid": 268531221
    },
    {
      "id": "35504092",
      "type": "author",
      "name": "Cunxiang Wang"
    },
    {
      "id": "30819687",
      "type": "author",
      "name": "Ruoxi Ning"
    },
    {
      "id": "2292184774",
      "type": "author",
      "name": "Boqi Pan"
    },
    {
      "id": "2292208424",
      "type": "author",
      "name": "Tonghui Wu"
    },
    {
      "id": "3187768",
      "type": "author",
      "name": "Qipeng Guo"
    },
    {
      "id": "2292147095",
      "type": "author",
      "name": "Cheng Deng"
    },
    {
      "id": "1993226927",
      "type": "author",
      "name": "Guangsheng Bao"
    },
    {
      "id": "2292261580",
      "type": "author",
      "name": "Qian Wang"
    },
    {
      "id": "2261496744",
      "type": "author",
      "name": "Yue Zhang"
    },
    {
      "id": "0ed0d844544c4a2981acccb9332dead922294664",
      "type": "paper",
      "title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models",
      "abstract": "Developing Large Language Models (LLMs) with robust long-context capabilities has been the recent research focus, resulting in the emergence of long-context LLMs proficient in Chinese. However, the evaluation of these models remains underdeveloped due to a lack of benchmarks. To address this gap, we present CLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs. CLongEval is characterized by three key features: (1) Sufficient data volume, comprising 7 distinct tasks and 7,267 examples; (2) Broad applicability, accommodating to models with context windows size from 1K to 100K; (3) High quality, with over 2,000 manually annotated question-answer pairs in addition to the automatically constructed labels. With CLongEval, we undertake a comprehensive assessment of 6 open-source long-context LLMs and 2 leading commercial counterparts that feature both long-context abilities and proficiency in Chinese. We also provide in-depth analysis based on the empirical results, trying to shed light on the critical capabilities that present challenges in long-context settings. The dataset, evaluation scripts, and model outputs are released.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "url": "https://www.semanticscholar.org/paper/0ed0d844544c4a2981acccb9332dead922294664",
      "citation_count": 2,
      "reference_count": 51,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work presents CLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs, and undertake a comprehensive assessment of 6 open-source long-context LLMs and 2 leading commercial counterparts that feature both long-context abilities and proficiency in Chinese.",
      "external_id_arxiv": "2403.03514",
      "external_id_dblp": "journals/corr/abs-2403-03514",
      "external_id_doi": "10.48550/arXiv.2403.03514",
      "external_id_corpusid": 268253182
    },
    {
      "id": "2279541575",
      "type": "author",
      "name": "Zexuan Qiu"
    },
    {
      "id": "2274072714",
      "type": "author",
      "name": "Jingjing Li"
    },
    {
      "id": "2149271292",
      "type": "author",
      "name": "Shijue Huang"
    },
    {
      "id": "2290075391",
      "type": "author",
      "name": "Wanjun Zhong"
    },
    {
      "id": "2273926416",
      "type": "author",
      "name": "Irwin King"
    },
    {
      "id": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "paper",
      "title": "∞Bench: Extending Long Context Evaluation Beyond 100K Tokens",
      "abstract": "Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.",
      "year": 2024,
      "venue": "Volume 1",
      "url": "https://www.semanticscholar.org/paper/f05e84702562cb693dd68d3d1c88072519a7bd71",
      "citation_count": 101,
      "reference_count": 53,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper proposes $\\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens, and evaluates the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts.",
      "external_id_arxiv": "2402.13718",
      "external_id_dblp": "journals/corr/abs-2402-13718",
      "external_id_doi": "10.48550/arXiv.2402.13718",
      "external_id_corpusid": 267770255
    },
    {
      "id": "2254576790",
      "type": "author",
      "name": "Xinrong Zhang"
    },
    {
      "id": "2109274417",
      "type": "author",
      "name": "Yingfa Chen"
    },
    {
      "id": "1576223501",
      "type": "author",
      "name": "Shengding Hu"
    },
    {
      "id": "2284948588",
      "type": "author",
      "name": "Zihang Xu"
    },
    {
      "id": "2284931475",
      "type": "author",
      "name": "Junhao Chen"
    },
    {
      "id": "2284863107",
      "type": "author",
      "name": "Moo Khai Hao"
    },
    {
      "id": "48506411",
      "type": "author",
      "name": "Xu Han"
    },
    {
      "id": "2284862784",
      "type": "author",
      "name": "Zhen Leng Thai"
    },
    {
      "id": "2267033597",
      "type": "author",
      "name": "Shuo Wang"
    },
    {
      "id": "2141313179",
      "type": "author",
      "name": "Zhiyuan Liu"
    },
    {
      "id": "2273551430",
      "type": "author",
      "name": "Maosong Sun"
    },
    {
      "id": "79d94b9c8f17f754c35bceb29eb76d7a39664c5b",
      "type": "paper",
      "title": "Exploring Large Language Models for Semantic Analysis and Categorization of Android Malware",
      "abstract": "Malware analysis is a complex process of examining and evaluating malicious software's functionality, origin, and potential impact. This arduous process typically involves dissecting the software to understand its components, infection vector, propagation mechanism, and payload. Over the years, deep reverse engineering of malware has become increasingly tedious, mainly due to modern malicious codebases' fast evolution and sophistication. Essentially, analysts are tasked with identifying the elusive needle in the haystack within the complexities of zero-day malware, all while under tight time constraints. Thus, in this paper, we explore leveraging Large Language Models (LLMs) for semantic malware analysis to expedite the analysis of known and novel samples. Built on GPT-4o-mini model, \\msp is designed to augment malware analysis for Android through a hierarchical-tiered summarization chain and strategic prompt engineering. Additionally, \\msp performs malware categorization, distinguishing potential malware from benign applications, thereby saving time during the malware reverse engineering process. Despite not being fine-tuned for Android malware analysis, we demonstrate that through optimized and advanced prompt engineering \\msp can achieve up to 77% classification accuracy while providing highly robust summaries at functional, class, and package levels. In addition, leveraging the backward tracing of the summaries from package to function levels allowed us to pinpoint the precise code snippets responsible for malicious behavior.",
      "year": 2025,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/79d94b9c8f17f754c35bceb29eb76d7a39664c5b",
      "citation_count": 1,
      "reference_count": 0,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper explores leveraging Large Language Models (LLMs) for semantic malware analysis to expedite the analysis of known and novel samples and demonstrates that through optimized and advanced prompt engineering, \\msp can achieve up to 77% classification accuracy while providing highly robust summaries at functional, class, and package levels.",
      "external_id_dblp": "journals/corr/abs-2501-04848",
      "external_id_arxiv": "2501.04848",
      "external_id_doi": "10.48550/arXiv.2501.04848",
      "external_id_corpusid": 275405967
    },
    {
      "id": "2339267799",
      "type": "author",
      "name": "Brandon J Walton"
    },
    {
      "id": "1654097718",
      "type": "author",
      "name": "Mst. Eshita Khatun"
    },
    {
      "id": "2273966090",
      "type": "author",
      "name": "J. Ghawaly"
    },
    {
      "id": "1405645138",
      "type": "author",
      "name": "Aisha I. Ali-Gombe"
    },
    {
      "id": "f02a5b86269fe40498e58061bb9b85a1c8b73a12",
      "type": "paper",
      "title": "Maltracker: A Fine-Grained NPM Malware Tracker Copiloted by LLM-Enhanced Dataset",
      "abstract": "",
      "year": 2024,
      "venue": "International Symposium on Software Testing and Analysis",
      "url": "https://www.semanticscholar.org/paper/f02a5b86269fe40498e58061bb9b85a1c8b73a12",
      "citation_count": 6,
      "reference_count": 17,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "",
      "external_id_dblp": "conf/issta/Yu0G024",
      "external_id_doi": "10.1145/3650212.3680397",
      "external_id_corpusid": 272608858
    },
    {
      "id": "2223172928",
      "type": "author",
      "name": "Zeliang Yu"
    },
    {
      "id": "2268784491",
      "type": "author",
      "name": "Ming Wen"
    },
    {
      "id": "2321401079",
      "type": "author",
      "name": "Xiaochen Guo"
    },
    {
      "id": "2240987357",
      "type": "author",
      "name": "Hai Jin"
    },
    {
      "id": "f8bb6887a591872b47f75d28b22baba7b0521655",
      "type": "paper",
      "title": "RACONTEUR: A Knowledgeable, Insightful, and Portable LLM-Powered Shell Command Explainer",
      "abstract": "Malicious shell commands are linchpins to many cyber-attacks, but may not be easy to understand by security analysts due to complicated and often disguised code structures. Advances in large language models (LLMs) have unlocked the possibility of generating understandable explanations for shell commands. However, existing general-purpose LLMs suffer from a lack of expert knowledge and a tendency to hallucinate in the task of shell command explanation. In this paper, we present Raconteur, a knowledgeable, expressive and portable shell command explainer powered by LLM. Raconteur is infused with professional knowledge to provide comprehensive explanations on shell commands, including not only what the command does (i.e., behavior) but also why the command does it (i.e., purpose). To shed light on the high-level intent of the command, we also translate the natural-language-based explanation into standard technique&tactic defined by MITRE ATT&CK, the worldwide knowledge base of cybersecurity. To enable Raconteur to explain unseen private commands, we further develop a documentation retriever to obtain relevant information from complementary documentations to assist the explanation process. We have created a large-scale dataset for training and conducted extensive experiments to evaluate the capability of Raconteur in shell command explanation. The experiments verify that Raconteur is able to provide high-quality explanations and in-depth insight of the intent of the command.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/f8bb6887a591872b47f75d28b22baba7b0521655",
      "citation_count": 4,
      "reference_count": 62,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Raconteur is infused with professional knowledge to provide comprehensive explanations on shell commands, including not only what the command does but also why the command does it, and a documentation retriever to obtain relevant information from complementary documentations to assist the explanation process.",
      "external_id_arxiv": "2409.02074",
      "external_id_dblp": "journals/corr/abs-2409-02074",
      "external_id_doi": "10.48550/arXiv.2409.02074",
      "external_id_corpusid": 272367794
    },
    {
      "id": "2189520187",
      "type": "author",
      "name": "Jiangyi Deng"
    },
    {
      "id": "2144405890",
      "type": "author",
      "name": "Xinfeng Li"
    },
    {
      "id": "2291606067",
      "type": "author",
      "name": "Yanjiao Chen"
    },
    {
      "id": "2130210943",
      "type": "author",
      "name": "Yijie Bai"
    },
    {
      "id": "2267734457",
      "type": "author",
      "name": "Haiqin Weng"
    },
    {
      "id": "2312389838",
      "type": "author",
      "name": "Yan Liu"
    },
    {
      "id": "2319449261",
      "type": "author",
      "name": "Tao Wei"
    },
    {
      "id": "2316678337",
      "type": "author",
      "name": "Wenyuan Xu"
    },
    {
      "id": "ca100914a70cf077eaff278b4827cdeeee742868",
      "type": "paper",
      "title": "ReCDA: Concept Drift Adaptation with Representation Enhancement for Network Intrusion Detection",
      "abstract": "",
      "year": 2024,
      "venue": "Knowledge Discovery and Data Mining",
      "url": "https://www.semanticscholar.org/paper/ca100914a70cf077eaff278b4827cdeeee742868",
      "citation_count": 1,
      "reference_count": 23,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "",
      "external_id_dblp": "conf/kdd/YangZLXWN24",
      "external_id_doi": "10.1145/3637528.3672007",
      "external_id_corpusid": 271955016
    },
    {
      "id": "2288749348",
      "type": "author",
      "name": "Shuo Yang"
    },
    {
      "id": "2288552032",
      "type": "author",
      "name": "Jinze Li"
    },
    {
      "id": "2185907462",
      "type": "author",
      "name": "Jinfeng Xu"
    },
    {
      "id": "2220666117",
      "type": "author",
      "name": "Xingjun Wang"
    },
    {
      "id": "2288524645",
      "type": "author",
      "name": "Edith C. H. Ngai"
    },
    {
      "id": "50fb8bcb90734b7b86cb257162038fcde3b47bad",
      "type": "paper",
      "title": "Tactics, Techniques, and Procedures (TTPs) in Interpreted Malware: A Zero-Shot Generation with Large Language Models",
      "abstract": "Nowadays, the open-source software (OSS) ecosystem suffers from security threats of software supply chain (SSC) attacks. Interpreted OSS malware plays a vital role in SSC attacks, as criminals have an arsenal of attack vectors to deceive users into installing malware and executing malicious activities. In this paper, we introduce tactics, techniques, and procedures (TTPs) proposed by MITRE ATT\\&CK into the interpreted malware analysis to characterize different phases of an attack lifecycle. Specifically, we propose GENTTP, a zero-shot approach to extracting a TTP of an interpreted malware package. GENTTP leverages large language models (LLMs) to automatically generate a TTP, where the input is a malicious package, and the output is a deceptive tactic and an execution tactic of attack vectors. To validate the effectiveness of GENTTP, we collect two datasets for evaluation: a dataset with ground truth labels and a large dataset in the wild. Experimental results show that GENTTP can generate TTPs with high accuracy and efficiency. To demonstrate GENTTP's benefits, we build an LLM-based Chatbot from 3,700+ PyPI malware's TTPs. We further conduct a quantitative analysis of malware's TTPs at a large scale. Our main findings include: (1) many OSS malicious packages share a relatively stable TTP, even with the increasing emergence of malware and attack campaigns, (2) a TTP reflects characteristics of a malware-based attack, and (3) an attacker's intent behind the malware is linked to a TTP.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/50fb8bcb90734b7b86cb257162038fcde3b47bad",
      "citation_count": 1,
      "reference_count": 78,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper proposes GENTTP, a zero-shot approach to extracting a TTP of an interpreted malware package that leverages large language models (LLMs) to automatically generate a TTP, where the input is a malicious package, and the output is a deceptive tactic and an execution tactic of attack vectors.",
      "external_id_arxiv": "2407.08532",
      "external_id_dblp": "journals/corr/abs-2407-08532",
      "external_id_doi": "10.48550/arXiv.2407.08532",
      "external_id_corpusid": 271097727
    },
    {
      "id": "2295694533",
      "type": "author",
      "name": "Ying Zhang"
    },
    {
      "id": "2295846651",
      "type": "author",
      "name": "Xiaoyan Zhou"
    },
    {
      "id": "2311384262",
      "type": "author",
      "name": "Hui Wen"
    },
    {
      "id": "2306959990",
      "type": "author",
      "name": "Wenjia Niu"
    },
    {
      "id": "2294928107",
      "type": "author",
      "name": "Jiqiang Liu"
    },
    {
      "id": "2145333268",
      "type": "author",
      "name": "Haining Wang"
    },
    {
      "id": "2295845506",
      "type": "author",
      "name": "Qiang Li"
    },
    {
      "id": "f82f49c20c6acc69f884f05e3a9f1ceea91061ce",
      "type": "paper",
      "title": "Detecting hallucinations in large language models using semantic entropy",
      "abstract": "",
      "year": 2024,
      "venue": "The Naturalist",
      "url": "https://www.semanticscholar.org/paper/f82f49c20c6acc69f884f05e3a9f1ceea91061ce",
      "citation_count": 130,
      "reference_count": 27,
      "fields_of_study": "[\"Medicine\", \"Computer Science\"]",
      "is_open_access": true,
      "tldr": "New methods grounded in statistics are proposed for detecting hallucinations in LLMs, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations.",
      "open_access_pdf_url": "https://www.nature.com/articles/s41586-024-07421-0.pdf",
      "open_access_status": "HYBRID",
      "external_id_dblp": "journals/nature/FarquharKKG24",
      "external_id_pubmedcentral": "11186750",
      "external_id_doi": "10.1038/s41586-024-07421-0",
      "external_id_corpusid": 270615909,
      "external_id_pubmed": "38898292"
    },
    {
      "id": "33859827",
      "type": "author",
      "name": "Sebastian Farquhar"
    },
    {
      "id": "2064853201",
      "type": "author",
      "name": "Jannik Kossen"
    },
    {
      "id": "39879848",
      "type": "author",
      "name": "Lorenz Kuhn"
    },
    {
      "id": "2303846295",
      "type": "author",
      "name": "Yarin Gal"
    },
    {
      "id": "4bfd5e2221b88b9d38c7111fc0a525027f64b619",
      "type": "paper",
      "title": "Transfer Learning in Pre-Trained Large Language Models for Malware Detection Based on System Calls",
      "abstract": "In the current cybersecurity landscape, protecting military devices such as communication and battlefield management systems against sophisticated cyber attacks is crucial. Malware exploits vulnerabilities through stealth methods, often evading traditional detection mechanisms such as software signatures. The application of ML/DL in vulnerability detection has been extensively explored in the literature. However, current ML/DL vulnerability detection methods struggle with understanding the context and intent behind complex attacks. Integrating large language models (LLMs) with system call analysis offers a promising approach to enhance malware detection. This work presents a novel framework leveraging LLMs to classify malware based on system call data. The framework uses transfer learning to adapt pre-trained LLMs for malware detection. By retraining LLMs on a dataset of benign and malicious system calls, the models are refined to detect signs of malware activity. Experiments with a dataset of over 1TB of system calls demonstrate that models with larger context sizes, such as BigBird and Longformer, achieve superior accuracy and an F1-Score of approximately 0.86. The results highlight the importance of context size in improving detection rates and underscore the trade-offs between computational complexity and performance. This approach shows significant potential for real-time detection in high-stakes environments, offering a robust solution to evolving cyber threats.",
      "year": 2024,
      "venue": "IEEE Military Communications Conference",
      "url": "https://www.semanticscholar.org/paper/4bfd5e2221b88b9d38c7111fc0a525027f64b619",
      "citation_count": 3,
      "reference_count": 24,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "This work presents a novel framework leveraging LLMs to classify malware based on system call data, using transfer learning to adapt pre-trained LLMs for malware detection and shows significant potential for real-time detection in high-stakes environments.",
      "open_access_pdf_url": "https://arxiv.org/pdf/2405.09318",
      "open_access_status": "GREEN",
      "external_id_dblp": "conf/milcom/SanchezCBP24",
      "external_id_arxiv": "2405.09318",
      "external_id_doi": "10.1109/MILCOM61039.2024.10773857",
      "external_id_corpusid": 269773304
    },
    {
      "id": "31039603",
      "type": "author",
      "name": "P. Sánchez"
    },
    {
      "id": "2301312718",
      "type": "author",
      "name": "Alberto Huertas Celdr'an"
    },
    {
      "id": "2280919953",
      "type": "author",
      "name": "Gérôme Bovet"
    },
    {
      "id": "2262988626",
      "type": "author",
      "name": "Gregorio Martínez Pérez"
    },
    {
      "id": "c37b66cae380ff6bc210338fef200ec1874ceb5f",
      "type": "paper",
      "title": "AppPoet: Large Language Model based Android malware detection via multi-view prompt engineering",
      "abstract": "Due to the vast array of Android applications, their multifarious functions and intricate behavioral semantics, attackers can adopt various tactics to conceal their genuine attack intentions within legitimate functions. However, numerous learning-based methods suffer from a limitation in mining behavioral semantic information, thus impeding the accuracy and efficiency of Android malware detection. Besides, the majority of existing learning-based methods are weakly interpretive and fail to furnish researchers with effective and readable detection reports. Inspired by the success of the Large Language Models (LLMs) in natural language understanding, we propose AppPoet, a LLM-assisted multi-view system for Android malware detection. Firstly, AppPoet employs a static method to comprehensively collect application features and formulate various observation views. Then, using our carefully crafted multi-view prompt templates, it guides the LLM to generate function descriptions and behavioral summaries for each view, enabling deep semantic analysis of the views. Finally, we collaboratively fuse the multi-view information to efficiently and accurately detect malware through a deep neural network (DNN) classifier and then generate the human-readable diagnostic reports. Experimental results demonstrate that our method achieves a detection accuracy of 97.15% and an F1 score of 97.21%, which is superior to the baseline methods. Furthermore, the case study evaluates the effectiveness of our generated diagnostic reports.",
      "year": 2024,
      "venue": "Expert systems with applications",
      "url": "https://www.semanticscholar.org/paper/c37b66cae380ff6bc210338fef200ec1874ceb5f",
      "citation_count": 5,
      "reference_count": 70,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Inspired by the success of the Large Language Models (LLMs) in natural language understanding, AppPoet is proposed, a LLM-assisted multi-view system for Android malware detection that achieves a detection accuracy of 97.15% and an F1 score of 97.21%, which is superior to the baseline methods.",
      "external_id_arxiv": "2404.18816",
      "external_id_dblp": "journals/corr/abs-2404-18816",
      "external_id_doi": "10.48550/arXiv.2404.18816",
      "external_id_corpusid": 269449532
    },
    {
      "id": "2298888414",
      "type": "author",
      "name": "Wenxiang Zhao"
    },
    {
      "id": "2298963125",
      "type": "author",
      "name": "Juntao Wu"
    },
    {
      "id": "3464094",
      "type": "author",
      "name": "Zhaoyi Meng"
    },
    {
      "id": "3882788d42339848f5828362e367437e9a5447e0",
      "type": "paper",
      "title": "Out of Sight, Out of Mind: Better Automatic Vulnerability Repair by Broadening Input Ranges and Sources",
      "abstract": "The advances of deep learning (DL) have paved the way for auto-matic software vulnerability repair approaches, which effectively learn the mapping from the vulnerable code to the fixed code. Never-theless, existing DL-based vulnerability repair methods face notable limitations: 1) they struggle to handle lengthy vulnerable code, 2) they treat code as natural language texts, neglecting its inherent structure, and 3) they do not tap into the valuable expert knowledge present in the expert system. To address this, we propose VulMaster, a Transformer-based neural network model that excels at generating vulnerability repairs by comprehensively understanding the entire vulnerable code, irrespective of its length. This model also integrates diverse information, encompassing vulnerable code structures and expert knowledge from the CWE system. We evaluated VulMaster on a real-world C/C++ vulnerability repair dataset comprising 1,754 projects with 5,800 vulnerable functions. The experimental results demonstrated that VulMaster exhibits substan-tial improvements compared to the learning-based state-of-the-art vulnerability repair approach. Specifically, VulMaster improves the EM, BLEU, and CodeBLEU scores from 10.2% to 20.0%, 21.3% to 29.3%, and 32.5% to 40.9%, respectively.",
      "year": 2024,
      "venue": "International Conference on Software Engineering",
      "url": "https://www.semanticscholar.org/paper/3882788d42339848f5828362e367437e9a5447e0",
      "citation_count": 18,
      "reference_count": 81,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "VulMaster is proposed, a Transformer-based neural network model that excels at generating vulnerability repairs by comprehensively understanding the entire vulnerable code, irrespective of its length, and integrates diverse information, encompassing vulnerable code structures and expert knowledge from the CWE system.",
      "open_access_pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3597503.3639222",
      "open_access_status": "HYBRID",
      "external_id_dblp": "conf/icse/ZhouKXH024",
      "external_id_doi": "10.1145/3597503.3639222",
      "external_id_corpusid": 269131469
    },
    {
      "id": "2148928671",
      "type": "author",
      "name": "Xin Zhou"
    },
    {
      "id": "35276441",
      "type": "author",
      "name": "Kisub Kim"
    },
    {
      "id": "2203459",
      "type": "author",
      "name": "Bowen Xu"
    },
    {
      "id": "7883212",
      "type": "author",
      "name": "Donggyun Han"
    },
    {
      "id": "2266943614",
      "type": "author",
      "name": "David Lo"
    },
    {
      "id": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "paper",
      "title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
      "abstract": "We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "citation_count": 101,
      "reference_count": 41,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens, which substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks.",
      "external_id_arxiv": "2406.11931",
      "external_id_dblp": "journals/corr/abs-2406-11931",
      "external_id_doi": "10.48550/arXiv.2406.11931",
      "external_id_corpusid": 270562723
    },
    {
      "id": "2307073216",
      "type": "author",
      "name": "DeepSeek-AI"
    },
    {
      "id": "2278223869",
      "type": "author",
      "name": "Qihao Zhu"
    },
    {
      "id": "2278834796",
      "type": "author",
      "name": "Daya Guo"
    },
    {
      "id": "144485528",
      "type": "author",
      "name": "Zhihong Shao"
    },
    {
      "id": "2278404250",
      "type": "author",
      "name": "Dejian Yang"
    },
    {
      "id": "2289796300",
      "type": "author",
      "name": "Peiyi Wang"
    },
    {
      "id": "1748844142",
      "type": "author",
      "name": "Runxin Xu"
    },
    {
      "id": "2276752202",
      "type": "author",
      "name": "Y. Wu"
    },
    {
      "id": "2307219221",
      "type": "author",
      "name": "Yukun Li"
    },
    {
      "id": "2278395340",
      "type": "author",
      "name": "Huazuo Gao"
    },
    {
      "id": "2278830967",
      "type": "author",
      "name": "Shirong Ma"
    },
    {
      "id": "2279051415",
      "type": "author",
      "name": "Wangding Zeng"
    },
    {
      "id": "2281036475",
      "type": "author",
      "name": "Xiao Bi"
    },
    {
      "id": "2307422404",
      "type": "author",
      "name": "Zihui Gu"
    },
    {
      "id": "2278643857",
      "type": "author",
      "name": "Hanwei Xu"
    },
    {
      "id": "2307085654",
      "type": "author",
      "name": "Damai Dai"
    },
    {
      "id": "2278218552",
      "type": "author",
      "name": "Kai Dong"
    },
    {
      "id": "2278257096",
      "type": "author",
      "name": "Liyue Zhang"
    },
    {
      "id": "2278218072",
      "type": "author",
      "name": "Yishi Piao"
    },
    {
      "id": "1797090",
      "type": "author",
      "name": "Zhibin Gou"
    },
    {
      "id": "2279107352",
      "type": "author",
      "name": "Zhenda Xie"
    },
    {
      "id": "2278220653",
      "type": "author",
      "name": "Zhewen Hao"
    },
    {
      "id": "2244285627",
      "type": "author",
      "name": "Bing-Li Wang"
    },
    {
      "id": "2258088582",
      "type": "author",
      "name": "Jun-Mei Song"
    },
    {
      "id": "2274200088",
      "type": "author",
      "name": "Deli Chen"
    },
    {
      "id": "2239599436",
      "type": "author",
      "name": "Xin Xie"
    },
    {
      "id": "2278219877",
      "type": "author",
      "name": "Kang Guan"
    },
    {
      "id": "2054452755",
      "type": "author",
      "name": "Yu-mei You"
    },
    {
      "id": "2278677183",
      "type": "author",
      "name": "A. Liu"
    },
    {
      "id": "2278218583",
      "type": "author",
      "name": "Qiushi Du"
    },
    {
      "id": "2272467392",
      "type": "author",
      "name": "W. Gao"
    },
    {
      "id": "2307222549",
      "type": "author",
      "name": "Xuan Lu"
    },
    {
      "id": "2307394725",
      "type": "author",
      "name": "Qinyu Chen"
    },
    {
      "id": "2278432120",
      "type": "author",
      "name": "Yaohui Wang"
    },
    {
      "id": "2278221484",
      "type": "author",
      "name": "C. Deng"
    },
    {
      "id": "2278337977",
      "type": "author",
      "name": "Jiashi Li"
    },
    {
      "id": "2278389597",
      "type": "author",
      "name": "Chenggang Zhao"
    },
    {
      "id": "2278217940",
      "type": "author",
      "name": "C. Ruan"
    },
    {
      "id": "2278218736",
      "type": "author",
      "name": "Fuli Luo"
    },
    {
      "id": "2278618633",
      "type": "author",
      "name": "W. Liang"
    },
    {
      "id": "5eac2a40422a7085cb6f03285ad08210b6f6744b",
      "type": "paper",
      "title": "OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement",
      "abstract": "The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "url": "https://www.semanticscholar.org/paper/5eac2a40422a7085cb6f03285ad08210b6f6744b",
      "citation_count": 79,
      "reference_count": 45,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work introduces OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code that brings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter into focus.",
      "external_id_arxiv": "2402.14658",
      "external_id_dblp": "journals/corr/abs-2402-14658",
      "external_id_doi": "10.48550/arXiv.2402.14658",
      "external_id_corpusid": 267782452
    },
    {
      "id": "2268491856",
      "type": "author",
      "name": "Tianyu Zheng"
    },
    {
      "id": "2143853895",
      "type": "author",
      "name": "Ge Zhang"
    },
    {
      "id": "2057973326",
      "type": "author",
      "name": "Tianhao Shen"
    },
    {
      "id": "2285130258",
      "type": "author",
      "name": "Xueling Liu"
    },
    {
      "id": "2285251994",
      "type": "author",
      "name": "Bill Yuchen Lin"
    },
    {
      "id": "2276508494",
      "type": "author",
      "name": "Jie Fu"
    },
    {
      "id": "2249847177",
      "type": "author",
      "name": "Wenhu Chen"
    },
    {
      "id": "2284988933",
      "type": "author",
      "name": "Xiang Yue"
    },
    {
      "id": "8946891e94831adc8cddb0d32311cce2445c96d2",
      "type": "paper",
      "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
      "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "url": "https://www.semanticscholar.org/paper/8946891e94831adc8cddb0d32311cce2445c96d2",
      "citation_count": 344,
      "reference_count": 96,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning, but it still falls short of human performance by 10.4%, which underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks.",
      "external_id_dblp": "conf/iclr/LuBX0LH0CG024",
      "external_id_arxiv": "2310.02255",
      "external_id_corpusid": 264491155
    },
    {
      "id": "2887562",
      "type": "author",
      "name": "Pan Lu"
    },
    {
      "id": "103404553",
      "type": "author",
      "name": "Hritik Bansal"
    },
    {
      "id": "2143749775",
      "type": "author",
      "name": "Tony Xia"
    },
    {
      "id": "2144174497",
      "type": "author",
      "name": "Jiacheng Liu"
    },
    {
      "id": "2109738542",
      "type": "author",
      "name": "Chun-yue Li"
    },
    {
      "id": "2548384",
      "type": "author",
      "name": "Hannaneh Hajishirzi"
    },
    {
      "id": "47413820",
      "type": "author",
      "name": "Hao Cheng"
    },
    {
      "id": "2256646491",
      "type": "author",
      "name": "Kai-Wei Chang"
    },
    {
      "id": "2253458981",
      "type": "author",
      "name": "Michel Galley"
    },
    {
      "id": "2256227183",
      "type": "author",
      "name": "Jianfeng Gao"
    },
    {
      "id": "fc6a2f7478f68adefd69e2071f27e38aa1647f2f",
      "type": "paper",
      "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
      "abstract": "In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.",
      "year": 2023,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/fc6a2f7478f68adefd69e2071f27e38aa1647f2f",
      "citation_count": 551,
      "reference_count": 86,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks.",
      "external_id_arxiv": "2308.12966",
      "external_id_corpusid": 261101015
    },
    {
      "id": "3768186",
      "type": "author",
      "name": "Shuai Bai"
    },
    {
      "id": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "paper",
      "title": "MiniCPM-V: A GPT-4V Level MLLM on Your Phone",
      "abstract": "The recent surge of Multimodal Large Language Models (MLLMs) has fundamentally reshaped the landscape of AI research and industry, shedding light on a promising path toward the next AI milestone. However, significant challenges remain preventing MLLMs from being practical in real-world applications. The most notable challenge comes from the huge cost of running an MLLM with a massive number of parameters and extensive computation. As a result, most MLLMs need to be deployed on high-performing cloud servers, which greatly limits their application scopes such as mobile, offline, energy-sensitive, and privacy-protective scenarios. In this work, we present MiniCPM-V, a series of efficient MLLMs deployable on end-side devices. By integrating the latest MLLM techniques in architecture, pretraining and alignment, the latest MiniCPM-Llama3-V 2.5 has several notable features: (1) Strong performance, outperforming GPT-4V-1106, Gemini Pro and Claude 3 on OpenCompass, a comprehensive evaluation over 11 popular benchmarks, (2) strong OCR capability and 1.8M pixel high-resolution image perception at any aspect ratio, (3) trustworthy behavior with low hallucination rates, (4) multilingual support for 30+ languages, and (5) efficient deployment on mobile phones. More importantly, MiniCPM-V can be viewed as a representative example of a promising trend: The model sizes for achieving usable (e.g., GPT-4V) level performance are rapidly decreasing, along with the fast growth of end-side computation capacity. This jointly shows that GPT-4V level MLLMs deployed on end devices are becoming increasingly possible, unlocking a wider spectrum of real-world AI applications in the near future.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "citation_count": 174,
      "reference_count": 0,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work presents MiniCPM-V, a series of efficient MLLMs deployable on end-side devices, and can be viewed as a representative example of a promising trend: the model sizes for achieving usable GPT-4V level performance are rapidly decreasing, along with the fast growth of end-side computation capacity.",
      "external_id_arxiv": "2408.01800",
      "external_id_dblp": "journals/corr/abs-2408-01800",
      "external_id_doi": "10.48550/arXiv.2408.01800",
      "external_id_corpusid": 271709626
    },
    {
      "id": "2302149588",
      "type": "author",
      "name": "Yuan Yao"
    },
    {
      "id": "2117902355",
      "type": "author",
      "name": "Tianyu Yu"
    },
    {
      "id": "2315247489",
      "type": "author",
      "name": "Ao Zhang"
    },
    {
      "id": "2249899670",
      "type": "author",
      "name": "Chongyi Wang"
    },
    {
      "id": "2292213035",
      "type": "author",
      "name": "Junbo Cui"
    },
    {
      "id": "2314888390",
      "type": "author",
      "name": "Hongji Zhu"
    },
    {
      "id": "2314831395",
      "type": "author",
      "name": "Tianchi Cai"
    },
    {
      "id": "2314861602",
      "type": "author",
      "name": "Haoyu Li"
    },
    {
      "id": "2150606888",
      "type": "author",
      "name": "Weilin Zhao"
    },
    {
      "id": "2315388099",
      "type": "author",
      "name": "Zhihui He"
    },
    {
      "id": "2157954216",
      "type": "author",
      "name": "Qi-An Chen"
    },
    {
      "id": "2315068277",
      "type": "author",
      "name": "Huarong Zhou"
    },
    {
      "id": "2314827568",
      "type": "author",
      "name": "Zhensheng Zou"
    },
    {
      "id": "2233320353",
      "type": "author",
      "name": "Haoye Zhang"
    },
    {
      "id": "2295929465",
      "type": "author",
      "name": "Zhi Zheng"
    },
    {
      "id": "2295789325",
      "type": "author",
      "name": "Jie Zhou"
    },
    {
      "id": "2295809950",
      "type": "author",
      "name": "Jie Cai"
    },
    {
      "id": "1398454307",
      "type": "author",
      "name": "Guoyang Zeng"
    },
    {
      "id": "2144118403",
      "type": "author",
      "name": "Dahai Li"
    },
    {
      "id": "2301534001",
      "type": "author",
      "name": "Zhiyuan Liu"
    },
    {
      "id": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "paper",
      "title": "ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation",
      "abstract": "We introduce a new benchmark, ChartMimic, aimed at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering. ChartMimic includes 1,000 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains(e.g., Physics, Computer Science, Economics, etc). These charts span 18 regular types and 4 advanced types, diversifying into 191 subcategories. Furthermore, we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts. Unlike existing code generation benchmarks, ChartMimic places emphasis on evaluating LMMs' capacity to harmonize a blend of cognitive capabilities, encompassing visual understanding, code generation, and cross-modal reasoning. The evaluation of 3 proprietary models and 11 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-4V, Claude-3-opus only achieve an average score of 73.2 and 53.7, respectively, indicating significant room for improvement. We anticipate that ChartMimic will inspire the development of LMMs, advancing the pursuit of artificial general intelligence.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/250043fdae97d2ac87245a44923682e7a3decd50",
      "citation_count": 14,
      "reference_count": 51,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work introduces a new benchmark, ChartMimic, aimed at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs), and places emphasis on LMMs' capacity to harmonize a blend of cognitive capabilities, encompassing visual understanding, code generation, and cross-modal reasoning.",
      "external_id_dblp": "journals/corr/abs-2406-09961",
      "external_id_arxiv": "2406.09961",
      "external_id_doi": "10.48550/arXiv.2406.09961",
      "external_id_corpusid": 270521907
    },
    {
      "id": "2261925600",
      "type": "author",
      "name": "Chufan Shi"
    },
    {
      "id": "2284580714",
      "type": "author",
      "name": "Cheng Yang"
    },
    {
      "id": "2306804471",
      "type": "author",
      "name": "Yaxin Liu"
    },
    {
      "id": "2220002319",
      "type": "author",
      "name": "Bo Shui"
    },
    {
      "id": "2143183255",
      "type": "author",
      "name": "Junjie Wang"
    },
    {
      "id": "2306784963",
      "type": "author",
      "name": "Mohan Jing"
    },
    {
      "id": "2307262806",
      "type": "author",
      "name": "Linran Xu"
    },
    {
      "id": "2116314158",
      "type": "author",
      "name": "Xinyu Zhu"
    },
    {
      "id": "47319720",
      "type": "author",
      "name": "Siheng Li"
    },
    {
      "id": "2108080174",
      "type": "author",
      "name": "Yuxiang Zhang"
    },
    {
      "id": "2269171464",
      "type": "author",
      "name": "Gongye Liu"
    },
    {
      "id": "2306790051",
      "type": "author",
      "name": "Xiaomei Nie"
    },
    {
      "id": "2283846474",
      "type": "author",
      "name": "Deng Cai"
    },
    {
      "id": "2283881403",
      "type": "author",
      "name": "Yujiu Yang"
    },
    {
      "id": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "paper",
      "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
      "abstract": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "citation_count": 666,
      "reference_count": 64,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "",
      "external_id_dblp": "journals/corr/abs-2404-14219",
      "external_id_arxiv": "2404.14219",
      "external_id_doi": "10.48550/arXiv.2404.14219",
      "external_id_corpusid": 269293048
    },
    {
      "id": "2114891202",
      "type": "author",
      "name": "Marah Abdin"
    },
    {
      "id": "2297768912",
      "type": "author",
      "name": "Sam Ade Jacobs"
    },
    {
      "id": "2942686",
      "type": "author",
      "name": "A. A. Awan"
    },
    {
      "id": "29956361",
      "type": "author",
      "name": "J. Aneja"
    },
    {
      "id": "113916198",
      "type": "author",
      "name": "Ahmed Awadallah"
    },
    {
      "id": "3032929",
      "type": "author",
      "name": "H. Awadalla"
    },
    {
      "id": "2297768888",
      "type": "author",
      "name": "Nguyen Bach"
    },
    {
      "id": "2297768420",
      "type": "author",
      "name": "Amit Bahree"
    },
    {
      "id": "2274106850",
      "type": "author",
      "name": "Arash Bakhtiari"
    },
    {
      "id": "145560551",
      "type": "author",
      "name": "Harkirat Singh Behl"
    },
    {
      "id": "102222453",
      "type": "author",
      "name": "Alon Benhaim"
    },
    {
      "id": "2297766346",
      "type": "author",
      "name": "Misha Bilenko"
    },
    {
      "id": "46278353",
      "type": "author",
      "name": "Johan Bjorck"
    },
    {
      "id": "121645690",
      "type": "author",
      "name": "Sébastien Bubeck"
    },
    {
      "id": "2297768425",
      "type": "author",
      "name": "Martin Cai"
    },
    {
      "id": "2157424631",
      "type": "author",
      "name": "C. C. T. Mendes"
    },
    {
      "id": "2264439430",
      "type": "author",
      "name": "Weizhu Chen"
    },
    {
      "id": "113810201",
      "type": "author",
      "name": "Vishrav Chaudhary"
    },
    {
      "id": "2297780966",
      "type": "author",
      "name": "Parul Chopra"
    },
    {
      "id": "50672277",
      "type": "author",
      "name": "Allison Del Giorno"
    },
    {
      "id": "2297768528",
      "type": "author",
      "name": "Gustavo de Rosa"
    },
    {
      "id": "2297768894",
      "type": "author",
      "name": "Matthew Dixon"
    },
    {
      "id": "2315830",
      "type": "author",
      "name": "Ronen Eldan"
    },
    {
      "id": "3310951",
      "type": "author",
      "name": "Dan Iter"
    },
    {
      "id": "2054713176",
      "type": "author",
      "name": "Abhishek Goswami"
    },
    {
      "id": "2281352409",
      "type": "author",
      "name": "S. Gunasekar"
    },
    {
      "id": "2297768377",
      "type": "author",
      "name": "Emman Haider"
    },
    {
      "id": "2266241191",
      "type": "author",
      "name": "Junheng Hao"
    },
    {
      "id": "2945519",
      "type": "author",
      "name": "Russell J. Hewett"
    },
    {
      "id": "2297778093",
      "type": "author",
      "name": "Jamie Huynh"
    },
    {
      "id": "51900416",
      "type": "author",
      "name": "Mojan Javaheripi"
    },
    {
      "id": "2268726222",
      "type": "author",
      "name": "Xin Jin"
    },
    {
      "id": "2160340819",
      "type": "author",
      "name": "Piero Kauffmann"
    },
    {
      "id": "1830939",
      "type": "author",
      "name": "Nikos Karampatziakis"
    },
    {
      "id": "2297803872",
      "type": "author",
      "name": "Dongwoo Kim"
    },
    {
      "id": "2152658577",
      "type": "author",
      "name": "Young Jin Kim"
    },
    {
      "id": "2297767306",
      "type": "author",
      "name": "Mahoud Khademi"
    },
    {
      "id": "2279749803",
      "type": "author",
      "name": "Lev Kurilenko"
    },
    {
      "id": "2297805471",
      "type": "author",
      "name": "James R. Lee"
    },
    {
      "id": "2239163839",
      "type": "author",
      "name": "Yin Tat Lee"
    },
    {
      "id": "2268435346",
      "type": "author",
      "name": "Yuanzhi Li"
    },
    {
      "id": "2269852520",
      "type": "author",
      "name": "Chen Liang"
    },
    {
      "id": "2268632175",
      "type": "author",
      "name": "Weishung Liu"
    },
    {
      "id": "2297769037",
      "type": "author",
      "name": "Eric Lin"
    },
    {
      "id": "2297828605",
      "type": "author",
      "name": "Zeqi Lin"
    },
    {
      "id": "46781068",
      "type": "author",
      "name": "Piyush Madan"
    },
    {
      "id": "2256988075",
      "type": "author",
      "name": "Arindam Mitra"
    },
    {
      "id": "2297767663",
      "type": "author",
      "name": "Hardik Modi"
    },
    {
      "id": "2274742737",
      "type": "author",
      "name": "Anh Nguyen"
    },
    {
      "id": "2172095",
      "type": "author",
      "name": "Brandon Norick"
    },
    {
      "id": "27419446",
      "type": "author",
      "name": "Barun Patra"
    },
    {
      "id": "1416388980",
      "type": "author",
      "name": "D. Perez-Becker"
    },
    {
      "id": "6625302",
      "type": "author",
      "name": "Thomas Portet"
    },
    {
      "id": "4099006",
      "type": "author",
      "name": "Reid Pryzant"
    },
    {
      "id": "2279740366",
      "type": "author",
      "name": "Heyang Qin"
    },
    {
      "id": "2297777494",
      "type": "author",
      "name": "Marko Radmilac"
    },
    {
      "id": "46177458",
      "type": "author",
      "name": "Liliang Ren"
    },
    {
      "id": "41016119",
      "type": "author",
      "name": "Corby Rosset"
    },
    {
      "id": "2298400461",
      "type": "author",
      "name": "Sambudha Roy"
    },
    {
      "id": "2347792",
      "type": "author",
      "name": "Olli Saarikivi"
    },
    {
      "id": "2282542305",
      "type": "author",
      "name": "Amin Saied"
    },
    {
      "id": "2297768516",
      "type": "author",
      "name": "Adil Salim"
    },
    {
      "id": "1413038175",
      "type": "author",
      "name": "Michael Santacroce"
    },
    {
      "id": "2297814416",
      "type": "author",
      "name": "Shital Shah"
    },
    {
      "id": "2284868563",
      "type": "author",
      "name": "Ning Shang"
    },
    {
      "id": "2266307341",
      "type": "author",
      "name": "Hiteshi Sharma"
    },
    {
      "id": "2291873212",
      "type": "author",
      "name": "Xianmin Song"
    },
    {
      "id": "2537545",
      "type": "author",
      "name": "Olatunji Ruwase"
    },
    {
      "id": "2127734657",
      "type": "author",
      "name": "Praneetha Vaddamanu"
    },
    {
      "id": "2153689937",
      "type": "author",
      "name": "Xin Wang"
    },
    {
      "id": "2274157852",
      "type": "author",
      "name": "Rachel Ward"
    },
    {
      "id": "2298020153",
      "type": "author",
      "name": "Guanhua Wang"
    },
    {
      "id": "2297768553",
      "type": "author",
      "name": "Philipp Witte"
    },
    {
      "id": "2226773110",
      "type": "author",
      "name": "Michael Wyatt"
    },
    {
      "id": "46747953",
      "type": "author",
      "name": "Can Xu"
    },
    {
      "id": "2257094139",
      "type": "author",
      "name": "Jiahang Xu"
    },
    {
      "id": "2297814868",
      "type": "author",
      "name": "Sonali Yadav"
    },
    {
      "id": "145338263",
      "type": "author",
      "name": "Fan Yang"
    },
    {
      "id": "2291073936",
      "type": "author",
      "name": "Ziyi Yang"
    },
    {
      "id": "2287794511",
      "type": "author",
      "name": "Donghan Yu"
    },
    {
      "id": "2287118838",
      "type": "author",
      "name": "Cheng-Yuan Zhang"
    },
    {
      "id": "2297810599",
      "type": "author",
      "name": "Cyril Zhang"
    },
    {
      "id": "2297818019",
      "type": "author",
      "name": "Jianwen Zhang"
    },
    {
      "id": "2274195530",
      "type": "author",
      "name": "L. Zhang"
    },
    {
      "id": "2271712604",
      "type": "author",
      "name": "Yi Zhang"
    },
    {
      "id": "2298130748",
      "type": "author",
      "name": "Yunan Zhang"
    },
    {
      "id": "2297801063",
      "type": "author",
      "name": "Xiren Zhou"
    },
    {
      "id": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "paper",
      "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code",
      "abstract": "Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and May 2024. We have evaluated 18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "citation_count": 137,
      "reference_count": 90,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work proposes LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces.",
      "external_id_arxiv": "2403.07974",
      "external_id_dblp": "journals/corr/abs-2403-07974",
      "external_id_doi": "10.48550/arXiv.2403.07974",
      "external_id_corpusid": 268379413
    },
    {
      "id": "1646458461",
      "type": "author",
      "name": "Naman Jain"
    },
    {
      "id": "2291438028",
      "type": "author",
      "name": "King Han"
    },
    {
      "id": "2261364306",
      "type": "author",
      "name": "Alex Gu"
    },
    {
      "id": "2108635077",
      "type": "author",
      "name": "Wen-Ding Li"
    },
    {
      "id": "2291072588",
      "type": "author",
      "name": "Fanjia Yan"
    },
    {
      "id": "2278894217",
      "type": "author",
      "name": "Sida I. Wang"
    },
    {
      "id": "2278433392",
      "type": "author",
      "name": "Armando Solar-Lezama"
    },
    {
      "id": "2268398121",
      "type": "author",
      "name": "Koushik Sen"
    },
    {
      "id": "2290576964",
      "type": "author",
      "name": "Ion Stoica"
    },
    {
      "id": "0c33df9fc20c9314a0883611e9912a94b511569c",
      "type": "paper",
      "title": "UniMoD: Efficient Unified Multimodal Transformers with Mixture-of-Depths",
      "abstract": "Unified multimodal transformers, which handle both generation and understanding tasks within a shared parameter space, have received increasing attention in recent research. Although various unified transformers have been proposed, training these models is costly due to redundant tokens and heavy attention computation. In the past, studies on large language models have demonstrated that token pruning methods, such as Mixture of Depths (MoD), can significantly improve computational efficiency. MoD employs a router to select the most important ones for processing within a transformer layer. However, directly applying MoD-based token pruning to unified transformers will result in suboptimal performance because different tasks exhibit varying levels of token redundancy. In our work, we analyze the unified transformers by (1) examining attention weight patterns, (2) evaluating the layer importance and token redundancy, and (3) analyzing task interactions. Our findings reveal that token redundancy is primarily influenced by different tasks and layers. Building on these findings, we introduce UniMoD, a task-aware token pruning method that employs a separate router for each task to determine which tokens should be pruned. We apply our method to Show-o and Emu3, reducing training FLOPs by approximately 15% in Show-o and 40% in Emu3, while maintaining or improving performance on several benchmarks. Code will be released at https://github.com/showlab/UniMoD.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/0c33df9fc20c9314a0883611e9912a94b511569c",
      "citation_count": 0,
      "reference_count": 127,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "UniMoD is introduced, a task-aware token pruning method that employs a separate router for each task to determine which tokens should be pruned, and reveals that token redundancy is primarily influenced by different tasks and layers.",
      "external_id_arxiv": "2502.06474",
      "external_id_corpusid": 276250228
    },
    {
      "id": "2258958963",
      "type": "author",
      "name": "Weijia Mao"
    },
    {
      "id": "2346316013",
      "type": "author",
      "name": "Zhenheng Yang"
    },
    {
      "id": "2344762475",
      "type": "author",
      "name": "Mike Zheng Shou"
    },
    {
      "id": "a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "type": "paper",
      "title": "Mixture of Hidden-Dimensions Transformer",
      "abstract": "Transformer models encounter challenges in scaling hidden dimensions efficiently, as uniformly increasing them inflates computational and memory costs while failing to emphasize the most relevant features for each token. For further understanding, we study hidden dimension sparsity and observe that trained Transformers utilize only a small fraction of token dimensions, revealing an\"activation flow\"pattern. Notably, there are shared sub-dimensions with sustained activation across multiple consecutive tokens and specialized sub-dimensions uniquely activated for each token. To better model token-relevant sub-dimensions, we propose MoHD (Mixture of Hidden Dimensions), a sparse conditional activation architecture. Particularly, MoHD employs shared sub-dimensions for common token features and a routing mechanism to dynamically activate specialized sub-dimensions. To mitigate potential information loss from sparsity, we design activation scaling and group fusion mechanisms to preserve activation flow. In this way, MoHD expands hidden dimensions with negligible increases in computation or parameters, efficient training and inference while maintaining performance. Evaluations across 10 NLP tasks show that MoHD surpasses Vanilla Transformers in parameter efficiency and task performance. It achieves 1.7% higher performance with 50% fewer activation parameters and 3.7% higher performance with a 3x parameter expansion at constant activation cost. MOHD offers a new perspective for scaling the model, showcasing the potential of hidden dimension sparsity to boost efficiency",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "citation_count": 0,
      "reference_count": 45,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "MoHD (Mixture of Hidden Dimensions), a sparse conditional activation architecture, is proposed, a sparse conditional activation architecture that expands hidden dimensions with negligible increases in computation or parameters, efficient training and inference while maintaining performance.",
      "external_id_dblp": "journals/corr/abs-2412-05644",
      "external_id_arxiv": "2412.05644",
      "external_id_doi": "10.48550/arXiv.2412.05644",
      "external_id_corpusid": 274598165
    },
    {
      "id": "2268796753",
      "type": "author",
      "name": "Yilong Chen"
    },
    {
      "id": "2257305563",
      "type": "author",
      "name": "Junyuan Shang"
    },
    {
      "id": "2335037545",
      "type": "author",
      "name": "Zhengyu Zhang"
    },
    {
      "id": "2054250919",
      "type": "author",
      "name": "Jiawei Sheng"
    },
    {
      "id": "2268772040",
      "type": "author",
      "name": "Tingwen Liu"
    },
    {
      "id": "104463827",
      "type": "author",
      "name": "Shuohuan Wang"
    },
    {
      "id": "2296100360",
      "type": "author",
      "name": "Yu Sun"
    },
    {
      "id": "2190177674",
      "type": "author",
      "name": "Hua Wu"
    },
    {
      "id": "2238917361",
      "type": "author",
      "name": "Haifeng Wang"
    },
    {
      "id": "1112c0b273c9466906ba6d51fc90881538d33769",
      "type": "paper",
      "title": "Designing Large Foundation Models for Efficient Training and Inference: A Survey",
      "abstract": "This paper focuses on modern efficient training and inference technologies on foundation models and illustrates them from two perspectives: model and system design. Model and System Design optimize LLM training and inference from different aspects to save computational resources, making LLMs more efficient, affordable, and more accessible. The paper list repository is available at https://github.com/NoakLiu/Efficient-Foundation-Models-Survey.",
      "year": 2024,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/1112c0b273c9466906ba6d51fc90881538d33769",
      "citation_count": 0,
      "reference_count": 186,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "",
      "external_id_arxiv": "2409.01990",
      "external_id_corpusid": 272366594
    },
    {
      "id": "2338973383",
      "type": "author",
      "name": "Dong Liu"
    },
    {
      "id": "2334859470",
      "type": "author",
      "name": "Yanxuan Yu"
    },
    {
      "id": "2334773796",
      "type": "author",
      "name": "Yite Wang"
    },
    {
      "id": "2335117100",
      "type": "author",
      "name": "Jing Wu"
    },
    {
      "id": "2334746553",
      "type": "author",
      "name": "Zhongwei Wan"
    },
    {
      "id": "2339774429",
      "type": "author",
      "name": "Sina Alinejad"
    },
    {
      "id": "2334743866",
      "type": "author",
      "name": "Benjamin Lengerich"
    },
    {
      "id": "2334970043",
      "type": "author",
      "name": "Ying Nian Wu"
    },
    {
      "id": "668f545fa8133b629ee3463c52c10b02190a2496",
      "type": "paper",
      "title": "Delta Decompression for MoE-based LLMs Compression",
      "abstract": "Mixture-of-Experts (MoE) architectures in large language models (LLMs) achieve exceptional performance, but face prohibitive storage and memory requirements. To address these challenges, we present $D^2$-MoE, a new delta decompression compressor for reducing the parameters of MoE LLMs. Based on observations of expert diversity, we decompose their weights into a shared base weight and unique delta weights. Specifically, our method first merges each expert's weight into the base weight using the Fisher information matrix to capture shared components. Then, we compress delta weights through Singular Value Decomposition (SVD) by exploiting their low-rank properties. Finally, we introduce a semi-dynamical structured pruning strategy for the base weights, combining static and dynamic redundancy analysis to achieve further parameter reduction while maintaining input adaptivity. In this way, our $D^2$-MoE successfully compact MoE LLMs to high compression ratios without additional training. Extensive experiments highlight the superiority of our approach, with over 13% performance gains than other compressors on Mixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\\sim$60% compression rates. Codes are available in https://github.com/lliai/D2MoE.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/668f545fa8133b629ee3463c52c10b02190a2496",
      "citation_count": 0,
      "reference_count": 34,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A new delta decompression compressor for reducing the parameters of MoE LLMs that first merges each expert's weight into the base weight using the Fisher information matrix to capture shared components, and introduces a semi-dynamical structured pruning strategy for the base weights.",
      "external_id_arxiv": "2502.17298",
      "external_id_corpusid": 276575054
    },
    {
      "id": "5d93440621bd49953be4aab0474b0d26dcae504e",
      "type": "paper",
      "title": "Mixup Regularization: A Probabilistic Perspective",
      "abstract": "In recent years, mixup regularization has gained popularity as an effective way to improve the generalization performance of deep learning models by training on convex combinations of training data. While many mixup variants have been explored, the proper adoption of the technique to conditional density estimation and probabilistic machine learning remains relatively unexplored. This work introduces a novel framework for mixup regularization based on probabilistic fusion that is better suited for conditional density estimation tasks. For data distributed according to a member of the exponential family, we show that likelihood functions can be analytically fused using log-linear pooling. We further propose an extension of probabilistic mixup, which allows for fusion of inputs at an arbitrary intermediate layer of the neural network. We provide a theoretical analysis comparing our approach to standard mixup variants. Empirical results on synthetic and real datasets demonstrate the benefits of our proposed framework compared to existing mixup variants.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/5d93440621bd49953be4aab0474b0d26dcae504e",
      "citation_count": 0,
      "reference_count": 37,
      "fields_of_study": "[\"Computer Science\", \"Mathematics\"]",
      "is_open_access": false,
      "tldr": "A novel framework for mixup regularization based on probabilistic fusion that is better suited for conditional density estimation tasks and an extension of probabilistic mixup, which allows for fusion of inputs at an arbitrary intermediate layer of the neural network are introduced.",
      "external_id_arxiv": "2502.13825",
      "external_id_corpusid": 276450113
    },
    {
      "id": "2346113879",
      "type": "author",
      "name": "Yousef El-Laham"
    },
    {
      "id": "2207655",
      "type": "author",
      "name": "Niccolò Dalmasso"
    },
    {
      "id": "2264978144",
      "type": "author",
      "name": "Svitlana Vyetrenko"
    },
    {
      "id": "2330666",
      "type": "author",
      "name": "Vamsi K. Potluru"
    },
    {
      "id": "2346113190",
      "type": "author",
      "name": "Manuela Veloso"
    },
    {
      "id": "9e6b56fa4f6ea626c6748b6386214b279dd8f76f",
      "type": "paper",
      "title": "A Survey of Personalized Large Language Models: Progress and Future Directions",
      "abstract": "Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research. Updated information about this survey can be found at the https://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/9e6b56fa4f6ea626c6748b6386214b279dd8f76f",
      "citation_count": 0,
      "reference_count": 97,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level).",
      "external_id_arxiv": "2502.11528",
      "external_id_corpusid": 276408512
    },
    {
      "id": "2144131350",
      "type": "author",
      "name": "Jiahong Liu"
    },
    {
      "id": "2346820279",
      "type": "author",
      "name": "Zhongyang Li"
    },
    {
      "id": "2345819983",
      "type": "author",
      "name": "Quanyu Dai"
    },
    {
      "id": "2240695630",
      "type": "author",
      "name": "Jieming Zhu"
    },
    {
      "id": "2249778653",
      "type": "author",
      "name": "Minda Hu"
    },
    {
      "id": "2301207729",
      "type": "author",
      "name": "Menglin Yang"
    },
    {
      "id": "2309174208",
      "type": "author",
      "name": "Irwin King"
    },
    {
      "id": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "paper",
      "title": "Safety Evaluation of DeepSeek Models in Chinese Contexts",
      "abstract": "Recently, the DeepSeek series of models, leveraging their exceptional reasoning capabilities and open-source strategy, is reshaping the global AI landscape. Despite these advantages, they exhibit significant safety deficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 has a 100\\% attack success rate when processing harmful prompts. Additionally, multiple safety companies and research institutions have confirmed critical safety vulnerabilities in this model. As models demonstrating robust performance in Chinese and English, DeepSeek models require equally crucial safety assessments in both language contexts. However, current research has predominantly focused on safety evaluations in English environments, leaving a gap in comprehensive assessments of their safety performance in Chinese contexts. In response to this gap, this study introduces CHiSafetyBench, a Chinese-specific safety evaluation benchmark. This benchmark systematically evaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts, revealing their performance across safety categories. The experimental results quantify the deficiencies of these two models in Chinese contexts, providing key insights for subsequent improvements. It should be noted that, despite our efforts to establish a comprehensive, objective, and authoritative evaluation benchmark, the selection of test samples, characteristics of data distribution, and the setting of evaluation criteria may inevitably introduce certain biases into the evaluation results. We will continuously optimize the evaluation benchmark and periodically update this report to provide more comprehensive and accurate assessment outcomes. Please refer to the latest version of the paper for the most recent evaluation results and conclusions.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/349354e5886d566536bb30c11cf9c74bed458594",
      "citation_count": 0,
      "reference_count": 15,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This study introduces CHiSafetyBench, a Chinese-specific safety evaluation benchmark that systematically evaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts, revealing their performance across safety categories.",
      "external_id_arxiv": "2502.11137",
      "external_id_corpusid": 276408128
    },
    {
      "id": "2298863422",
      "type": "author",
      "name": "Wenjing Zhang"
    },
    {
      "id": "2306966575",
      "type": "author",
      "name": "Xuejiao Lei"
    },
    {
      "id": "2292657641",
      "type": "author",
      "name": "Zhaoxiang Liu"
    },
    {
      "id": "2308853745",
      "type": "author",
      "name": "Ning Wang"
    },
    {
      "id": "2345816011",
      "type": "author",
      "name": "Zhenhong Long"
    },
    {
      "id": "2345984737",
      "type": "author",
      "name": "Peijun Yang"
    },
    {
      "id": "2345874673",
      "type": "author",
      "name": "Jiaojiao Zhao"
    },
    {
      "id": "31698884",
      "type": "author",
      "name": "Minjie Hua"
    },
    {
      "id": "2345823846",
      "type": "author",
      "name": "Chaoyang Ma"
    },
    {
      "id": "2307181856",
      "type": "author",
      "name": "Kai Wang"
    },
    {
      "id": "2292613998",
      "type": "author",
      "name": "Shiguo Lian"
    },
    {
      "id": "98fda72e51ffecf914da5fe5c8f3d64d4078e144",
      "type": "paper",
      "title": "MoETuner: Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing",
      "abstract": "Mixture-of-Experts (MoE) model architecture has emerged as a promising solution for scaling transformer models efficiently, offering sparse activation that reduces computational costs while increasing model capacity. However, as MoE models scale, they need to be distributed across GPU devices, thus face critical performance bottlenecks due to their large memory footprint. Expert parallelism distributes experts across GPUs, however, faces key challenges including an unbalanced token routing and expert activation, resulting in communication tail latency and processing inefficiencies. While existing solutions address some of these issues, they fail to resolve the dual challenges of load imbalance and communication skew. The imbalance in token processing load across experts causes uneven processing times on different GPUs, while communication skew between GPUs leads to unbalanced inter-GPU data transfers. These factors degrade the performance of MoE models by increasing tail latency and reducing overall throughput. To address these limitations, we propose an Integer Linear Programming (ILP) formulation to optimize expert placement by jointly considering token load, communication, and computation costs. We exploit the property that there is a token routing dependency across layers, where tokens routed to a specific expert in one layer are likely to be routed to a limited set of experts in the subsequent layer. Our solution, MoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU token routing costs and balances token processing across devices, thereby reducing tail latency and end-to-end execution time. Experimental results demonstrate 9.3% and 17.5% of end-to-end speedups for single-node and multi-node inference respectively, showcasing the potential of our ILP-based optimization for offering expert parallel solutions for next-generation MoEs.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/98fda72e51ffecf914da5fe5c8f3d64d4078e144",
      "citation_count": 0,
      "reference_count": 39,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The solution, MoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU token routing costs and balances token processing across devices, thereby reducing tail latency and end-to-end execution time.",
      "external_id_arxiv": "2502.06643",
      "external_id_corpusid": 276250538
    },
    {
      "id": "2344765112",
      "type": "author",
      "name": "Seokjin Go"
    },
    {
      "id": "2344765052",
      "type": "author",
      "name": "Divya Mahajan"
    },
    {
      "id": "ae57f38c1c62e2b0a143c0c862f94bb4c11f4fab",
      "type": "paper",
      "title": "Hierarchical Time-Aware Mixture of Experts for Multi-Modal Sequential Recommendation",
      "abstract": "Multi-modal sequential recommendation (SR) leverages multi-modal data to learn more comprehensive item features and user preferences than traditional SR methods, which has become a critical topic in both academia and industry. Existing methods typically focus on enhancing multi-modal information utility through adaptive modality fusion to capture the evolving of user preference from user-item interaction sequences. However, most of them overlook the interference caused by redundant interest-irrelevant information contained in rich multi-modal data. Additionally, they primarily rely on implicit temporal information based solely on chronological ordering, neglecting explicit temporal signals that could more effectively represent dynamic user interest over time. To address these limitations, we propose a Hierarchical time-aware Mixture of experts for multi-modal Sequential Recommendation (HM4SR) with a two-level Mixture of Experts (MoE) and a multi-task learning strategy. Specifically, the first MoE, named Interactive MoE, extracts essential user interest-related information from the multi-modal data of each item. Then, the second MoE, termed Temporal MoE, captures user dynamic interests by introducing explicit temporal embeddings from timestamps in modality encoding. To further address data sparsity, we propose three auxiliary supervision tasks: sequence-level category prediction (CP) for item feature understanding, contrastive learning on ID (IDCL) to align sequence context with user interests, and placeholder contrastive learning (PCL) to integrate temporal information with modalities for dynamic interest modeling. Extensive experiments on four public datasets verify the effectiveness of HM4SR compared to several state-of-the-art approaches.",
      "year": 2025,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/ae57f38c1c62e2b0a143c0c862f94bb4c11f4fab",
      "citation_count": 0,
      "reference_count": 62,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A Hierarchical time-aware Mixture of experts for multi-modal Sequential Recommendation (HM4SR) with a two-level Mixture of Experts (MoE) and a multi-task learning strategy to address data sparsity.",
      "external_id_dblp": "journals/corr/abs-2501-14269",
      "external_id_arxiv": "2501.14269",
      "external_id_doi": "10.48550/arXiv.2501.14269",
      "external_id_corpusid": 275906820
    },
    {
      "id": "2342574394",
      "type": "author",
      "name": "Shengzhe Zhang"
    },
    {
      "id": "2293559850",
      "type": "author",
      "name": "Liyi Chen"
    },
    {
      "id": "2065251473",
      "type": "author",
      "name": "Dazhong Shen"
    },
    {
      "id": "2324627531",
      "type": "author",
      "name": "Chao Wang"
    },
    {
      "id": "2266789166",
      "type": "author",
      "name": "Hui Xiong"
    },
    {
      "id": "8970a42d5de27dcf0833f3f1387de586b6d85a3e",
      "type": "paper",
      "title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models",
      "abstract": "Mixture of experts (MoE) has become the standard for constructing production-level large language models (LLMs) due to its promise to boost model capacity without causing significant overheads. Nevertheless, existing MoE methods usually enforce a constant top-k routing for all tokens, which is arguably restrictive because various tokens (e.g.,\"\"vs.\"apple\") may require various numbers of experts for feature abstraction. Lifting such a constraint can help make the most of limited resources and unleash the potential of the model for downstream tasks. In this sense, we introduce AdaMoE to realize token-adaptive routing for MoE, where different tokens are permitted to select a various number of experts. AdaMoE makes minimal modifications to the vanilla MoE with top-k routing -- it simply introduces a fixed number of null experts, which do not consume any FLOPs, to the expert set and increases the value of k. AdaMoE does not force each token to occupy a fixed number of null experts but ensures the average usage of the null experts with a load-balancing loss, leading to an adaptive number of null/true experts used by each token. AdaMoE exhibits a strong resemblance to MoEs with expert choice routing while allowing for trivial auto-regressive modeling. AdaMoE is easy to implement and can be effectively applied to pre-trained (MoE-)LLMs. Extensive studies show that AdaMoE can reduce average expert load (FLOPs) while achieving superior performance. For example, on the ARC-C dataset, applying our method to fine-tuning Mixtral-8x7B can reduce FLOPs by 14.5% while increasing accuracy by 1.69%.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "url": "https://www.semanticscholar.org/paper/8970a42d5de27dcf0833f3f1387de586b6d85a3e",
      "citation_count": 5,
      "reference_count": 36,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "AdaMoE is introduced to realize token-adaptive routing for MoE, where different tokens are permitted to select a various number of experts, and exhibits a strong resemblance to MoEs with expert choice routing while allowing for trivial auto-regressive modeling.",
      "external_id_dblp": "journals/corr/abs-2406-13233",
      "external_id_arxiv": "2406.13233",
      "external_id_doi": "10.48550/arXiv.2406.13233",
      "external_id_corpusid": 270619842
    },
    {
      "id": "2309672667",
      "type": "author",
      "name": "Zihao Zeng"
    },
    {
      "id": "2188993538",
      "type": "author",
      "name": "Yibo Miao"
    },
    {
      "id": "2162081759",
      "type": "author",
      "name": "Hongcheng Gao"
    },
    {
      "id": "2289837431",
      "type": "author",
      "name": "Hao Zhang"
    },
    {
      "id": "2295008708",
      "type": "author",
      "name": "Zhijie Deng"
    },
    {
      "id": "a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8",
      "type": "paper",
      "title": "Flextron: Many-in-One Flexible Large Language Model",
      "abstract": "Training modern LLMs is extremely resource intensive, and customizing them for various deployment scenarios characterized by limited compute and memory resources through repeated training is impractical. In this paper, we introduce Flextron, a network architecture and post-training model optimization framework supporting flexible model deployment. The Flextron architecture utilizes a nested elastic structure to rapidly adapt to specific user-defined latency and accuracy targets during inference with no additional fine-tuning required. It is also input-adaptive, and can automatically route tokens through its sub-networks for improved performance and efficiency. We present a sample-efficient training method and associated routing algorithms for systematically transforming an existing trained LLM into a Flextron model. We evaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate superior performance over multiple end-to-end trained variants and other state-of-the-art elastic networks, all with a single pretraining run that consumes a mere 7.63% tokens compared to original pretraining.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "url": "https://www.semanticscholar.org/paper/a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8",
      "citation_count": 9,
      "reference_count": 51,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper presents a sample-efficient training method and associated routing algorithms for systematically transforming an existing trained LLM into a Flextron model, and evaluates Flextron on the GPT-3 and LLama-2 family of LLMs.",
      "external_id_dblp": "journals/corr/abs-2406-10260",
      "external_id_arxiv": "2406.10260",
      "external_id_doi": "10.48550/arXiv.2406.10260",
      "external_id_corpusid": 270560556
    },
    {
      "id": "2209882676",
      "type": "author",
      "name": "Ruisi Cai"
    },
    {
      "id": "31225166",
      "type": "author",
      "name": "Saurav Muralidharan"
    },
    {
      "id": "2273650910",
      "type": "author",
      "name": "Greg Heinrich"
    },
    {
      "id": "1989015",
      "type": "author",
      "name": "Hongxu Yin"
    },
    {
      "id": "2307032205",
      "type": "author",
      "name": "Zhangyang Wang"
    },
    {
      "id": "2273651410",
      "type": "author",
      "name": "Jan Kautz"
    },
    {
      "id": "2824500",
      "type": "author",
      "name": "Pavlo Molchanov"
    },
    {
      "id": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "paper",
      "title": "Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models",
      "abstract": "In this technical report, we introduce the training methodologies implemented in the development of Skywork-MoE, a high-performance mixture-of-experts (MoE) large language model (LLM) with 146 billion parameters and 16 experts. It is initialized from the pre-existing dense checkpoints of our Skywork-13B model. We explore the comparative effectiveness of upcycling versus training from scratch initializations. Our findings suggest that the choice between these two approaches should consider both the performance of the existing dense checkpoints and the MoE training budget. We highlight two innovative techniques: gating logit normalization, which improves expert diversification, and adaptive auxiliary loss coefficients, allowing for layer-specific adjustment of auxiliary loss coefficients. Our experimental results validate the effectiveness of these methods. Leveraging these techniques and insights, we trained our upcycled Skywork-MoE on a condensed subset of our SkyPile corpus. The evaluation results demonstrate that our model delivers strong performance across a wide range of benchmarks.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "citation_count": 15,
      "reference_count": 28,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This technical report introduces the training methodologies implemented in the development of Skywork-MoE, a high-performance mixture-of-experts (MoE) large language model (LLM) with 146 billion parameters and 16 experts and highlights two innovative techniques: gating logit normalization, which improves expert diversification, and adaptive auxiliary loss coefficients, allowing for layer-specific adjustment of auxiliary loss coefficients.",
      "external_id_arxiv": "2406.06563",
      "external_id_dblp": "journals/corr/abs-2406-06563",
      "external_id_doi": "10.48550/arXiv.2406.06563",
      "external_id_corpusid": 270379962
    },
    {
      "id": "2263421000",
      "type": "author",
      "name": "Tianwen Wei"
    },
    {
      "id": "2264387465",
      "type": "author",
      "name": "Bo Zhu"
    },
    {
      "id": "2263759170",
      "type": "author",
      "name": "Liang Zhao"
    },
    {
      "id": "2263934201",
      "type": "author",
      "name": "Cheng Cheng"
    },
    {
      "id": "2264501869",
      "type": "author",
      "name": "Biye Li"
    },
    {
      "id": "2264120205",
      "type": "author",
      "name": "Weiwei Lü"
    },
    {
      "id": "2263436071",
      "type": "author",
      "name": "Peng Cheng"
    },
    {
      "id": "2263534340",
      "type": "author",
      "name": "Jianhao Zhang"
    },
    {
      "id": "2213848720",
      "type": "author",
      "name": "Xiaoyu Zhang"
    },
    {
      "id": "2304645914",
      "type": "author",
      "name": "Liang Zeng"
    },
    {
      "id": "2264407156",
      "type": "author",
      "name": "Xiaokun Wang"
    },
    {
      "id": "2263715028",
      "type": "author",
      "name": "Yutuan Ma"
    },
    {
      "id": "2263413652",
      "type": "author",
      "name": "Rui Hu"
    },
    {
      "id": "2264103352",
      "type": "author",
      "name": "Shuicheng Yan"
    },
    {
      "id": "2293095227",
      "type": "author",
      "name": "Han Fang"
    },
    {
      "id": "2263493641",
      "type": "author",
      "name": "Yahui Zhou"
    },
    {
      "id": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "paper",
      "title": "Yuan 2.0-M32: Mixture of Experts with Attention Router",
      "abstract": "Yuan 2.0-M32, with a similar base architecture as Yuan-2.0 2B, uses a mixture-of-experts architecture with 32 experts of which 2 experts are active. A new router network, Attention Router, is proposed and adopted for a more efficient selection of experts, which improves the accuracy compared to the model with classical router network. Yuan 2.0-M32 is trained with 2000B tokens from scratch, and the training computation consumption is only 9.25% of a dense model at the same parameter scale. Yuan 2.0-M32 demonstrates competitive capability on coding, math, and various domains of expertise, with only 3.7B active parameters of 40B in total, and 7.4 GFlops forward computation per token, both of which are only 1/19 of Llama3-70B. Yuan 2.0-M32 surpass Llama3-70B on MATH and ARC-Challenge benchmark, with accuracy of 55.89 and 95.8 respectively. The models and source codes of Yuan 2.0-M32 are released at Github1.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "citation_count": 4,
      "reference_count": 2,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Yuan 2.0-M32 demonstrates competitive capability on coding, math, and various domains of expertise, with only 3.7B active parameters of 40B in total, and 7.4 GFlops forward computation per token, both of which are only 1/19 of Llama3-70B.",
      "external_id_arxiv": "2405.17976",
      "external_id_dblp": "journals/corr/abs-2405-17976",
      "external_id_doi": "10.48550/arXiv.2405.17976",
      "external_id_corpusid": 270067526
    },
    {
      "id": "2303587299",
      "type": "author",
      "name": "Shaohua Wu"
    },
    {
      "id": "2141243564",
      "type": "author",
      "name": "Jiangang Luo"
    },
    {
      "id": "2268629065",
      "type": "author",
      "name": "Xi Chen"
    },
    {
      "id": "2268554345",
      "type": "author",
      "name": "Lingjun Li"
    },
    {
      "id": "2222759010",
      "type": "author",
      "name": "Xudong Zhao"
    },
    {
      "id": "2269022612",
      "type": "author",
      "name": "Tong Yu"
    },
    {
      "id": "2268560784",
      "type": "author",
      "name": "Chao Wang"
    },
    {
      "id": "2303773199",
      "type": "author",
      "name": "Yue Wang"
    },
    {
      "id": "2303903609",
      "type": "author",
      "name": "Fei Wang"
    },
    {
      "id": "2303462671",
      "type": "author",
      "name": "Weixu Qiao"
    },
    {
      "id": "2303615702",
      "type": "author",
      "name": "Houbo He"
    },
    {
      "id": "2303694488",
      "type": "author",
      "name": "Zeru Zhang"
    },
    {
      "id": "2303515829",
      "type": "author",
      "name": "Zeyu Sun"
    },
    {
      "id": "2303534151",
      "type": "author",
      "name": "Junxiong Mao"
    },
    {
      "id": "2303757494",
      "type": "author",
      "name": "Chong Shen"
    },
    {
      "id": "7f6f34090d17d0523b91c0d3cd34fe9f66ebb189",
      "type": "paper",
      "title": "Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models",
      "abstract": "The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the efficiency of training and inference for Transformer-based foundational models, yielding promising results. However, the performance of SMoE heavily depends on the choice of hyper-parameters, such as the number of experts and the number of experts to be activated (referred to as top-k), resulting in significant computational overhead due to the extensive model training by searching over various hyper-parameter configurations. As a remedy, we introduce the Dynamic Mixture of Experts (DynMoE) technique. DynMoE incorporates (1) a novel gating method that enables each token to automatically determine the number of experts to activate. (2) An adaptive process automatically adjusts the number of experts during training. Extensive numerical results across Vision, Language, and Vision-Language tasks demonstrate the effectiveness of our approach to achieve competitive performance compared to GMoE for vision and language tasks, and MoE-LLaVA for vision-language tasks, while maintaining efficiency by activating fewer parameters. Our code is available at https://github.com/LINs-lab/DynMoE.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/7f6f34090d17d0523b91c0d3cd34fe9f66ebb189",
      "citation_count": 5,
      "reference_count": 64,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The Dynamic Mixture of Experts (DynMoE) technique is introduced, which incorporates a novel gating method that enables each token to automatically determine the number of experts to activate and an adaptive process that automatically adjusts the number of experts during training.",
      "external_id_arxiv": "2405.14297",
      "external_id_dblp": "journals/corr/abs-2405-14297",
      "external_id_doi": "10.48550/arXiv.2405.14297",
      "external_id_corpusid": 269982525
    },
    {
      "id": "2164726084",
      "type": "author",
      "name": "Yongxin Guo"
    },
    {
      "id": "2302893462",
      "type": "author",
      "name": "Zhenglin Cheng"
    },
    {
      "id": "2166492136",
      "type": "author",
      "name": "Xiaoying Tang"
    },
    {
      "id": "2257122070",
      "type": "author",
      "name": "Tao Lin"
    },
    {
      "id": "d9c05ef795a82e35280ddc04f9a0c203114f546e",
      "type": "paper",
      "title": "Unchosen Experts Can Contribute Too: Unleashing MoE Models' Power by Self-Contrast",
      "abstract": "Mixture-of-Experts (MoE) has emerged as a prominent architecture for scaling model size while maintaining computational efficiency. In MoE, each token in the input sequence activates a different subset of experts determined by a routing mechanism. However, the unchosen experts in MoE models do not contribute to the output, potentially leading to underutilization of the model's capacity. In this work, we first conduct exploratory studies to demonstrate that increasing the number of activated experts does not necessarily improve and can even degrade the output quality. Then, we show that output distributions from an MoE model using different routing strategies substantially differ, indicating that different experts do not always act synergistically. Motivated by these findings, we propose Self-Contrast Mixture-of-Experts (SCMoE), a training-free strategy that utilizes unchosen experts in a self-contrast manner during inference. In SCMoE, the next-token probabilities are determined by contrasting the outputs from strong and weak activation using the same MoE model. Our method is conceptually simple and computationally lightweight, as it incurs minimal latency compared to greedy decoding. Experiments on several benchmarks (GSM8K, StrategyQA, MBPP and HumanEval) demonstrate that SCMoE can consistently enhance Mixtral 8x7B's reasoning capability across various domains. For example, it improves the accuracy on GSM8K from 61.79 to 66.94. Moreover, combining SCMoE with self-consistency yields additional gains, increasing major@20 accuracy from 75.59 to 78.31.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "url": "https://www.semanticscholar.org/paper/d9c05ef795a82e35280ddc04f9a0c203114f546e",
      "citation_count": 7,
      "reference_count": 41,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Self-Contrast Mixture-of-Experts (SCMoE), a training-free strategy that utilizes unchosen experts in a self-contrast manner during inference that can consistently enhance Mixtral 8x7B's reasoning capability across various domains is proposed.",
      "external_id_arxiv": "2405.14507",
      "external_id_dblp": "journals/corr/abs-2405-14507",
      "external_id_doi": "10.48550/arXiv.2405.14507",
      "external_id_corpusid": 269983593
    },
    {
      "id": "2302816223",
      "type": "author",
      "name": "Jiahao Wang"
    },
    {
      "id": "2137407647",
      "type": "author",
      "name": "Taiqiang Wu"
    },
    {
      "id": "2302994789",
      "type": "author",
      "name": "Yu Meng"
    },
    {
      "id": "53a803388e83ae89261624099d7be4287ace67cb",
      "type": "paper",
      "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
      "abstract": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/53a803388e83ae89261624099d7be4287ace67cb",
      "citation_count": 235,
      "reference_count": 58,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
      "external_id_arxiv": "2405.04434",
      "external_id_dblp": "journals/corr/abs-2405-04434",
      "external_id_doi": "10.48550/arXiv.2405.04434",
      "external_id_corpusid": 269613809
    },
    {
      "id": "10780897",
      "type": "author",
      "name": "Damai Dai"
    },
    {
      "id": "2156640188",
      "type": "author",
      "name": "Bo Liu (Benjamin Liu)"
    },
    {
      "id": "2243360876",
      "type": "author",
      "name": "Zihan Wang"
    },
    {
      "id": "2238628841",
      "type": "author",
      "name": "Huajian Xin"
    },
    {
      "id": "4b879f069d023e03bf537309a99bdaeb39916ea5",
      "type": "paper",
      "title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training",
      "abstract": "Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. Recently, a fully-differentiable MoE architecture, SMEAR, was proposed (Muqeeth et al., 2023), which softly merges experts in the parameter space; nevertheless, its effectiveness was only demonstrated in downstream fine-tuning on classification tasks. In this paper, we present Lory, the first approach that scales such architectures to autoregressive language model pre-training. Lory introduces two key techniques: (1) a causal segment routing strategy that achieves high efficiency for expert merging operations while preserving the autoregressive nature of language models; (2) a similarity-based data batching method that encourages expert specialization by grouping similar documents in training instances. We pre-train a series of Lory models on 150B tokens from scratch, with up to 32 experts and 30B (1.5B active) parameters. Experimental results show significant performance gains over parameter-matched dense models on both perplexity (+13.9%) and a variety of downstream tasks (+1.5%-11.1%). Despite segment-level routing, Lory models achieve competitive performance compared to state-of-the-art MoE models with token-level routing. We further demonstrate that the trained experts in Lory capture domain-level specialization without supervision. Our work highlights the potential of fully-differentiable MoE architectures for language model pre-training and advocates future research in this area.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/4b879f069d023e03bf537309a99bdaeb39916ea5",
      "citation_count": 10,
      "reference_count": 48,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Lory introduces two key techniques: a causal segment routing strategy that achieves high efficiency for expert merging operations while preserving the autoregressive nature of language models; and a similarity-based data batching method that encourages expert specialization by grouping similar documents in training instances.",
      "external_id_arxiv": "2405.03133",
      "external_id_dblp": "journals/corr/abs-2405-03133",
      "external_id_doi": "10.48550/arXiv.2405.03133",
      "external_id_corpusid": 268891288
    },
    {
      "id": "49164966",
      "type": "author",
      "name": "Zexuan Zhong"
    },
    {
      "id": "67284811",
      "type": "author",
      "name": "Mengzhou Xia"
    },
    {
      "id": "50536468",
      "type": "author",
      "name": "Danqi Chen"
    },
    {
      "id": "2261973116",
      "type": "author",
      "name": "Mike Lewis"
    },
    {
      "id": "ebcf108f8bc42140721ff02b6727b0a291362957",
      "type": "paper",
      "title": "MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based Mixture of Experts",
      "abstract": "Fine-tuning Large Language Models (LLMs) is a common practice to adapt pre-trained models for specific applications. While methods like LoRA have effectively addressed GPU memory constraints during fine-tuning, their performance often falls short, especially in multi-task scenarios. In contrast, Mixture-of-Expert (MoE) models, such as Mixtral 8x7B, demonstrate remarkable performance in multi-task learning scenarios while maintaining a reduced parameter count. However, the resource requirements of these MoEs remain challenging, particularly for consumer-grade GPUs with less than 24GB memory. To tackle these challenges, we propose MixLoRA, an approach to construct a resource-efficient sparse MoE model based on LoRA. MixLoRA inserts multiple LoRA-based experts within the feed-forward network block of a frozen pre-trained dense model and employs a commonly used top-k router. Unlike other LoRA-based MoE methods, MixLoRA enhances model performance by utilizing independent attention-layer LoRA adapters. Additionally, an auxiliary load balance loss is employed to address the imbalance problem of the router. Our evaluations show that MixLoRA improves about 9% accuracy compared to state-of-the-art PEFT methods in multi-task learning scenarios. We also propose a new high-throughput framework to alleviate the computation and memory bottlenecks during the training and inference of MOE models. This framework reduces GPU memory consumption by 40% and token computation latency by 30% during both training and inference.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/ebcf108f8bc42140721ff02b6727b0a291362957",
      "citation_count": 32,
      "reference_count": 69,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work proposes MixLoRA, an approach to construct a resource-efficient sparse MoE model based on LoRA, and proposes a new high-throughput framework to alleviate the computation and memory bottlenecks during the training and inference of MOE models.",
      "external_id_arxiv": "2404.15159",
      "external_id_dblp": "journals/corr/abs-2404-15159",
      "external_id_doi": "10.48550/arXiv.2404.15159",
      "external_id_corpusid": 269302398
    },
    {
      "id": "2269777270",
      "type": "author",
      "name": "Dengchun Li"
    },
    {
      "id": "2298231704",
      "type": "author",
      "name": "Yingzi Ma"
    },
    {
      "id": "2298214784",
      "type": "author",
      "name": "Naizheng Wang"
    },
    {
      "id": "2266012864",
      "type": "author",
      "name": "Zhiyuan Cheng"
    },
    {
      "id": "2269737823",
      "type": "author",
      "name": "Lei Duan"
    },
    {
      "id": "2279917725",
      "type": "author",
      "name": "Jie Zuo"
    },
    {
      "id": "2297927648",
      "type": "author",
      "name": "Cal Yang"
    },
    {
      "id": "2269766976",
      "type": "author",
      "name": "Mingjie Tang"
    },
    {
      "id": "c9ff9fbbe21985b35d6a070b67f22a0e065c4328",
      "type": "paper",
      "title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
      "abstract": "Large Language Models (LLMs) have achieved remarkable results, but their increasing resource demand has become a major obstacle to the development of powerful and accessible super-human intelligence. This report introduces JetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B outperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results suggest that LLM training can be much more cost-effective than generally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B is highly open and academia-friendly, using only public datasets and training code. All training parameters and data mixtures have been detailed in this report to facilitate future efforts in the development of open foundation models. This transparency aims to encourage collaboration and further advancements in the field of accessible and efficient LLMs. The model weights are publicly available at https://github.com/myshell-ai/JetMoE.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/c9ff9fbbe21985b35d6a070b67f22a0e065c4328",
      "citation_count": 16,
      "reference_count": 64,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "JetMoE-8B demonstrates impressive performance, with JetMoE-8B outperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the Llama2-13B-Chat model, suggesting that LLM training can be much more cost-effective than generally thought.",
      "external_id_dblp": "journals/corr/abs-2404-07413",
      "external_id_arxiv": "2404.07413",
      "external_id_doi": "10.48550/arXiv.2404.07413",
      "external_id_corpusid": 269042695
    },
    {
      "id": "2296151561",
      "type": "author",
      "name": "Yikang Shen"
    },
    {
      "id": "2282899341",
      "type": "author",
      "name": "Zhen Guo"
    },
    {
      "id": "2295986722",
      "type": "author",
      "name": "Tianle Cai"
    },
    {
      "id": "2296026250",
      "type": "author",
      "name": "Zengyi Qin"
    },
    {
      "id": "c814eb8684f2ce07a4f21a7ed4c06c75d63fea23",
      "type": "paper",
      "title": "Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts",
      "abstract": "Efficiency, specialization, and adaptability to new data distributions are qualities that are hard to combine in current Large Language Models. The Mixture of Experts (MoE) architecture has been the focus of significant research because its inherent conditional computation enables such desirable properties. In this work, we focus on\"upcycling\"dense expert models into an MoE, aiming to improve specialization while also adding the ability to adapt to new tasks easily. We introduce Nexus, an enhanced MoE architecture with adaptive routing where the model learns to project expert embeddings from domain representations. This approach allows Nexus to flexibly add new experts after the initial upcycling through separately trained dense models, without requiring large-scale MoE training for unseen data domains. Our experiments show that Nexus achieves a relative gain of up to 2.1% over the baseline for initial upcycling, and a 18.8% relative gain for extending the MoE with a new expert by using limited finetuning data. This flexibility of Nexus is crucial to enable an open-source ecosystem where every user continuously assembles their own MoE-mix according to their needs.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/c814eb8684f2ce07a4f21a7ed4c06c75d63fea23",
      "citation_count": 0,
      "reference_count": 60,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Nexus, an enhanced MoE architecture with adaptive routing where the model learns to project expert embeddings from domain representations, is introduced, an enhanced MoE architecture with adaptive routing where the model learns to project expert embeddings from domain representations.",
      "external_id_arxiv": "2408.15901",
      "external_id_dblp": "journals/corr/abs-2408-15901",
      "external_id_doi": "10.48550/arXiv.2408.15901",
      "external_id_corpusid": 271974671
    },
    {
      "id": "2153302934",
      "type": "author",
      "name": "Nikolas Gritsch"
    },
    {
      "id": "2283437085",
      "type": "author",
      "name": "Qizhen Zhang"
    },
    {
      "id": "153563548",
      "type": "author",
      "name": "Acyr F. Locatelli"
    },
    {
      "id": "2261493078",
      "type": "author",
      "name": "Sara Hooker"
    },
    {
      "id": "82290814",
      "type": "author",
      "name": "A. Ustun"
    },
    {
      "id": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "paper",
      "title": "BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts",
      "abstract": "The Mixture of Experts (MoE) framework has become a popular architecture for large language models due to its superior performance over dense models. However, training MoEs from scratch in a large-scale regime is prohibitively expensive. Existing methods mitigate this by pre-training multiple dense expert models independently and using them to initialize an MoE. This is done by using experts' feed-forward network (FFN) to initialize the MoE's experts while merging other parameters. However, this method limits the reuse of dense model parameters to only the FFN layers, thereby constraining the advantages when\"upcycling\"these models into MoEs. We propose BAM (Branch-Attend-Mix), a simple yet effective method that addresses this shortcoming. BAM makes full use of specialized dense models by not only using their FFN to initialize the MoE layers but also leveraging experts' attention parameters fully by initializing them into a soft-variant of Mixture of Attention (MoA) layers. We explore two methods for upcycling attention parameters: 1) initializing separate attention experts from dense models including all attention parameters for the best model performance; and 2) sharing key and value parameters across all experts to facilitate for better inference efficiency. To further improve efficiency, we adopt a parallel attention transformer architecture to MoEs, which allows the attention experts and FFN experts to be computed concurrently. Our experiments on seed models ranging from 590 million to 2 billion parameters demonstrate that BAM surpasses baselines in both perplexity and downstream task performance, within the same computational and data constraints.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "url": "https://www.semanticscholar.org/paper/175b52728d7924a625e6ec99a63343128b3c03d0",
      "citation_count": 3,
      "reference_count": 50,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "BAM (Branch-Attend-Mix), a simple yet effective method that addresses this shortcoming of existing methods for upcycling attention parameters, and adopts a parallel attention transformer architecture to MoEs, which allows the attention experts and FFN experts to be computed concurrently.",
      "external_id_arxiv": "2408.08274",
      "external_id_dblp": "journals/corr/abs-2408-08274",
      "external_id_doi": "10.48550/arXiv.2408.08274",
      "external_id_corpusid": 271874635
    },
    {
      "id": "1734996170",
      "type": "author",
      "name": "Dwaraknath Gnaneshwar"
    },
    {
      "id": "2316167977",
      "type": "author",
      "name": "Simon Guo"
    },
    {
      "id": "2303257254",
      "type": "author",
      "name": "David Cairuz"
    },
    {
      "id": "8795464",
      "type": "author",
      "name": "Bharat Venkitesh"
    },
    {
      "id": "2301154187",
      "type": "author",
      "name": "Jakob Foerster"
    },
    {
      "id": "2283848746",
      "type": "author",
      "name": "Phil Blunsom"
    },
    {
      "id": "2884561",
      "type": "author",
      "name": "Sebastian Ruder"
    },
    {
      "id": "d433b0e7f3824d6fb34d84e19146a1857695e67e",
      "type": "paper",
      "title": "Sparse Upcycling: Inference Inefficient Finetuning",
      "abstract": "Small, highly trained, open-source large language models are widely used due to their inference efficiency, but further improving their quality remains a challenge. Sparse upcycling is a promising approach that transforms a pretrained dense model into a Mixture-of-Experts (MoE) architecture, increasing the model's parameter count and quality. In this work, we compare the effectiveness of sparse upcycling against continued pretraining (CPT) across different model sizes, compute budgets, and pretraining durations. Our experiments show that sparse upcycling can achieve better quality, with improvements of over 20% relative to CPT in certain scenarios. However, this comes with a significant inference cost, leading to 40% slowdowns in high-demand inference settings for larger models. Our findings highlight the trade-off between model quality and inference efficiency, offering insights for practitioners seeking to balance model quality and deployment constraints.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/d433b0e7f3824d6fb34d84e19146a1857695e67e",
      "citation_count": 0,
      "reference_count": 38,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work compares the effectiveness of sparse upcycling against continued pretraining (CPT) across different model sizes, compute budgets, and pretraining durations, and shows that sparse upcycling can achieve better quality, but comes with a significant inference cost, leading to 40% slowdowns in high-demand inference settings for larger models.",
      "external_id_dblp": "journals/corr/abs-2411-08968",
      "external_id_arxiv": "2411.08968",
      "external_id_doi": "10.48550/arXiv.2411.08968",
      "external_id_corpusid": 274023346
    },
    {
      "id": "2040790531",
      "type": "author",
      "name": "Sasha Doubov"
    },
    {
      "id": "2076413731",
      "type": "author",
      "name": "Nikhil Sardana"
    },
    {
      "id": "120486138",
      "type": "author",
      "name": "Vitaliy Chiley"
    },
    {
      "id": "990e38839261dd52678f0de44b67b62b4991dcfb",
      "type": "paper",
      "title": "Collective Model Intelligence Requires Compatible Specialization",
      "abstract": "In this work, we explore the limitations of combining models by averaging intermediate features, referred to as model merging, and propose a new direction for achieving collective model intelligence through what we call compatible specialization. Current methods for model merging, such as parameter and feature averaging, struggle to effectively combine specialized models due to representational divergence during fine-tuning. As models specialize to their individual domains, their internal feature representations become increasingly incompatible, leading to poor performance when attempting to merge them for new tasks. We analyze this phenomenon using centered kernel alignment (CKA) and show that as models specialize, the similarity in their feature space structure diminishes, hindering their capacity for collective use. To address these challenges, we investigate routing-based merging strategies, which offer more flexible methods for combining specialized models by dynamically routing across different layers. This allows us to improve on existing methods by combining features from multiple layers rather than relying on fixed, layer-wise combinations. However, we find that these approaches still face limitations when layers within models are representationally incompatible. Our findings highlight the importance of designing new approaches for model merging that operate on well-defined input and output spaces, similar to how humans communicate through language rather than intermediate neural activations.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/990e38839261dd52678f0de44b67b62b4991dcfb",
      "citation_count": 1,
      "reference_count": 32,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work investigates routing-based merging strategies, which offer more flexible methods for combining specialized models by dynamically routing across different layers, and proposes a new direction for achieving collective model intelligence through what is called compatible specialization.",
      "external_id_dblp": "journals/corr/abs-2411-02207",
      "external_id_arxiv": "2411.02207",
      "external_id_doi": "10.48550/arXiv.2411.02207",
      "external_id_corpusid": 273821064
    },
    {
      "id": "1518270974",
      "type": "author",
      "name": "Jyothish Pari"
    },
    {
      "id": "47009988",
      "type": "author",
      "name": "Samy Jelassi"
    },
    {
      "id": "2329183817",
      "type": "author",
      "name": "Pulkit Agrawal"
    },
    {
      "id": "d0e5421fc93c99fbdcc2c3f045b11c351ea76364",
      "type": "paper",
      "title": "LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models",
      "abstract": "Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops \\emph{LibMoE}, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms. Built upon three core principles: (i) modular design, (ii) efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs more accessible to a wide range of researchers by standardizing the training and evaluation pipelines. Using LibMoE, we extensively benchmarked five state-of-the-art MoE algorithms over three different LLMs and 11 datasets under the zero-shot setting. The results show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks. With the modular design and extensive evaluation, we believe LibMoE will be invaluable for researchers to make meaningful progress towards the next generation of MoE and LLMs. Project page: \\url{https://fsoft-aic.github.io/fsoft-LibMoE.github.io}.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/d0e5421fc93c99fbdcc2c3f045b11c351ea76364",
      "citation_count": 0,
      "reference_count": 51,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "LibMoE is developed, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms and show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks.",
      "external_id_arxiv": "2411.00918",
      "external_id_dblp": "journals/corr/abs-2411-00918",
      "external_id_doi": "10.48550/arXiv.2411.00918",
      "external_id_corpusid": 273812436
    },
    {
      "id": "2324075316",
      "type": "author",
      "name": "Nam V. Nguyen"
    },
    {
      "id": "2324059216",
      "type": "author",
      "name": "Thong T. Doan"
    },
    {
      "id": "2329094191",
      "type": "author",
      "name": "Luong Tran"
    },
    {
      "id": "2329520351",
      "type": "author",
      "name": "Van Nguyen"
    },
    {
      "id": "2324053774",
      "type": "author",
      "name": "Quang Pham"
    },
    {
      "id": "680eedfe1f66c1a58105d2ee8ee071ae962a0fb6",
      "type": "paper",
      "title": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts",
      "abstract": "Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensure its quality. In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity. Technically, we propose a novel MoE routing method that employs language-specific experts and cross-lingual routing. Inspired by circuit theory, our routing analysis revealed a Spread Out in the End information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence. This insight directly led to the development of the Post-MoE architecture, which applies sparse routing only in the later layers while maintaining dense others. Experimental results demonstrate that this approach enhances the generalization of multilingual models to other languages while preserving interpretability. Finally, to efficiently scale the model to 50 languages, we introduce the concept of language family experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/680eedfe1f66c1a58105d2ee8ee071ae962a0fb6",
      "citation_count": 4,
      "reference_count": 80,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work proposes a novel MoE routing method that employs language-specific experts and cross-lingual routing, and introduces the concept of language family experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters.",
      "external_id_arxiv": "2410.10626",
      "external_id_dblp": "journals/corr/abs-2410-10626",
      "external_id_doi": "10.48550/arXiv.2410.10626",
      "external_id_corpusid": 273345382
    },
    {
      "id": "2325945678",
      "type": "author",
      "name": "Guorui Zheng"
    },
    {
      "id": "2267007135",
      "type": "author",
      "name": "Xidong Wang"
    },
    {
      "id": "2212240432",
      "type": "author",
      "name": "Juhao Liang"
    },
    {
      "id": "2290186835",
      "type": "author",
      "name": "Nuo Chen"
    },
    {
      "id": "2325912821",
      "type": "author",
      "name": "Yuping Zheng"
    },
    {
      "id": "2267007505",
      "type": "author",
      "name": "Benyou Wang"
    },
    {
      "id": "1bd59bff99d5774388764bc42390e6732cf52a1f",
      "type": "paper",
      "title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation",
      "abstract": "By merging models, AI systems can combine the distinct strengths of separate language models, achieving a balance between multiple capabilities without requiring substantial retraining. However, the integration process can be intricate due to differences in training methods and fine-tuning, typically necessitating specialized knowledge and repeated refinement. This paper explores model merging techniques across a spectrum of complexity, examining where automated methods like evolutionary strategies stand compared to hyperparameter-driven approaches such as DARE, TIES-Merging and simpler methods like Model Soups. In addition, we introduce Differentiable Adaptive Merging (DAM), an efficient, adaptive merging approach as an alternative to evolutionary merging that optimizes model integration through scaling coefficients, minimizing computational demands. Our findings reveal that even simple averaging methods, like Model Soups, perform competitively when model similarity is high, underscoring each technique's unique strengths and limitations. We open-sourced DAM, including the implementation code and experiment pipeline, on GitHub: https://github.com/arcee-ai/DAM.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/1bd59bff99d5774388764bc42390e6732cf52a1f",
      "citation_count": 2,
      "reference_count": 24,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Differentiable Adaptive Merging (DAM) is introduced, an efficient, adaptive merging approach as an alternative to evolutionary merging that optimizes model integration through scaling coefficients, minimizing computational demands.",
      "external_id_arxiv": "2410.08371",
      "external_id_dblp": "journals/corr/abs-2410-08371",
      "external_id_doi": "10.48550/arXiv.2410.08371",
      "external_id_corpusid": 273323680
    },
    {
      "id": "2325729401",
      "type": "author",
      "name": "Thomas Gauthier-Caron"
    },
    {
      "id": "51516859",
      "type": "author",
      "name": "Shamane Siriwardhana"
    },
    {
      "id": "2325730893",
      "type": "author",
      "name": "Elliot Stein"
    },
    {
      "id": "2175482685",
      "type": "author",
      "name": "Malikeh Ehghaghi"
    },
    {
      "id": "2292260669",
      "type": "author",
      "name": "Charles Goddard"
    },
    {
      "id": "2292260070",
      "type": "author",
      "name": "Mark McQuade"
    },
    {
      "id": "2047397646",
      "type": "author",
      "name": "Jacob Solawetz"
    },
    {
      "id": "2325729518",
      "type": "author",
      "name": "Maxime Labonne"
    },
    {
      "id": "b16c7d45183b9d595ab64301be019741b1528860",
      "type": "paper",
      "title": "Llemma: An Open Language Model For Mathematics",
      "abstract": "We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.",
      "year": 2023,
      "venue": "International Conference on Learning Representations",
      "url": "https://www.semanticscholar.org/paper/b16c7d45183b9d595ab64301be019741b1528860",
      "citation_count": 221,
      "reference_count": 92,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Llemma is a large language model for mathematics that outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis, and is capable of tool use and formal theorem proving without any further finetuning.",
      "external_id_dblp": "conf/iclr/AzerbayevSPSMJD24",
      "external_id_arxiv": "2310.10631",
      "external_id_doi": "10.48550/arXiv.2310.10631",
      "external_id_corpusid": 264172303
    },
    {
      "id": "2124977416",
      "type": "author",
      "name": "Zhangir Azerbayev"
    },
    {
      "id": "2184031883",
      "type": "author",
      "name": "Hailey Schoelkopf"
    },
    {
      "id": "73775191",
      "type": "author",
      "name": "Keiran Paster"
    },
    {
      "id": "2257332063",
      "type": "author",
      "name": "Marco Dos Santos"
    },
    {
      "id": "2258957756",
      "type": "author",
      "name": "S. McAleer"
    },
    {
      "id": "2278435713",
      "type": "author",
      "name": "Albert Q. Jiang"
    },
    {
      "id": "2260297681",
      "type": "author",
      "name": "Jia Deng"
    },
    {
      "id": "103476203",
      "type": "author",
      "name": "Stella Biderman"
    },
    {
      "id": "2129663",
      "type": "author",
      "name": "S. Welleck"
    },
    {
      "id": "fdacf2a732f55befdc410ea927091cad3b791f13",
      "type": "paper",
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "abstract": "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the\"Colossal Clean Crawled Corpus\"and achieve a 4x speedup over the T5-XXL model.",
      "year": 2021,
      "venue": "Journal of machine learning research",
      "url": "https://www.semanticscholar.org/paper/fdacf2a732f55befdc410ea927091cad3b791f13",
      "citation_count": 1799,
      "reference_count": 65,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work simplifies the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs and shows large sparse models may be trained, for the first time, with lower precision formats.",
      "external_id_dblp": "journals/jmlr/FedusZS22",
      "external_id_arxiv": "2101.03961",
      "external_id_corpusid": 231573431
    },
    {
      "id": "26958176",
      "type": "author",
      "name": "W. Fedus"
    },
    {
      "id": "35b142ea69598e6241f0011312128031df55895c",
      "type": "paper",
      "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
      "abstract": "Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/35b142ea69598e6241f0011312128031df55895c",
      "citation_count": 254,
      "reference_count": 57,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO is introduced.",
      "external_id_arxiv": "2402.03300",
      "external_id_dblp": "journals/corr/abs-2402-03300",
      "external_id_doi": "10.48550/arXiv.2402.03300",
      "external_id_corpusid": 267412607
    },
    {
      "id": "144202874",
      "type": "author",
      "name": "Peiyi Wang"
    },
    {
      "id": "2274917091",
      "type": "author",
      "name": "R. Xu"
    },
    {
      "id": "2278384213",
      "type": "author",
      "name": "Mingchuan Zhang"
    },
    {
      "id": "2278599324",
      "type": "author",
      "name": "Y. K. Li"
    },
    {
      "id": "49176273",
      "type": "author",
      "name": "Yu Wu"
    },
    {
      "id": "37ac7683543f0e039197a56e71e752a9ebe5998e",
      "type": "paper",
      "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
      "abstract": "To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development. One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped. Finally, we rethink our design based on the above-mentioned observations and analysis. To facilitate future MoE LLM development, we propose potential strategies for mitigating the issues we found and further improving off-the-shelf MoE LLM designs.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "url": "https://www.semanticscholar.org/paper/37ac7683543f0e039197a56e71e752a9ebe5998e",
      "citation_count": 60,
      "reference_count": 61,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development and proposes potential strategies for mitigating the issues found and further improving off-the-shelf MoE LLM designs.",
      "external_id_dblp": "journals/corr/abs-2402-01739",
      "external_id_arxiv": "2402.01739",
      "external_id_doi": "10.48550/arXiv.2402.01739",
      "external_id_corpusid": 267413104
    },
    {
      "id": "2144332771",
      "type": "author",
      "name": "Fuzhao Xue"
    },
    {
      "id": "2282549674",
      "type": "author",
      "name": "Zian Andy Zheng"
    },
    {
      "id": "2294813888",
      "type": "author",
      "name": "Yao Fu"
    },
    {
      "id": "2282536569",
      "type": "author",
      "name": "Jinjie Ni"
    },
    {
      "id": "2109654065",
      "type": "author",
      "name": "Zangwei Zheng"
    },
    {
      "id": "2249891947",
      "type": "author",
      "name": "Wangchunshu Zhou"
    },
    {
      "id": "2282532623",
      "type": "author",
      "name": "Yang You"
    },
    {
      "id": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "paper",
      "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
      "abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.",
      "year": 2024,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "url": "https://www.semanticscholar.org/paper/16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "citation_count": 151,
      "reference_count": 61,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.",
      "external_id_dblp": "conf/acl/DaiDZXGCLZYWXLH24",
      "external_id_arxiv": "2401.06066",
      "external_id_doi": "10.48550/arXiv.2401.06066",
      "external_id_corpusid": 266933338
    },
    {
      "id": "2279159169",
      "type": "author",
      "name": "Xingkai Yu"
    },
    {
      "id": "2278251752",
      "type": "author",
      "name": "Panpan Huang"
    },
    {
      "id": "3335836",
      "type": "author",
      "name": "Zhifang Sui"
    },
    {
      "id": "1cff5549753a518ed9d6a3517b5050968d710b27",
      "type": "paper",
      "title": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer",
      "abstract": "In recent times, substantial advancements have been witnessed in large language models (LLMs), exemplified by ChatGPT, showcasing remarkable proficiency across a range of complex tasks. However, many mainstream LLMs (e.g. LLaMA) are pretrained on English-dominant corpus, which limits their performance in other non-English languages. In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language. To answer this question, we conduct an extensive empirical investigation based on LLaMA, accumulating over 1440 GPU hours. We analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer. To accurately assess the model's level of knowledge, we employ four widely used standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a comprehensive evaluation of the model's response quality is conducted, considering aspects such as accuracy, fluency, informativeness, logical coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting instruction tasks from 17 diverse categories. Our evaluation results demonstrate that comparable performance to state-of-the-art transfer models can be achieved with less than 1% of the pretraining data, both in terms of knowledge alignment and response quality. Furthermore, the experimental outcomes across the thirteen low-resource languages also exhibit similar trends. We anticipate that the conclusions revealed by the experiments will aid the community in developing non-English LLMs.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/1cff5549753a518ed9d6a3517b5050968d710b27",
      "citation_count": 58,
      "reference_count": 37,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "An extensive empirical investigation based on LLaMA is conducted, demonstrating that comparable performance to state-of-the-art transfer models can be achieved with less than 1% of the pretraining data, both in terms of knowledge alignment and response quality.",
      "external_id_dblp": "journals/corr/abs-2401-01055",
      "external_id_arxiv": "2401.01055",
      "external_id_doi": "10.48550/arXiv.2401.01055",
      "external_id_corpusid": 266725709
    },
    {
      "id": "2257315251",
      "type": "author",
      "name": "Jun Zhao"
    },
    {
      "id": "2262654002",
      "type": "author",
      "name": "Zhihao Zhang"
    },
    {
      "id": "2257376355",
      "type": "author",
      "name": "Qi Zhang"
    },
    {
      "id": "2067331064",
      "type": "author",
      "name": "Tao Gui"
    },
    {
      "id": "2257129989",
      "type": "author",
      "name": "Xuanjing Huang"
    },
    {
      "id": "f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31",
      "type": "paper",
      "title": "DiLoCo: Distributed Low-Communication Training of Language Models",
      "abstract": "Large language models (LLM) have become a critical component in many applications of machine learning. However, standard approaches to training LLM require a large number of tightly interconnected accelerators, with devices exchanging gradients and other intermediate states at each optimization step. While it is difficult to build and maintain a single computing cluster hosting many accelerators, it might be easier to find several computing clusters each hosting a smaller number of devices. In this work, we propose a distributed optimization algorithm, Distributed Low-Communication (DiLoCo), that enables training of language models on islands of devices that are poorly connected. The approach is a variant of federated averaging, where the number of inner steps is large, the inner optimizer is AdamW, and the outer optimizer is Nesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8 workers performs as well as fully synchronous optimization while communicating 500 times less. DiLoCo exhibits great robustness to the data distribution of each worker. It is also robust to resources becoming unavailable over time, and vice versa, it can seamlessly leverage resources that become available during training.",
      "year": 2023,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31",
      "citation_count": 20,
      "reference_count": 38,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work proposes a distributed optimization algorithm, Distributed Low-Communication (DiLoCo), that enables training of language models on islands of devices that are poorly connected and exhibits great robustness to the data distribution of each worker.",
      "external_id_dblp": "journals/corr/abs-2311-08105",
      "external_id_arxiv": "2311.08105",
      "external_id_doi": "10.48550/arXiv.2311.08105",
      "external_id_corpusid": 265158012
    },
    {
      "id": "1660848177",
      "type": "author",
      "name": "Arthur Douillard"
    },
    {
      "id": "2191691224",
      "type": "author",
      "name": "Qixuang Feng"
    },
    {
      "id": "2228824",
      "type": "author",
      "name": "Andrei A. Rusu"
    },
    {
      "id": "2104677959",
      "type": "author",
      "name": "Rachita Chhaparia"
    },
    {
      "id": "2266468147",
      "type": "author",
      "name": "Yani Donchev"
    },
    {
      "id": "3376845",
      "type": "author",
      "name": "A. Kuncoro"
    },
    {
      "id": "1706809",
      "type": "author",
      "name": "Marc'Aurelio Ranzato"
    },
    {
      "id": "3149531",
      "type": "author",
      "name": "Arthur Szlam"
    },
    {
      "id": "2266802555",
      "type": "author",
      "name": "Jiajun Shen"
    },
    {
      "id": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "paper",
      "title": "Code Llama: Open Foundation Models for Code",
      "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",
      "year": 2023,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/0b0debb710366cdff461938c80763eace1651af6",
      "citation_count": 1537,
      "reference_count": 92,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "",
      "open_access_pdf_url": "https://arxiv.org/pdf/2308.12950",
      "open_access_status": "CLOSED",
      "external_id_dblp": "journals/corr/abs-2308-12950",
      "external_id_arxiv": "2308.12950",
      "external_id_doi": "10.48550/arXiv.2308.12950",
      "external_id_corpusid": 261100919
    },
    {
      "id": "2401865",
      "type": "author",
      "name": "Jonas Gehring"
    },
    {
      "id": "2233293602",
      "type": "author",
      "name": "Fabian Gloeckle"
    },
    {
      "id": "46879944",
      "type": "author",
      "name": "Xiaoqing Tan"
    },
    {
      "id": "2727584",
      "type": "author",
      "name": "Yossi Adi"
    },
    {
      "id": "2301500865",
      "type": "author",
      "name": "Jingyu Liu"
    },
    {
      "id": "2756187",
      "type": "author",
      "name": "J. Rapin"
    },
    {
      "id": "2233293962",
      "type": "author",
      "name": "Artyom Kozhevnikov"
    },
    {
      "id": "22229139",
      "type": "author",
      "name": "I. Evtimov"
    },
    {
      "id": "50420713",
      "type": "author",
      "name": "Manish P Bhatt"
    },
    {
      "id": "22253126",
      "type": "author",
      "name": "Wenhan Xiong"
    },
    {
      "id": "2233294119",
      "type": "author",
      "name": "Alexandre D'efossez"
    },
    {
      "id": "34672074",
      "type": "author",
      "name": "F. Azhar"
    },
    {
      "id": "2282478",
      "type": "author",
      "name": "Gabriel Synnaeve"
    },
    {
      "id": "464770587aece80cc9e3451050058e30c2aa6666",
      "type": "paper",
      "title": "Scaling Expert Language Models with Unsupervised Domain Discovery",
      "abstract": "Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessible approach to training large language models.",
      "year": 2023,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/464770587aece80cc9e3451050058e30c2aa6666",
      "citation_count": 44,
      "reference_count": 62,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "This work introduces a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora, which clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference.",
      "open_access_pdf_url": "http://arxiv.org/pdf/2303.14177",
      "open_access_status": "CLOSED",
      "external_id_dblp": "journals/corr/abs-2303-14177",
      "external_id_arxiv": "2303.14177",
      "external_id_doi": "10.48550/arXiv.2303.14177",
      "external_id_corpusid": 257756896
    },
    {
      "id": "2118481100",
      "type": "author",
      "name": "Margaret Li"
    },
    {
      "id": "35084211",
      "type": "author",
      "name": "M. Lewis"
    },
    {
      "id": "3040379",
      "type": "author",
      "name": "Weijia Shi"
    },
    {
      "id": "1745524",
      "type": "author",
      "name": "Tim Althoff"
    },
    {
      "id": "144365875",
      "type": "author",
      "name": "Noah A. Smith"
    },
    {
      "id": "1982950",
      "type": "author",
      "name": "Luke Zettlemoyer"
    },
    {
      "id": "3d300c233e7f2c0dfda0266db077a6041a592aae",
      "type": "paper",
      "title": "Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs",
      "abstract": "Although Aligned Large Language Models (LLMs) are trained to refuse harmful requests, they remain vulnerable to jailbreak attacks. Unfortunately, existing methods often focus on surface-level patterns, overlooking the deeper attack essences. As a result, defenses fail when attack prompts change, even though the underlying\"attack essence\"remains the same. To address this issue, we introduce EDDF, an \\textbf{E}ssence-\\textbf{D}riven \\textbf{D}efense \\textbf{F}ramework Against Jailbreak Attacks in LLMs. EDDF is a plug-and-play input-filtering method and operates in two stages: 1) offline essence database construction, and 2) online adversarial query detection. The key idea behind EDDF is to extract the\"attack essence\"from a diverse set of known attack instances and store it in an offline vector database. Experimental results demonstrate that EDDF significantly outperforms existing methods by reducing the Attack Success Rate by at least 20\\%, underscoring its superior robustness against jailbreak attacks.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/3d300c233e7f2c0dfda0266db077a6041a592aae",
      "citation_count": 0,
      "reference_count": 41,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Experimental results demonstrate that EDDF significantly outperforms existing methods by reducing the Attack Success Rate by at least 20\\%, underscoring its superior robustness against jailbreak attacks.",
      "external_id_arxiv": "2502.19041",
      "external_id_corpusid": 276617936
    },
    {
      "id": "2347348532",
      "type": "author",
      "name": "Shiyu Xiang"
    },
    {
      "id": "bdf8d1fae0387b7d49766c650fe9f0f7d5c887e4",
      "type": "paper",
      "title": "ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models",
      "abstract": "Reasoning over sequences of images remains a challenge for multimodal large language models (MLLMs). While recent models incorporate multi-image data during pre-training, they still struggle to recognize sequential structures, often treating images independently. This work introduces ImageChain, a framework that enhances MLLMs with sequential reasoning capabilities over image data by modeling visual sequences as a multi-turn conversation. In ImageChain, images are interleaved with corresponding textual descriptions to form a controlled dialogue that explicitly captures temporal dependencies and narrative progression. Our method optimizes for the task of next-scene description, where the model generates a context-aware description of an upcoming scene based on preceding visual and textual cues. We demonstrate that our approach improves performance on the next-scene description task -- achieving an average improvement from 3.7% to 19% in SimRate, a metric that quantifies semantic similarity to human-annotated ground truths. Moreover, ImageChain achieves robust zero-shot out-of-domain performance in applications ranging from comics to robotics. Extensive experiments validate that instruction-tuning in a multimodal, multi-turn conversation design is key to bridging the gap between static image understanding and temporally-aware reasoning.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/bdf8d1fae0387b7d49766c650fe9f0f7d5c887e4",
      "citation_count": 0,
      "reference_count": 48,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work introduces ImageChain, a framework that enhances MLLMs with sequential reasoning capabilities over image data by modeling visual sequences as a multi-turn conversation, and achieves robust zero-shot out-of-domain performance in applications ranging from comics to robotics.",
      "external_id_arxiv": "2502.19409",
      "external_id_corpusid": 276617501
    },
    {
      "id": "2347345159",
      "type": "author",
      "name": "Danae S'anchez Villegas"
    },
    {
      "id": "2319415753",
      "type": "author",
      "name": "Ingo Ziegler"
    },
    {
      "id": "407c4f4237fc0defd4182b027d67af145ca59f3f",
      "type": "paper",
      "title": "BIG-Bench Extra Hard",
      "abstract": "Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/407c4f4237fc0defd4182b027d67af145ca59f3f",
      "citation_count": 0,
      "reference_count": 61,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs.",
      "external_id_arxiv": "2502.19187",
      "external_id_corpusid": 276617687
    },
    {
      "id": "2232080874",
      "type": "author",
      "name": "M. kazemi"
    },
    {
      "id": "3422551",
      "type": "author",
      "name": "Bahare Fatemi"
    },
    {
      "id": "2317010356",
      "type": "author",
      "name": "Hritik Bansal"
    },
    {
      "id": "1798404",
      "type": "author",
      "name": "John Palowitch"
    },
    {
      "id": "2029521",
      "type": "author",
      "name": "Chrysovalantis Anastasiou"
    },
    {
      "id": "47613860",
      "type": "author",
      "name": "Sanket Vaibhav Mehta"
    },
    {
      "id": "2054098978",
      "type": "author",
      "name": "Lalit K. Jain"
    },
    {
      "id": "46254985",
      "type": "author",
      "name": "Virginia Aglietti"
    },
    {
      "id": "2347352643",
      "type": "author",
      "name": "Disha Jindal"
    },
    {
      "id": "7fb0bb1de65c819bbe82459ec924495f1039915b",
      "type": "paper",
      "title": "Steered Generation via Gradient Descent on Sparse Features",
      "abstract": "Large language models (LLMs) encode a diverse range of linguistic features within their latent representations, which can be harnessed to steer their output toward specific target characteristics. In this paper, we modify the internal structure of LLMs by training sparse autoencoders to learn a sparse representation of the query embedding, allowing precise control over the model's attention distribution. We demonstrate that manipulating this sparse representation effectively transforms the output toward different stylistic and cognitive targets. Specifically, in an educational setting, we show that the cognitive complexity of LLM-generated feedback can be systematically adjusted by modifying the encoded query representation at a specific layer. To achieve this, we guide the learned sparse embedding toward the representation of samples from the desired cognitive complexity level, using gradient-based optimization in the latent space.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/7fb0bb1de65c819bbe82459ec924495f1039915b",
      "citation_count": 0,
      "reference_count": 65,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper modify the internal structure of LLMs by training sparse autoencoders to learn a sparse representation of the query embedding, allowing precise control over the model's attention distribution.",
      "external_id_arxiv": "2502.18644",
      "external_id_corpusid": 276618068
    },
    {
      "id": "c77e4e3e3fcc0572c901e4567aa671c9cbeda556",
      "type": "paper",
      "title": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for Legal Reasoning",
      "abstract": "The Four-Element Theory is a fundamental framework in criminal law, defining the constitution of crime through four dimensions: Subject, Object, Subjective aspect, and Objective aspect. This theory is widely referenced in legal reasoning, and many Large Language Models (LLMs) attempt to incorporate it when handling legal tasks. However, current approaches rely on LLMs' internal knowledge to incorporate this theory, often lacking completeness and representativeness. To address this limitation, we introduce JUREX-4E, an expert-annotated knowledge base covering 155 criminal charges. It is structured through a progressive hierarchical annotation framework that prioritizes legal source validity and employs diverse legal interpretation methods to ensure comprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge Distinction task and apply it to Legal Case Retrieval, demonstrating its effectiveness in improving LLM performance. Experimental results validate the high quality of JUREX-4E and its substantial impact on downstream legal tasks, underscoring its potential for advancing legal AI applications. Code: https://github.com/THUlawtech/JUREX",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/c77e4e3e3fcc0572c901e4567aa671c9cbeda556",
      "citation_count": 0,
      "reference_count": 45,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "JUREX-4E, an expert-annotated knowledge base covering 155 criminal charges is introduced, structured through a progressive hierarchical annotation framework that prioritizes legal source validity and employs diverse legal interpretation methods to ensure comprehensiveness and authority.",
      "external_id_arxiv": "2502.17166",
      "external_id_corpusid": 276576211
    },
    {
      "id": "2256311663",
      "type": "author",
      "name": "Huanghai Liu"
    },
    {
      "id": "2007771781",
      "type": "author",
      "name": "Quzhe Huang"
    },
    {
      "id": "2347043166",
      "type": "author",
      "name": "Qingjing Chen"
    },
    {
      "id": "2212045428",
      "type": "author",
      "name": "Yiran Hu"
    },
    {
      "id": "2347348065",
      "type": "author",
      "name": "Jiayu Ma"
    },
    {
      "id": "33a1c680ebce8d6b371a1628af201e5528cd14ab",
      "type": "paper",
      "title": "Unlocking Scientific Concepts: How Effective Are LLM-Generated Analogies for Student Understanding and Classroom Practice?",
      "abstract": "Teaching scientific concepts is essential but challenging, and analogies help students connect new concepts to familiar ideas. Advancements in large language models (LLMs) enable generating analogies, yet their effectiveness in education remains underexplored. In this paper, we first conducted a two-stage study involving high school students and teachers to assess the effectiveness of LLM-generated analogies in biology and physics through a controlled in-class test and a classroom field study. Test results suggested that LLM-generated analogies could enhance student understanding particularly in biology, but require teachers' guidance to prevent over-reliance and overconfidence. Classroom experiments suggested that teachers could refine LLM-generated analogies to their satisfaction and inspire new analogies from generated ones, encouraged by positive classroom feedback and homework performance boosts. Based on findings, we developed and evaluated a practical system to help teachers generate and refine teaching analogies. We discussed future directions for developing and evaluating LLM-supported teaching and learning by analogy.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/33a1c680ebce8d6b371a1628af201e5528cd14ab",
      "citation_count": 0,
      "reference_count": 85,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A practical system to help teachers generate and refine teaching analogies is developed and evaluated to enhance student understanding particularly in biology, but require teachers' guidance to prevent over-reliance and overconfidence.",
      "external_id_arxiv": "2502.16895",
      "external_id_corpusid": 276575410
    },
    {
      "id": "2210213483",
      "type": "author",
      "name": "Zekai Shao"
    },
    {
      "id": "4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "type": "paper",
      "title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding",
      "abstract": "Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existing models do not incorporate this selectivity, limiting their ability to generate perception-aligned responses. To address this, we introduce Intention-Informed Auditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM (AAD-LLM), a prototype system that integrates brain signals to infer listener attention. AAD-LLM extends an auditory LLM by incorporating intracranial electroencephalography (iEEG) recordings to decode which speaker a listener is attending to and refine responses accordingly. The model first predicts the attended speaker from neural activity, then conditions response generation on this inferred attentional state. We evaluate AAD-LLM on speaker description, speech transcription and extraction, and question answering in multitalker scenarios, with both objective and subjective ratings showing improved alignment with listener intention. By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems. Demo and code available: https://aad-llm.github.io.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "citation_count": 0,
      "reference_count": 58,
      "fields_of_study": "[\"Computer Science\", \"Engineering\"]",
      "is_open_access": false,
      "tldr": "This work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems that integrates brain signals to infer listener attention.",
      "external_id_arxiv": "2502.16794",
      "external_id_corpusid": 276576148
    },
    {
      "id": "2243118841",
      "type": "author",
      "name": "Xilin Jiang"
    },
    {
      "id": "2294424733",
      "type": "author",
      "name": "Sukru Samet Dindar"
    },
    {
      "id": "2283145483",
      "type": "author",
      "name": "Vishal Choudhari"
    },
    {
      "id": "2275225078",
      "type": "author",
      "name": "Stephan Bickel"
    },
    {
      "id": "31593564",
      "type": "author",
      "name": "A. Mehta"
    },
    {
      "id": "2671690",
      "type": "author",
      "name": "G. Mckhann"
    },
    {
      "id": "145898353",
      "type": "author",
      "name": "A. Flinker"
    },
    {
      "id": "2346983817",
      "type": "author",
      "name": "Daniel Friedman"
    },
    {
      "id": "1686269",
      "type": "author",
      "name": "N. Mesgarani"
    },
    {
      "id": "0369023acce546aa8ceabbbc36a17a2f2845ebe3",
      "type": "paper",
      "title": "Proactive Privacy Amnesia for Large Language Models: Safeguarding PII with Negligible Impact on Model Utility",
      "abstract": "With the rise of large language models (LLMs), increasing research has recognized their risk of leaking personally identifiable information (PII) under malicious attacks. Although efforts have been made to protect PII in LLMs, existing methods struggle to balance privacy protection with maintaining model utility. In this paper, inspired by studies of amnesia in cognitive science, we propose a novel approach, Proactive Privacy Amnesia (PPA), to safeguard PII in LLMs while preserving their utility. This mechanism works by actively identifying and forgetting key memories most closely associated with PII in sequences, followed by a memory implanting using suitable substitute memories to maintain the LLM's functionality. We conduct evaluations across multiple models to protect common PII, such as phone numbers and physical addresses, against prevalent PII-targeted attacks, demonstrating the superiority of our method compared with other existing defensive techniques. The results show that our PPA method completely eliminates the risk of phone number exposure by 100% and significantly reduces the risk of physical address exposure by 9.8% - 87.6%, all while maintaining comparable model utility performance.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/0369023acce546aa8ceabbbc36a17a2f2845ebe3",
      "citation_count": 1,
      "reference_count": 43,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper proposes a novel approach, Proactive Privacy Amnesia (PPA), to safeguard PII in LLMs while preserving their utility by actively identifying and forgetting key memories most closely associated with PII in sequences, followed by a memory implanting using suitable substitute memories to maintain the LLM's functionality.",
      "external_id_arxiv": "2502.17591",
      "external_id_corpusid": 276580410
    },
    {
      "id": "2211526996",
      "type": "author",
      "name": "Martin Kuo"
    },
    {
      "id": "2316444865",
      "type": "author",
      "name": "Jingyang Zhang"
    },
    {
      "id": "2265652686",
      "type": "author",
      "name": "Jianyi Zhang"
    },
    {
      "id": "3d1e88f27198ab65aceb35034d0f2321555c0844",
      "type": "paper",
      "title": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large Language Models for Code Generation",
      "abstract": "With the rapid advancement of Large Language Models (LLMs), the demand for robust instruction-following capabilities in code generation tasks has grown significantly. Code generation not only facilitates faster prototyping and automated testing, but also augments developer efficiency through improved maintainability and reusability of code. In this paper, we introduce CodeIF, the first benchmark specifically designed to assess the abilities of LLMs to adhere to task-oriented instructions within diverse code generation scenarios. CodeIF encompasses a broad range of tasks, including function synthesis, error debugging, algorithmic refactoring, and code explanation, thereby providing a comprehensive suite to evaluate model performance across varying complexity levels and programming domains. We conduct extensive experiments with LLMs, analyzing their strengths and limitations in meeting the demands of these tasks. The experimental results offer valuable insights into how well current models align with human instructions, as well as the extent to which they can generate consistent, maintainable, and contextually relevant code. Our findings not only underscore the critical role that instruction-following LLMs can play in modern software development, but also illuminate pathways for future research aimed at enhancing their adaptability, reliability, and overall effectiveness in automated code generation.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/3d1e88f27198ab65aceb35034d0f2321555c0844",
      "citation_count": 0,
      "reference_count": 32,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper introduces CodeIF, the first benchmark specifically designed to assess the abilities of LLMs to adhere to task-oriented instructions within diverse code generation scenarios, and conducts extensive experiments with LLMs, offering valuable insights into how well current models align with human instructions.",
      "external_id_arxiv": "2502.19166",
      "external_id_corpusid": 276617882
    },
    {
      "id": "2347351201",
      "type": "author",
      "name": "Kaiwen Yan"
    },
    {
      "id": "7007fc0fefeec40107984217f9cfa52c87b3bc8f",
      "type": "paper",
      "title": "IndicEval-XL: Bridging Linguistic Diversity in Code Generation Across Indic Languages",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation from natural language prompts, revolutionizing software development workflows. As we advance towards agent-based development paradigms, these models form the cornerstone of next-generation software development lifecycles. However, current benchmarks for evaluating multilingual code generation capabilities are predominantly English-centric, limiting their applicability across the global developer community. To address this limitation, we present IndicEval-XL, a comprehensive benchmark for code generation that incorporates 6 major Indic languages, collectively spoken by approximately 14\\% of the world's population. Our benchmark bridges these languages with 12 programming languages, creating a robust evaluation framework. This work is particularly significant given India's representation of one-eighth of the global population and the crucial role Indic languages play in Indian society. IndicEval-XL represents a significant step toward expanding the linguistic diversity in code generation systems and evaluation frameworks. By developing resources that support multiple languages, we aim to make AI-powered development tools more inclusive and accessible to developers of various linguistic backgrounds. To facilitate further research and development in this direction, we make our dataset and evaluation benchmark publicly available at https://github.com/telekom/IndicEval-XL",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/7007fc0fefeec40107984217f9cfa52c87b3bc8f",
      "citation_count": 0,
      "reference_count": 23,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work presents IndicEval-XL, a comprehensive benchmark for code generation that incorporates 6 major Indic languages, collectively spoken by approximately 14\\% of the world's population, and bridges these languages with 12 programming languages, creating a robust evaluation framework.",
      "external_id_arxiv": "2502.19067",
      "external_id_corpusid": 276617914
    },
    {
      "id": "2347321811",
      "type": "author",
      "name": "Ujjwal Singh"
    },
    {
      "id": "2347327645",
      "type": "author",
      "name": "Aditi Sharma"
    },
    {
      "id": "e09f2fbd978500d9782ae52f93c33c5cd4af99d8",
      "type": "paper",
      "title": "Controlled Diversity: Length-optimized Natural Language Generation",
      "abstract": "LLMs are not generally able to adjust the length of their outputs based on strict length requirements, a capability that would improve their usefulness in applications that require adherence to diverse user and system requirements. We present an approach to train LLMs to acquire this capability by augmenting existing data and applying existing fine-tuning techniques, which we compare based on the trained models' adherence to the length requirement and overall response quality relative to the baseline model. Our results demonstrate that these techniques can be successfully applied to train LLMs to adhere to length requirements, with the trained models generating texts which better align to the length requirements. Our results indicate that our method may change the response quality when using training data that was not generated by the baseline model. This allows simultaneous alignment to another training objective in certain scenarios, but is undesirable otherwise. Training on a dataset containing the model's own responses eliminates this issue.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/e09f2fbd978500d9782ae52f93c33c5cd4af99d8",
      "citation_count": 0,
      "reference_count": 20,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work demonstrates that existing fine-tuning techniques can be successfully applied to train LLMs to adhere to length requirements, with the trained models generating texts which better align to the length requirements.",
      "external_id_arxiv": "2502.19347",
      "external_id_corpusid": 276617917
    },
    {
      "id": "2347352054",
      "type": "author",
      "name": "Diana Marie Schenke"
    },
    {
      "id": "2347353699",
      "type": "author",
      "name": "Timo Baumann"
    },
    {
      "id": "32d01d0710586a50b95d12f504f94d7310a208fe",
      "type": "paper",
      "title": "Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go Beyond",
      "abstract": "Large language models (LLMs) should undergo rigorous audits to identify potential risks, such as copyright and privacy infringements. Once these risks emerge, timely updates are crucial to remove undesirable responses, ensuring legal and safe model usage. It has spurred recent research into LLM unlearning, focusing on erasing targeted undesirable knowledge without compromising the integrity of other, non-targeted responses. Existing studies have introduced various unlearning objectives to pursue LLM unlearning without necessitating complete retraining. However, each of these objectives has unique properties, and no unified framework is currently available to comprehend them thoroughly. To fill the gap, we propose a toolkit of the gradient effect (G-effect), quantifying the impacts of unlearning objectives on model performance from a gradient perspective. A notable advantage is its broad ability to detail the unlearning impacts from various aspects across instances, updating steps, and LLM layers. Accordingly, the G-effect offers new insights into identifying drawbacks of existing unlearning objectives, further motivating us to explore a series of new solutions for their mitigation and improvements. Finally, we outline promising directions that merit further studies, aiming at contributing to the community to advance this important field.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/32d01d0710586a50b95d12f504f94d7310a208fe",
      "citation_count": 1,
      "reference_count": 37,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A toolkit of the gradient effect (G-effect) is proposed, quantifying the impacts of unlearning objectives on model performance from a gradient perspective, offering new insights into identifying drawbacks of existing unlearning objectives.",
      "external_id_arxiv": "2502.19301",
      "external_id_corpusid": 276617972
    },
    {
      "id": "2257363922",
      "type": "author",
      "name": "Qizhou Wang"
    },
    {
      "id": "50af31e0aa76103dfb0ed901acea0bddbdbaef8d",
      "type": "paper",
      "title": "Towards Optimal Multi-draft Speculative Decoding",
      "abstract": "Large Language Models (LLMs) have become an indispensable part of natural language processing tasks. However, autoregressive sampling has become an efficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent approach where, when generating each token, a small draft model generates multiple drafts, and the target LLM verifies them in parallel, ensuring that the final output conforms to the target model distribution. The two main design choices in MDSD are the draft sampling method and the verification algorithm. For a fixed draft sampling method, the optimal acceptance rate is a solution to an optimal transport problem, but the complexity of this problem makes it difficult to solve for the optimal acceptance rate and measure the gap between existing verification algorithms and the theoretical upper bound. This paper discusses the dual of the optimal transport problem, providing a way to efficiently compute the optimal acceptance rate. For the first time, we measure the theoretical upper bound of MDSD efficiency for vocabulary sizes in the thousands and quantify the gap between existing verification algorithms and this bound. We also compare different draft sampling methods based on their optimal acceptance rates. Our results show that the draft sampling method strongly influences the optimal acceptance rate, with sampling without replacement outperforming sampling with replacement. Additionally, existing verification algorithms do not reach the theoretical upper bound for both without replacement and with replacement sampling. Our findings suggest that carefully designed draft sampling methods can potentially improve the optimal acceptance rate and enable the development of verification algorithms that closely match the theoretical upper bound.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/50af31e0aa76103dfb0ed901acea0bddbdbaef8d",
      "citation_count": 0,
      "reference_count": 37,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The results show that the draft sampling method strongly influences the optimal acceptance rate, with sampling without replacement outperforming sampling with replacement, and the theoretical upper bound of MDSD efficiency is measured for the first time.",
      "external_id_arxiv": "2502.18779",
      "external_id_corpusid": 276617900
    },
    {
      "id": "2ede2aeaf08265ae32a2abf16df2263aa4c5491e",
      "type": "paper",
      "title": "ChatMotion: A Multimodal Multi-Agent for Human Motion Analysis",
      "abstract": "Advancements in Multimodal Large Language Models (MLLMs) have improved human motion understanding. However, these models remain constrained by their\"instruct-only\"nature, lacking interactivity and adaptability for diverse analytical perspectives. To address these challenges, we introduce ChatMotion, a multimodal multi-agent framework for human motion analysis. ChatMotion dynamically interprets user intent, decomposes complex tasks into meta-tasks, and activates specialized function modules for motion comprehension. It integrates multiple specialized modules, such as the MotionCore, to analyze human motion from various perspectives. Extensive experiments demonstrate ChatMotion's precision, adaptability, and user engagement for human motion understanding.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/2ede2aeaf08265ae32a2abf16df2263aa4c5491e",
      "citation_count": 0,
      "reference_count": 33,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "ChatMotion is introduced, a multimodal multi-agent framework for human motion analysis that dynamically interprets user intent, decomposes complex tasks into meta-tasks, and activates specialized function modules for motion comprehension.",
      "external_id_arxiv": "2502.18180",
      "external_id_corpusid": 276580945
    },
    {
      "id": "2347139631",
      "type": "author",
      "name": "Li Lei"
    },
    {
      "id": "2347044005",
      "type": "author",
      "name": "Jia Sen"
    },
    {
      "id": "2347161512",
      "type": "author",
      "name": "Jianhao Wang"
    },
    {
      "id": "2347044495",
      "type": "author",
      "name": "Zhaochong An"
    },
    {
      "id": "2347165252",
      "type": "author",
      "name": "Jiaang Li"
    },
    {
      "id": "2347042311",
      "type": "author",
      "name": "Hwang Jenq-Neng"
    },
    {
      "id": "2347043491",
      "type": "author",
      "name": "Belongie Serge"
    },
    {
      "id": "0fdad43469669767e9ce55b9eddab6620d989f0b",
      "type": "paper",
      "title": "PacQ: A SIMT Microarchitecture for Efficient Dataflow in Hyper-asymmetric GEMMs",
      "abstract": "Weight-only quantization has been widely explored in large language models (LLMs) to reduce memory storage and data loading overhead. During deployment on single-instruction-multiple-threads (SIMT) architectures, weights are stored in low-precision integer (INT) format, while activations remain in full-precision floating-point (FP) format to preserve inference accuracy. Although memory footprint and data loading requirements for weight matrices are reduced, computation performance gains remain limited due to the need to convert weights back to FP format through unpacking and dequantization before GEMM operations. In this work, we investigate methods to accelerate GEMM operations involving packed low-precision INT weights and high-precision FP activations, defining this as the hyper-asymmetric GEMM problem. Our approach co-optimizes tile-level packing and dataflow strategies for INT weight matrices. We further design a specialized FP-INT multiplier unit tailored to our packing and dataflow strategies, enabling parallel processing of multiple INT weights. Finally, we integrate the packing, dataflow, and multiplier unit into PacQ, a SIMT microarchitecture designed to efficiently accelerate hyper-asymmetric GEMMs. We show that PacQ can achieve up to 1.99x speedup and 81.4% reduction in EDP compared to weight-only quantized LLM workloads running on conventional SIMT baselines.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/0fdad43469669767e9ce55b9eddab6620d989f0b",
      "citation_count": 0,
      "reference_count": 24,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work investigates methods to accelerate GEMM operations involving packed low-precision INT weights and high-precision FP activations, defining this as the hyper-asymmetric GEMM problem and designs a specialized FP-INT multiplier unit tailored to the packing and dataflow strategies, enabling parallel processing of multiple INT weights.",
      "external_id_arxiv": "2502.18627",
      "external_id_corpusid": 276618110
    },
    {
      "id": "1820826857",
      "type": "author",
      "name": "Ruokai Yin"
    },
    {
      "id": "2265595883",
      "type": "author",
      "name": "Yuhang Li"
    },
    {
      "id": "9352814",
      "type": "author",
      "name": "P. Panda"
    },
    {
      "id": "4b6209d66a0b1cc391b3ef0ff3f78ce4e14d1f63",
      "type": "paper",
      "title": "Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines",
      "abstract": "In this work, we study the effect of annotation guidelines -- textual descriptions of event types and arguments, when instruction-tuning large language models for event extraction. We conducted a series of experiments with both human-provided and machine-generated guidelines in both full- and low-data settings. Our results demonstrate the promise of annotation guidelines when there is a decent amount of training data and highlight its effectiveness in improving cross-schema generalization and low-frequency event-type performance.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/4b6209d66a0b1cc391b3ef0ff3f78ce4e14d1f63",
      "citation_count": 0,
      "reference_count": 47,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The results demonstrate the promise of annotation guidelines when there is a decent amount of training data and highlight its effectiveness in improving cross-schema generalization and low-frequency event-type performance.",
      "external_id_arxiv": "2502.16377",
      "external_id_corpusid": 276575342
    },
    {
      "id": "cb94a289092ba4db0989d15be6d9ba79a0c50a6f",
      "type": "paper",
      "title": "PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented Generation via Parametric Pruning",
      "abstract": "Knowledge-Augmented Generation (KAG) has shown great promise in updating the internal memory of Large Language Models (LLMs) by integrating external knowledge. However, KAG inevitably faces knowledge conflicts when the internal memory contradicts external information. Current approaches to mitigating these conflicts mainly focus on improving external knowledge utilization. However, these methods have shown only limited effectiveness in mitigating the knowledge conflict problem, as internal knowledge continues to influence the generation process of LLMs. In this paper, we propose a ParametrIc Pruning-based Knowledge-Augmented Generation (PIP-KAG) approach, which prunes internal knowledge of LLMs and incorporates a plug-and-play adaptation module to help LLMs better leverage external sources. Additionally, we construct the CoConflictQA benchmark based on the hallucination of LLMs to better evaluate contextual faithfulness during answering questions. Experimental results on CoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts and improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by 13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes are available at https://github.com/OpenBMB/PIP-KAG.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/cb94a289092ba4db0989d15be6d9ba79a0c50a6f",
      "citation_count": 0,
      "reference_count": 69,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A ParametrIc Pruning-based Knowledge-Augmented Generation (PIP-KAG) approach is proposed, which prunes internal knowledge of LLMs and incorporates a plug-and-play adaptation module to help LLMs better leverage external sources.",
      "external_id_arxiv": "2502.15543",
      "external_id_corpusid": 276558062
    },
    {
      "id": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "paper",
      "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
      "abstract": "Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.",
      "year": 2022,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "citation_count": 249,
      "reference_count": 85,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper describes the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes and releases OPT-IML at both scales, together with an evaluation framework to measure three types of model generalizations.",
      "external_id_arxiv": "2212.12017",
      "external_id_dblp": "journals/corr/abs-2212-12017",
      "external_id_corpusid": 255096269
    },
    {
      "id": "2053620465",
      "type": "author",
      "name": "S. Iyer"
    },
    {
      "id": "143724481",
      "type": "author",
      "name": "Xi Victoria Lin"
    },
    {
      "id": "10721120",
      "type": "author",
      "name": "Ramakanth Pasunuru"
    },
    {
      "id": "2082239112",
      "type": "author",
      "name": "Daniel Simig"
    },
    {
      "id": "2162840444",
      "type": "author",
      "name": "Ping Yu"
    },
    {
      "id": "35752280",
      "type": "author",
      "name": "Kurt Shuster"
    },
    {
      "id": "2118914337",
      "type": "author",
      "name": "Tianlu Wang"
    },
    {
      "id": "2154975456",
      "type": "author",
      "name": "Qing Liu"
    },
    {
      "id": "2116235416",
      "type": "author",
      "name": "Xian Li"
    },
    {
      "id": "2146367747",
      "type": "author",
      "name": "Brian O'Horo"
    },
    {
      "id": "2064737506",
      "type": "author",
      "name": "Gabriel Pereyra"
    },
    {
      "id": "2155451431",
      "type": "author",
      "name": "Jeff Wang"
    },
    {
      "id": "2065332326",
      "type": "author",
      "name": "Christopher Dewan"
    },
    {
      "id": "1709797",
      "type": "author",
      "name": "Asli Celikyilmaz"
    },
    {
      "id": "2137813791",
      "type": "author",
      "name": "Luke S. Zettlemoyer"
    },
    {
      "id": "1759422",
      "type": "author",
      "name": "Veselin Stoyanov"
    },
    {
      "id": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "paper",
      "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
      "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",
      "year": 2022,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "citation_count": 2142,
      "reference_count": 171,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "BLOOM is a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers and achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning.",
      "open_access_pdf_url": "http://arxiv.org/pdf/2211.05100",
      "open_access_status": "GREEN",
      "external_id_dblp": "journals/corr/abs-2211-05100",
      "external_id_arxiv": "2211.05100",
      "external_id_doi": "10.48550/arXiv.2211.05100",
      "external_id_corpusid": 253420279
    },
    {
      "id": "2003696840",
      "type": "author",
      "name": "Christopher Akiki"
    },
    {
      "id": "2949185",
      "type": "author",
      "name": "Ellie Pavlick"
    },
    {
      "id": "2066663381",
      "type": "author",
      "name": "Suzana Ili'c"
    },
    {
      "id": "80424302",
      "type": "author",
      "name": "Daniel Hesslow"
    },
    {
      "id": "2190282134",
      "type": "author",
      "name": "Roman Castagn'e"
    },
    {
      "id": "2993731",
      "type": "author",
      "name": "A. Luccioni"
    },
    {
      "id": "1846431",
      "type": "author",
      "name": "François Yvon"
    },
    {
      "id": "2907260",
      "type": "author",
      "name": "Matthias Gallé"
    },
    {
      "id": "50195579",
      "type": "author",
      "name": "J. Tow"
    },
    {
      "id": "2531268",
      "type": "author",
      "name": "Alexander M. Rush"
    },
    {
      "id": "1451644426",
      "type": "author",
      "name": "Pawan Sasanka Ammanamanchi"
    },
    {
      "id": "68990982",
      "type": "author",
      "name": "Benoît Sagot"
    },
    {
      "id": "46219923",
      "type": "author",
      "name": "Albert Villanova del Moral"
    },
    {
      "id": "48983885",
      "type": "author",
      "name": "Rachel Bawden"
    },
    {
      "id": "32136590",
      "type": "author",
      "name": "Stas Bekman"
    },
    {
      "id": "1584940075",
      "type": "author",
      "name": "Angelina McMillan-Major"
    },
    {
      "id": "46181066",
      "type": "author",
      "name": "Iz Beltagy"
    },
    {
      "id": "2168170616",
      "type": "author",
      "name": "Huu Nguyen"
    },
    {
      "id": "145814654",
      "type": "author",
      "name": "Samson Tan"
    },
    {
      "id": "147846651",
      "type": "author",
      "name": "Pedro Ortiz Suarez"
    },
    {
      "id": "2285868436",
      "type": "author",
      "name": "Victor Sanh"
    },
    {
      "id": "2172404846",
      "type": "author",
      "name": "Hugo Laurenccon"
    },
    {
      "id": "2262249",
      "type": "author",
      "name": "Yacine Jernite"
    },
    {
      "id": "143945447",
      "type": "author",
      "name": "Julien Launay"
    },
    {
      "id": "49501003",
      "type": "author",
      "name": "Margaret Mitchell"
    },
    {
      "id": "2402716",
      "type": "author",
      "name": "Colin Raffel"
    },
    {
      "id": "2273789852",
      "type": "author",
      "name": "Aaron Gokaslan"
    },
    {
      "id": "2183598223",
      "type": "author",
      "name": "Adi Simhi"
    },
    {
      "id": "2078619062",
      "type": "author",
      "name": "Aitor Soroa Etxabe"
    },
    {
      "id": "8129718",
      "type": "author",
      "name": "Alham Fikri Aji"
    },
    {
      "id": "73769093",
      "type": "author",
      "name": "Amit Alfassy"
    },
    {
      "id": "145046059",
      "type": "author",
      "name": "Anna Rogers"
    },
    {
      "id": "2190281124",
      "type": "author",
      "name": "Ariel Kreisberg Nitzav"
    },
    {
      "id": "66247317",
      "type": "author",
      "name": "Canwen Xu"
    },
    {
      "id": "35966970",
      "type": "author",
      "name": "Chenghao Mou"
    },
    {
      "id": "1591176064",
      "type": "author",
      "name": "Chris C. Emezue"
    },
    {
      "id": "2261291789",
      "type": "author",
      "name": "Christopher Klamm"
    },
    {
      "id": "89269402",
      "type": "author",
      "name": "Colin Leong"
    },
    {
      "id": "71075073",
      "type": "author",
      "name": "Daniel Alexander van Strien"
    },
    {
      "id": "2518906",
      "type": "author",
      "name": "David Ifeoluwa Adelani"
    },
    {
      "id": "9215251",
      "type": "author",
      "name": "Dragomir R. Radev"
    },
    {
      "id": "79512668",
      "type": "author",
      "name": "E. G. Ponferrada"
    },
    {
      "id": "2190281122",
      "type": "author",
      "name": "Efrat Levkovizh"
    },
    {
      "id": "2047591327",
      "type": "author",
      "name": "Ethan Kim"
    },
    {
      "id": "2088048322",
      "type": "author",
      "name": "Eyal Natan"
    },
    {
      "id": "2067891070",
      "type": "author",
      "name": "F. Toni"
    },
    {
      "id": "13656138",
      "type": "author",
      "name": "Gérard Dupont"
    },
    {
      "id": "2158858559",
      "type": "author",
      "name": "Giada Pistilli"
    },
    {
      "id": "2218938",
      "type": "author",
      "name": "Hady ElSahar"
    },
    {
      "id": "90563027",
      "type": "author",
      "name": "Hamza Benyamina"
    },
    {
      "id": "2057078797",
      "type": "author",
      "name": "H. Tran"
    },
    {
      "id": "47948569",
      "type": "author",
      "name": "Ian Yu"
    },
    {
      "id": "1429833598",
      "type": "author",
      "name": "Idris Abdulmumin"
    },
    {
      "id": "2060080508",
      "type": "author",
      "name": "Isaac Johnson"
    },
    {
      "id": "1404791152",
      "type": "author",
      "name": "Itziar Gonzalez-Dios"
    },
    {
      "id": "144979591",
      "type": "author",
      "name": "Javier de la Rosa"
    },
    {
      "id": "2164872258",
      "type": "author",
      "name": "Jenny Chim"
    },
    {
      "id": "34176020",
      "type": "author",
      "name": "Jesse Dodge"
    },
    {
      "id": "144549416",
      "type": "author",
      "name": "Jian Zhu"
    },
    {
      "id": "2116123009",
      "type": "author",
      "name": "Jonathan Chang"
    },
    {
      "id": "2146695800",
      "type": "author",
      "name": "Jorg Frohberg"
    },
    {
      "id": "2094755167",
      "type": "author",
      "name": "Josephine Tobing"
    },
    {
      "id": "143779690",
      "type": "author",
      "name": "J. Bhattacharjee"
    },
    {
      "id": "90615055",
      "type": "author",
      "name": "Khalid Almubarak"
    },
    {
      "id": "2157630500",
      "type": "author",
      "name": "Kimbo Chen"
    },
    {
      "id": "46258841",
      "type": "author",
      "name": "Kyle Lo"
    },
    {
      "id": "51128119",
      "type": "author",
      "name": "L. V. Werra"
    },
    {
      "id": "20308468",
      "type": "author",
      "name": "Leon Weber"
    },
    {
      "id": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "paper",
      "title": "OPT: Open Pre-trained Transformer Language Models",
      "abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
      "year": 2022,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/13a0d8bb38f739990c8cd65a44061c6534f17221",
      "citation_count": 3162,
      "reference_count": 120,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, is presented, which is aimed to fully and responsibly share with interested researchers.",
      "external_id_arxiv": "2205.01068",
      "external_id_dblp": "journals/corr/abs-2205-01068",
      "external_id_corpusid": 248496292
    },
    {
      "id": "2108244542",
      "type": "author",
      "name": "Susan Zhang"
    },
    {
      "id": "3849208",
      "type": "author",
      "name": "Stephen Roller"
    },
    {
      "id": "2347956",
      "type": "author",
      "name": "Mikel Artetxe"
    },
    {
      "id": "1782969",
      "type": "author",
      "name": "Shuohui Chen"
    },
    {
      "id": "2138579860",
      "type": "author",
      "name": "Mona T. Diab"
    },
    {
      "id": "40511414",
      "type": "author",
      "name": "Myle Ott"
    },
    {
      "id": "88728159",
      "type": "author",
      "name": "Sam Shleifer"
    },
    {
      "id": "5382923",
      "type": "author",
      "name": "Anjali Sridhar"
    },
    {
      "id": "1785372925",
      "type": "author",
      "name": "Tianlu Wang"
    },
    {
      "id": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "paper",
      "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
      "abstract": "We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe GPT-NeoX-20B’s architecture and training, and evaluate its performance. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.",
      "year": 2022,
      "venue": "BIGSCIENCE",
      "url": "https://www.semanticscholar.org/paper/e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "citation_count": 741,
      "reference_count": 142,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "GPT-NeoX-20B is introduced, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.",
      "open_access_pdf_url": "http://arxiv.org/pdf/2204.06745",
      "open_access_status": "GREEN",
      "external_id_acl": "2022.bigscience-1.9",
      "external_id_dblp": "journals/corr/abs-2204-06745",
      "external_id_arxiv": "2204.06745",
      "external_id_doi": "10.48550/arXiv.2204.06745",
      "external_id_corpusid": 248177957
    },
    {
      "id": "2044098905",
      "type": "author",
      "name": "Sid Black"
    },
    {
      "id": "2162462983",
      "type": "author",
      "name": "Eric Hallahan"
    },
    {
      "id": "1404060481",
      "type": "author",
      "name": "Quentin G. Anthony"
    },
    {
      "id": "2044198157",
      "type": "author",
      "name": "Laurence Golding"
    },
    {
      "id": "46350295",
      "type": "author",
      "name": "Horace He"
    },
    {
      "id": "2044198134",
      "type": "author",
      "name": "Connor Leahy"
    },
    {
      "id": "2049410219",
      "type": "author",
      "name": "Kyle McDonell"
    },
    {
      "id": "80842917",
      "type": "author",
      "name": "Jason Phang"
    },
    {
      "id": "15043672",
      "type": "author",
      "name": "M. Pieler"
    },
    {
      "id": "2162462141",
      "type": "author",
      "name": "USVSN Sai Prashanth"
    },
    {
      "id": "2162467233",
      "type": "author",
      "name": "Shivanshu Purohit"
    },
    {
      "id": "2049583158",
      "type": "author",
      "name": "Laria Reynolds"
    },
    {
      "id": "2167077094",
      "type": "author",
      "name": "Benqi Wang"
    },
    {
      "id": "2024731554",
      "type": "author",
      "name": "Samuel Weinbach"
    },
    {
      "id": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "paper",
      "title": "Training Verifiers to Solve Math Word Problems",
      "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
      "year": 2021,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "citation_count": 3050,
      "reference_count": 31,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "It is demonstrated that verification significantly improves performance on GSM8K, and there is strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
      "external_id_arxiv": "2110.14168",
      "external_id_dblp": "journals/corr/abs-2110-14168",
      "external_id_corpusid": 239998651
    },
    {
      "id": "6062736",
      "type": "author",
      "name": "K. Cobbe"
    },
    {
      "id": "13622184",
      "type": "author",
      "name": "Vineet Kosaraju"
    },
    {
      "id": "40527594",
      "type": "author",
      "name": "Lukasz Kaiser"
    },
    {
      "id": "3407285",
      "type": "author",
      "name": "Matthias Plappert"
    },
    {
      "id": "2052366271",
      "type": "author",
      "name": "Jacob Hilton"
    },
    {
      "id": "144239765",
      "type": "author",
      "name": "Christopher Hesse"
    },
    {
      "id": "77d956cdab4508d569ae5741549b78e715fd0749",
      "type": "paper",
      "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      "abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
      "year": 2021,
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "url": "https://www.semanticscholar.org/paper/77d956cdab4508d569ae5741549b78e715fd0749",
      "citation_count": 1462,
      "reference_count": 64,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "It is suggested that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
      "open_access_pdf_url": "https://aclanthology.org/2022.acl-long.229.pdf",
      "open_access_status": "HYBRID",
      "external_id_dblp": "journals/corr/abs-2109-07958",
      "external_id_acl": "2022.acl-long.229",
      "external_id_arxiv": "2109.07958",
      "external_id_doi": "10.18653/v1/2022.acl-long.229",
      "external_id_corpusid": 237532606
    },
    {
      "id": "48639938",
      "type": "author",
      "name": "Stephanie C. Lin"
    },
    {
      "id": "47107786",
      "type": "author",
      "name": "Owain Evans"
    },
    {
      "id": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "paper",
      "title": "Program Synthesis with Large Language Models",
      "abstract": "This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.",
      "year": 2021,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "citation_count": 1437,
      "reference_count": 106,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The limits of the current generation of large language models for program synthesis in general purpose programming languages are explored, and the semantic grounding of these models is explored by fine-tuning them to predict the results of program execution.",
      "external_id_dblp": "journals/corr/abs-2108-07732",
      "external_id_arxiv": "2108.07732",
      "external_id_corpusid": 237142385
    },
    {
      "id": "2058365883",
      "type": "author",
      "name": "Jacob Austin"
    },
    {
      "id": "2624088",
      "type": "author",
      "name": "Augustus Odena"
    },
    {
      "id": "51150953",
      "type": "author",
      "name": "Maxwell Nye"
    },
    {
      "id": "40377863",
      "type": "author",
      "name": "Maarten Bosma"
    },
    {
      "id": "122064392",
      "type": "author",
      "name": "Ellen Jiang"
    },
    {
      "id": "145941081",
      "type": "author",
      "name": "Carrie J. Cai"
    },
    {
      "id": "2053829286",
      "type": "author",
      "name": "Michael Terry"
    },
    {
      "id": "1397917613",
      "type": "author",
      "name": "Quoc V. Le"
    },
    {
      "id": "152549864",
      "type": "author",
      "name": "Charles Sutton"
    },
    {
      "id": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "type": "paper",
      "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
      "abstract": "Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",
      "year": 2021,
      "venue": "NeurIPS Datasets and Benchmarks",
      "url": "https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "citation_count": 1344,
      "reference_count": 71,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work introduces MATH, a new dataset of 12,500 challenging competition mathematics problems which can be used to teach models to generate answer derivations and explanations and shows that accuracy remains relatively low, even with enormous Transformer models.",
      "external_id_dblp": "conf/nips/HendrycksBKABTS21",
      "external_id_arxiv": "2103.03874",
      "external_id_corpusid": 232134851
    },
    {
      "id": "2054194196",
      "type": "author",
      "name": "Akul Arora"
    },
    {
      "id": "2090511698",
      "type": "author",
      "name": "Eric Tang"
    },
    {
      "id": "399e7d8129c60818ee208f236c8dda17e876d21f",
      "type": "paper",
      "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
      "abstract": "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning “bad” words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
      "year": 2020,
      "venue": "Findings",
      "url": "https://www.semanticscholar.org/paper/399e7d8129c60818ee208f236c8dda17e876d21f",
      "citation_count": 1044,
      "reference_count": 88,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "It is found that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts, and empirically assess several controllable generation methods find that while data- or compute-intensive methods are more effective at steering away from toxicity than simpler solutions, no current method is failsafe against neural toxic degeneration.",
      "open_access_pdf_url": "https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf",
      "open_access_status": "HYBRID",
      "external_id_mag": "3088599783",
      "external_id_acl": "2020.findings-emnlp.301",
      "external_id_dblp": "journals/corr/abs-2009-11462",
      "external_id_arxiv": "2009.11462",
      "external_id_doi": "10.18653/v1/2020.findings-emnlp.301",
      "external_id_corpusid": 221878771
    },
    {
      "id": "1962694751",
      "type": "author",
      "name": "Samuel Gehman"
    },
    {
      "id": "2729164",
      "type": "author",
      "name": "Maarten Sap"
    },
    {
      "id": "1699545",
      "type": "author",
      "name": "Yejin Choi"
    },
    {
      "id": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "paper",
      "title": "Natural Questions: A Benchmark for Question Answering Research",
      "abstract": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
      "year": 2019,
      "venue": "Transactions of the Association for Computational Linguistics",
      "url": "https://www.semanticscholar.org/paper/17dbd7b72029181327732e4d11b52a08ed4630d0",
      "citation_count": 2870,
      "reference_count": 38,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "The Natural Questions corpus, a question answering data set, is presented, introducing robust metrics for the purposes of evaluating question answering systems; demonstrating high human upper bounds on these metrics; and establishing baseline results using competitive methods drawn from related literature.",
      "open_access_pdf_url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00276/1923288/tacl_a_00276.pdf",
      "open_access_status": "GOLD",
      "external_id_acl": "Q19-1026",
      "external_id_mag": "2912924812",
      "external_id_dblp": "journals/tacl/KwiatkowskiPRCP19",
      "external_id_doi": "10.1162/tacl_a_00276",
      "external_id_corpusid": 86611921
    },
    {
      "id": "15652489",
      "type": "author",
      "name": "T. Kwiatkowski"
    },
    {
      "id": "52578817",
      "type": "author",
      "name": "J. Palomaki"
    },
    {
      "id": "90784578",
      "type": "author",
      "name": "Olivia Redfield"
    },
    {
      "id": "123052390",
      "type": "author",
      "name": "Michael Collins"
    },
    {
      "id": "144729897",
      "type": "author",
      "name": "Ankur P. Parikh"
    },
    {
      "id": "114577307",
      "type": "author",
      "name": "Chris Alberti"
    },
    {
      "id": "153215783",
      "type": "author",
      "name": "D. Epstein"
    },
    {
      "id": "3443442",
      "type": "author",
      "name": "Illia Polosukhin"
    },
    {
      "id": "39172707",
      "type": "author",
      "name": "Jacob Devlin"
    },
    {
      "id": "2544107",
      "type": "author",
      "name": "Kenton Lee"
    },
    {
      "id": "3259253",
      "type": "author",
      "name": "Kristina Toutanova"
    },
    {
      "id": "145024664",
      "type": "author",
      "name": "Llion Jones"
    },
    {
      "id": "2554321",
      "type": "author",
      "name": "Matthew Kelcey"
    },
    {
      "id": "1744179",
      "type": "author",
      "name": "Ming-Wei Chang"
    },
    {
      "id": "2555924",
      "type": "author",
      "name": "Andrew M. Dai"
    },
    {
      "id": "39328010",
      "type": "author",
      "name": "Jakob Uszkoreit"
    },
    {
      "id": "2827616",
      "type": "author",
      "name": "Quoc V. Le"
    },
    {
      "id": "1754497",
      "type": "author",
      "name": "Slav Petrov"
    },
    {
      "id": "718c86daf8e8e59446a7c14f19e8c06ed4659581",
      "type": "paper",
      "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling",
      "abstract": "In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. This deficiency significantly limits the ability of a single CLIP model to handle images rich in visual detail. In this work, we propose a simple yet effective model-agnostic strategy, Diversified Multiplet Upcycling (DMU), for CLIP. DMU efficiently fine-tunes a series of CLIP models that capture different feature spaces, from a dense pre-trained CLIP checkpoint, sharing parameters except for the Feed-Forward Network (FFN). These models can then be transformed into a CLIP-MoE with a larger model capacity, leading to significantly enhanced performance with minimal computational overhead. To the best of our knowledge, Diversified Multiplet Upcycling is the first approach to introduce sparsely activated MoE into CLIP foundation models. Extensive experiments demonstrate the significant performance of CLIP-MoE across various zero-shot retrieval, zero-shot image classification tasks, and downstream Multimodal Large Language Model (MLLM) benchmarks by serving as a vision encoder. Furthermore, Diversified Multiplet Upcycling enables the conversion of any dense CLIP model into CLIP-MoEs, which can seamlessly replace CLIP in a plug-and-play manner without requiring further adaptation in downstream frameworks. Through Diversified Multiplet Upcycling, we aim to provide valuable insights for future research on developing more efficient and effective multimodal learning systems.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/718c86daf8e8e59446a7c14f19e8c06ed4659581",
      "citation_count": 3,
      "reference_count": 49,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Diversified Multiplet Upcycling enables the conversion of any dense CLIP model into CLIP-MoEs, which can seamlessly replace CLIP in a plug-and-play manner without requiring further adaptation in downstream frameworks.",
      "external_id_dblp": "journals/corr/abs-2409-19291",
      "external_id_arxiv": "2409.19291",
      "external_id_doi": "10.48550/arXiv.2409.19291",
      "external_id_corpusid": 272988128
    },
    {
      "id": "2284727955",
      "type": "author",
      "name": "Jihai Zhang"
    },
    {
      "id": "2265753258",
      "type": "author",
      "name": "Xiaoye Qu"
    },
    {
      "id": "1914586128",
      "type": "author",
      "name": "Tong Zhu"
    },
    {
      "id": "2307325455",
      "type": "author",
      "name": "Yu Cheng"
    },
    {
      "id": "ff8a40349db17daaed78def5f192229c3c2e2527",
      "type": "paper",
      "title": "MoE Jetpack: From Dense Checkpoints to Adaptive Mixture of Experts for Vision Tasks",
      "abstract": "The sparsely activated mixture of experts (MoE) model presents a promising alternative to traditional densely activated (dense) models, enhancing both quality and computational efficiency. However, training MoE models from scratch demands extensive data and computational resources. Moreover, public repositories like timm mainly provide pre-trained dense checkpoints, lacking similar resources for MoE models, hindering their adoption. To bridge this gap, we introduce MoE Jetpack, an effective method for fine-tuning dense checkpoints into MoE models. MoE Jetpack incorporates two key techniques: (1) checkpoint recycling, which repurposes dense checkpoints as initial weights for MoE models, thereby accelerating convergence, enhancing accuracy, and alleviating the computational burden of pre-training; (2) hyperspherical adaptive MoE (SpheroMoE) layer, which optimizes the MoE architecture for better integration of dense checkpoints, enhancing fine-tuning performance. Our experiments on vision tasks demonstrate that MoE Jetpack significantly improves convergence speed and accuracy when fine-tuning dense checkpoints into MoE models. Our code will be publicly available at https://github.com/Adlith/MoE-Jetpack.",
      "year": 2024,
      "venue": "Neural Information Processing Systems",
      "url": "https://www.semanticscholar.org/paper/ff8a40349db17daaed78def5f192229c3c2e2527",
      "citation_count": 2,
      "reference_count": 55,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "MoE Jetpack is introduced, an effective method for fine-tuning dense checkpoints into MoE models, and includes hyperspherical adaptive MoE (SpheroMoE) layer, which optimizes the MoE architecture for better integration of dense checkpoints, enhancing fine-tuning performance.",
      "external_id_dblp": "conf/nips/ZhuGLCLB24",
      "external_id_arxiv": "2406.04801",
      "external_id_doi": "10.48550/arXiv.2406.04801",
      "external_id_corpusid": 270357939
    },
    {
      "id": "2284636478",
      "type": "author",
      "name": "Xingkui Zhu"
    },
    {
      "id": "2305630565",
      "type": "author",
      "name": "Yiran Guan"
    },
    {
      "id": "94882716",
      "type": "author",
      "name": "Dingkang Liang"
    },
    {
      "id": "2305566339",
      "type": "author",
      "name": "Yuchao Chen"
    },
    {
      "id": "2266428398",
      "type": "author",
      "name": "Yuliang Liu"
    },
    {
      "id": "2238115894",
      "type": "author",
      "name": "Xiang Bai"
    },
    {
      "id": "4915538917afdfebbdc97132b6a430497db4fc54",
      "type": "paper",
      "title": "XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts",
      "abstract": "We introduce XFT, a simple yet powerful training scheme, by simply merging upcycled Mixture-of-Experts (MoE) to unleash the performance limit of instruction-tuned code Large Language Models (LLMs). While vanilla sparse upcycling fails to improve instruction tuning, XFT introduces a shared expert mechanism with a novel routing weight normalization strategy into sparse upcycling, which significantly boosts instruction tuning. After fine-tuning the upcycled MoE model, XFT introduces a learnable model merging mechanism to compile the upcycled MoE model back to a dense model, achieving upcycled MoE-level performance with only dense-model compute. By applying XFT to a 1.3B model, we create a new state-of-the-art tiny code LLM (<3B) with 67.1 and 64.6 pass@1 on HumanEval and HumanEval+ respectively. With the same data and model architecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+, along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and DS-1000, demonstrating its generalizability. XFT is fully orthogonal to existing techniques such as Evol-Instruct and OSS-Instruct, opening a new dimension for improving code instruction tuning. Codes are available at https://github.com/ise-uiuc/xft.",
      "year": 2024,
      "venue": "Volume 1",
      "url": "https://www.semanticscholar.org/paper/4915538917afdfebbdc97132b6a430497db4fc54",
      "citation_count": 1,
      "reference_count": 48,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "XFT, a simple yet powerful training scheme, by simply merging upcycled Mixture-of-Experts (MoE) to unleash the performance limit of instruction-tuned code Large Language Models (LLMs), opens a new dimension for improving code instruction tuning.",
      "external_id_dblp": "journals/corr/abs-2404-15247",
      "external_id_arxiv": "2404.15247",
      "external_id_doi": "10.48550/arXiv.2404.15247",
      "external_id_corpusid": 269302544
    },
    {
      "id": "2269694074",
      "type": "author",
      "name": "Yifeng Ding"
    },
    {
      "id": "2296736695",
      "type": "author",
      "name": "Jiawei Liu"
    },
    {
      "id": "2237736409",
      "type": "author",
      "name": "Yuxiang Wei"
    },
    {
      "id": "2080123731",
      "type": "author",
      "name": "Terry Yue Zhuo"
    },
    {
      "id": "2289125201",
      "type": "author",
      "name": "Lingming Zhang"
    },
    {
      "id": "a12bc2ddaa3a91edab682f6822e381ce9323e8fc",
      "type": "paper",
      "title": "Routers in Vision Mixture of Experts: An Empirical Study",
      "abstract": "Mixture-of-Experts (MoE) models are a promising way to scale up model capacity without significantly increasing computational cost. A key component of MoEs is the router, which decides which subset of parameters (experts) process which feature embeddings (tokens). In this paper, we present a comprehensive study of routers in MoEs for computer vision tasks. We introduce a unified MoE formulation that subsumes different MoEs with two parametric routing tensors. This formulation covers both sparse MoE, which uses a binary or hard assignment between experts and tokens, and soft MoE, which uses a soft assignment between experts and weighted combinations of tokens. Routers for sparse MoEs can be further grouped into two variants: Token Choice, which matches experts to each token, and Expert Choice, which matches tokens to each expert. We conduct head-to-head experiments with 6 different routers, including existing routers from prior work and new ones we introduce. We show that (i) many routers originally developed for language modeling can be adapted to perform strongly in vision tasks, (ii) in sparse MoE, Expert Choice routers generally outperform Token Choice routers, and (iii) soft MoEs generally outperform sparse MoEs with a fixed compute budget. These results provide new insights regarding the crucial role of routers in vision MoE models.",
      "year": 2024,
      "venue": "Trans. Mach. Learn. Res.",
      "url": "https://www.semanticscholar.org/paper/a12bc2ddaa3a91edab682f6822e381ce9323e8fc",
      "citation_count": 3,
      "reference_count": 43,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "It is shown that many routers originally developed for language modeling can be adapted to perform strongly in vision tasks, and in sparse MoE, Expert Choice routers generally outperform Token Choice routers, and soft MoEs generally outperform sparse MoEs with a fixed compute budget.",
      "external_id_arxiv": "2401.15969",
      "external_id_dblp": "journals/tmlr/LiuBRP24",
      "external_id_doi": "10.48550/arXiv.2401.15969",
      "external_id_corpusid": 267311863
    },
    {
      "id": "2281864351",
      "type": "author",
      "name": "Tianlin Liu"
    },
    {
      "id": "2281742464",
      "type": "author",
      "name": "Mathieu Blondel"
    },
    {
      "id": "145814174",
      "type": "author",
      "name": "C. Riquelme"
    },
    {
      "id": "f2e78a574925486d1f13440f55688bcffde80101",
      "type": "paper",
      "title": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks",
      "abstract": "Large language models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across general tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce parameter-efficient sparsity crafting (PESC), which crafts dense models into sparse models using the mixture-of-experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal parameter increase when guaranteeing the quality of approximation in function space compared to original sparse upcycling. Our empirical evaluation demonstrates the effectiveness of the PESC method. Using PESC during instruction tuning, our best sparse model outperforms other sparse and dense models and exhibits superior general capabilities compared to GPT-3.5.Our code is available at https://github.com/wuhy68/Parameter-Efficient-MoE.",
      "year": 2024,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "url": "https://www.semanticscholar.org/paper/f2e78a574925486d1f13440f55688bcffde80101",
      "citation_count": 10,
      "reference_count": 72,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Using PESC during instruction tuning, the best sparse model outperforms other sparse and dense models and exhibits superior general capabilities compared to GPT-3.5.",
      "external_id_dblp": "journals/corr/abs-2401-02731",
      "external_id_arxiv": "2401.02731",
      "external_id_acl": "2024.emnlp-main.43",
      "external_id_doi": "10.48550/arXiv.2401.02731",
      "external_id_corpusid": 266818175
    },
    {
      "id": "2275801674",
      "type": "author",
      "name": "Haoyuan Wu"
    },
    {
      "id": "67219756",
      "type": "author",
      "name": "Haisheng Zheng"
    },
    {
      "id": "2278380755",
      "type": "author",
      "name": "Bei Yu"
    },
    {
      "id": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "paper",
      "title": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling",
      "abstract": "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. We show experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field.",
      "year": 2023,
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "url": "https://www.semanticscholar.org/paper/ab7d320cbae173aef86c31faa087780cba44551f",
      "citation_count": 112,
      "reference_count": 53,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work introduces SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks, and presents DUS, a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining.",
      "external_id_arxiv": "2312.15166",
      "external_id_dblp": "conf/naacl/KimKPLSKKKLKAYLPGCLK24",
      "external_id_doi": "10.48550/arXiv.2312.15166",
      "external_id_corpusid": 266550918
    },
    {
      "id": "2276488417",
      "type": "author",
      "name": "Dahyun Kim"
    },
    {
      "id": "2115195904",
      "type": "author",
      "name": "Chanjun Park"
    },
    {
      "id": "2276457059",
      "type": "author",
      "name": "Sanghoon Kim"
    },
    {
      "id": "2257348655",
      "type": "author",
      "name": "Wonsung Lee"
    },
    {
      "id": "2276493203",
      "type": "author",
      "name": "Wonho Song"
    },
    {
      "id": "2276638684",
      "type": "author",
      "name": "Yunsu Kim"
    },
    {
      "id": "2256984804",
      "type": "author",
      "name": "Hyeonwoo Kim"
    },
    {
      "id": "2276638686",
      "type": "author",
      "name": "Yungi Kim"
    },
    {
      "id": "2276460413",
      "type": "author",
      "name": "Hyeonju Lee"
    },
    {
      "id": "2276491119",
      "type": "author",
      "name": "Jihoo Kim"
    },
    {
      "id": "2276426727",
      "type": "author",
      "name": "Changbae Ahn"
    },
    {
      "id": "2276490531",
      "type": "author",
      "name": "Seonghoon Yang"
    },
    {
      "id": "2276483653",
      "type": "author",
      "name": "Sukyung Lee"
    },
    {
      "id": "2276491256",
      "type": "author",
      "name": "Hyunbyung Park"
    },
    {
      "id": "2276424847",
      "type": "author",
      "name": "Gyoungjin Gim"
    },
    {
      "id": "2276424927",
      "type": "author",
      "name": "Mikyoung Cha"
    },
    {
      "id": "2276482720",
      "type": "author",
      "name": "Hwalsuk Lee"
    },
    {
      "id": "2276457144",
      "type": "author",
      "name": "Sunghun Kim"
    },
    {
      "id": "6f88133bc591cd964667a626a06debad17775757",
      "type": "paper",
      "title": "Experts Weights Averaging: A New General Training Scheme for Vision Transformers",
      "abstract": "Structural re-parameterization is a general training scheme for Convolutional Neural Networks (CNNs), which achieves performance improvement without increasing inference cost. As Vision Transformers (ViTs) are gradually surpassing CNNs in various visual tasks, one may question: if a training scheme specifically for ViTs exists that can also achieve performance improvement without increasing inference cost? Recently, Mixture-of-Experts (MoE) has attracted increasing attention, as it can efficiently scale up the capacity of Transformers at a fixed cost through sparsely activated experts. Considering that MoE can also be viewed as a multi-branch structure, can we utilize MoE to implement a ViT training scheme similar to structural re-parameterization? In this paper, we affirmatively answer these questions, with a new general training strategy for ViTs. Specifically, we decouple the training and inference phases of ViTs. During training, we replace some Feed-Forward Networks (FFNs) of the ViT with specially designed, more efficient MoEs that assign tokens to experts by random uniform partition, and perform Experts Weights Averaging (EWA) on these MoEs at the end of each iteration. After training, we convert each MoE into an FFN by averaging the experts, transforming the model back into original ViT for inference. We further provide a theoretical analysis to show why and how it works. Comprehensive experiments across various 2D and 3D visual tasks, ViT architectures, and datasets validate the effectiveness and generalizability of the proposed training scheme. Besides, our training scheme can also be applied to improve performance when fine-tuning ViTs. Lastly, but equally important, the proposed EWA technique can significantly improve the effectiveness of naive MoE in various 2D visual small datasets and 3D visual tasks.",
      "year": 2023,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/6f88133bc591cd964667a626a06debad17775757",
      "citation_count": 6,
      "reference_count": 51,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "A new general training strategy for ViTs that decouple the training and inference phases of ViTs, and can significantly improve the effectiveness of naive MoE in various 2D visual small datasets and 3D visual tasks.",
      "open_access_pdf_url": "https://arxiv.org/pdf/2308.06093",
      "open_access_status": "GREEN",
      "external_id_arxiv": "2308.06093",
      "external_id_dblp": "journals/corr/abs-2308-06093",
      "external_id_doi": "10.48550/arXiv.2308.06093",
      "external_id_corpusid": 260865986
    },
    {
      "id": "10271383",
      "type": "author",
      "name": "Yongqian Huang"
    },
    {
      "id": "2142512521",
      "type": "author",
      "name": "Peng Ye"
    },
    {
      "id": "2116084167",
      "type": "author",
      "name": "Xiaoshui Huang"
    },
    {
      "id": "39541577",
      "type": "author",
      "name": "Sheng Li"
    },
    {
      "id": "144799987",
      "type": "author",
      "name": "Tao Chen"
    },
    {
      "id": "3001348",
      "type": "author",
      "name": "Wanli Ouyang"
    },
    {
      "id": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "paper",
      "title": "Scaling Up Models and Data with t5x and seqio",
      "abstract": "Recent neural network-based language models have benefited greatly from scaling up the size of training datasets and the number of parameters in the models themselves. Scaling can be complicated due to various factors including the need to distribute computation on supercomputer clusters (e.g., TPUs), prevent bottlenecks when infeeding data, and ensure reproducible results. In this work, we present two software libraries that ease these issues: $\\texttt{t5x}$ simplifies the process of building and training large language models at scale while maintaining ease of use, and $\\texttt{seqio}$ provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines. These open-source libraries have been used to train models with hundreds of billions of parameters on datasets with multiple terabytes of training data. Along with the libraries, we release configurations and instructions for T5-like encoder-decoder models as well as GPT-like decoder-only architectures. $\\texttt{t5x}$ and $\\texttt{seqio}$ are open source and available at https://github.com/google-research/t5x and https://github.com/google/seqio, respectively.",
      "year": 2022,
      "venue": "Journal of machine learning research",
      "url": "https://www.semanticscholar.org/paper/1ed66e048bb025e75aa5ea660545285212e5341f",
      "citation_count": 185,
      "reference_count": 25,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "Two software libraries are presented that simplifies the process of building and training large language models at scale while maintaining ease of use and provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines.",
      "open_access_pdf_url": "http://arxiv.org/pdf/2203.17189",
      "open_access_status": "GREEN",
      "external_id_dblp": "journals/corr/abs-2203-17189",
      "external_id_arxiv": "2203.17189",
      "external_id_doi": "10.48550/arXiv.2203.17189",
      "external_id_corpusid": 247942109
    },
    {
      "id": "145625142",
      "type": "author",
      "name": "Adam Roberts"
    },
    {
      "id": "3351938",
      "type": "author",
      "name": "Hyung Won Chung"
    },
    {
      "id": "2159632445",
      "type": "author",
      "name": "Gaurav Mishra"
    },
    {
      "id": "2065251344",
      "type": "author",
      "name": "James Bradbury"
    },
    {
      "id": "3365603",
      "type": "author",
      "name": "D. Andor"
    },
    {
      "id": "144104130",
      "type": "author",
      "name": "Brian Lester"
    },
    {
      "id": "1579862074",
      "type": "author",
      "name": "Afroz Mohiuddin"
    },
    {
      "id": "41231781",
      "type": "author",
      "name": "Curtis Hawthorne"
    },
    {
      "id": "102549875",
      "type": "author",
      "name": "Aitor Lewkowycz"
    },
    {
      "id": "3251354",
      "type": "author",
      "name": "Alexandru Salcianu"
    },
    {
      "id": "2807540",
      "type": "author",
      "name": "Marc van Zee"
    },
    {
      "id": "7685850",
      "type": "author",
      "name": "Sebastian Goodman"
    },
    {
      "id": "2118879033",
      "type": "author",
      "name": "Haitang Hu"
    },
    {
      "id": "2160888237",
      "type": "author",
      "name": "Sasha Tsvyashchenko"
    },
    {
      "id": "1994065972",
      "type": "author",
      "name": "Jasmijn Bastings"
    },
    {
      "id": "2362210",
      "type": "author",
      "name": "Jannis Bulian"
    },
    {
      "id": "143936294",
      "type": "author",
      "name": "Xavier García"
    },
    {
      "id": "2148023",
      "type": "author",
      "name": "Jianmo Ni"
    },
    {
      "id": "2107790634",
      "type": "author",
      "name": "A. Chen"
    },
    {
      "id": "1914502282",
      "type": "author",
      "name": "Kathleen Kenealy"
    },
    {
      "id": "144797264",
      "type": "author",
      "name": "J. Clark"
    },
    {
      "id": "2108320352",
      "type": "author",
      "name": "Stephan Lee"
    },
    {
      "id": "69045302",
      "type": "author",
      "name": "Daniel H Garrette"
    },
    {
      "id": "144720379",
      "type": "author",
      "name": "Alexandre Passos"
    },
    {
      "id": "1406775898",
      "type": "author",
      "name": "Jeremy B. Maitin-Shepard"
    },
    {
      "id": "22640071",
      "type": "author",
      "name": "Noah Fiedel"
    },
    {
      "id": "35474601",
      "type": "author",
      "name": "Ryan Sepassi"
    },
    {
      "id": "102291298",
      "type": "author",
      "name": "A. Spiridonov"
    },
    {
      "id": "2813347",
      "type": "author",
      "name": "Andrea Gesmundo"
    },
    {
      "id": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
      "type": "paper",
      "title": "Scaling Vision with Sparse Mixture of Experts",
      "abstract": "Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are\"dense\", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.",
      "year": 2021,
      "venue": "Neural Information Processing Systems",
      "url": "https://www.semanticscholar.org/paper/8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
      "citation_count": 507,
      "reference_count": 75,
      "fields_of_study": "[\"Computer Science\", \"Mathematics\"]",
      "is_open_access": false,
      "tldr": "This work presents a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks, when applied to image recognition, while requiring as little as half of the compute at inference time.",
      "external_id_arxiv": "2106.05974",
      "external_id_dblp": "conf/nips/RiquelmePMNJPKH21",
      "external_id_corpusid": 235417196
    },
    {
      "id": "2060377146",
      "type": "author",
      "name": "Maxim Neumann"
    },
    {
      "id": "2068720",
      "type": "author",
      "name": "Rodolphe Jenatton"
    },
    {
      "id": "1809220",
      "type": "author",
      "name": "André Susano Pinto"
    },
    {
      "id": "51027911",
      "type": "author",
      "name": "Daniel Keysers"
    },
    {
      "id": "2a805d0e1b067444a554c5169d189fa1f649f411",
      "type": "paper",
      "title": "Scaling Vision Transformers",
      "abstract": "Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.",
      "year": 2021,
      "venue": "Computer Vision and Pattern Recognition",
      "url": "https://www.semanticscholar.org/paper/2a805d0e1b067444a554c5169d189fa1f649f411",
      "citation_count": 957,
      "reference_count": 54,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "A ViT model with two billion parameters is successfully trained, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy and performs well for few-shot transfer.",
      "open_access_pdf_url": "https://arxiv.org/pdf/2106.04560",
      "open_access_status": "GREEN",
      "external_id_arxiv": "2106.04560",
      "external_id_dblp": "journals/corr/abs-2106-04560",
      "external_id_doi": "10.1109/CVPR52688.2022.01179",
      "external_id_corpusid": 235367962
    },
    {
      "id": "2743563",
      "type": "author",
      "name": "Xiaohua Zhai"
    },
    {
      "id": "144629422",
      "type": "author",
      "name": "Alexander Kolesnikov"
    },
    {
      "id": "39611591",
      "type": "author",
      "name": "Lucas Beyer"
    },
    {
      "id": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "paper",
      "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
      "abstract": "The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in natural language processing. Surprisingly, we find that most modifications do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same codebase that we used or are relatively minor changes. We conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results.",
      "year": 2021,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "url": "https://www.semanticscholar.org/paper/79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "citation_count": 116,
      "reference_count": 73,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "It is found that most modifications to the Transformer architecture do not meaningfully improve performance, and conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results.",
      "open_access_pdf_url": "https://aclanthology.org/2021.emnlp-main.465.pdf",
      "open_access_status": "HYBRID",
      "external_id_arxiv": "2102.11972",
      "external_id_dblp": "journals/corr/abs-2102-11972",
      "external_id_acl": "2021.emnlp-main.465",
      "external_id_doi": "10.18653/v1/2021.emnlp-main.465",
      "external_id_corpusid": 232035936
    },
    {
      "id": "144447820",
      "type": "author",
      "name": "Yi Tay"
    },
    {
      "id": "79215748",
      "type": "author",
      "name": "Thibault Févry"
    },
    {
      "id": "1380243217",
      "type": "author",
      "name": "Michael Matena"
    },
    {
      "id": "1666667717",
      "type": "author",
      "name": "Karishma Malkan"
    },
    {
      "id": "34692532",
      "type": "author",
      "name": "Zhenzhong Lan"
    },
    {
      "id": "2389316",
      "type": "author",
      "name": "Yanqi Zhou"
    },
    {
      "id": "2157338362",
      "type": "author",
      "name": "Wei Li"
    },
    {
      "id": "2066767241",
      "type": "author",
      "name": "Nan Ding"
    },
    {
      "id": "2059685709",
      "type": "author",
      "name": "Jake Marcus"
    },
    {
      "id": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "paper",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
      "year": 2020,
      "venue": "International Conference on Learning Representations",
      "url": "https://www.semanticscholar.org/paper/268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "citation_count": 33889,
      "reference_count": 65,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
      "external_id_mag": "3094502228",
      "external_id_dblp": "conf/iclr/DosovitskiyB0WZ21",
      "external_id_arxiv": "2010.11929",
      "external_id_corpusid": 225039882
    },
    {
      "id": "2841331",
      "type": "author",
      "name": "Alexey Dosovitskiy"
    },
    {
      "id": "3319373",
      "type": "author",
      "name": "Dirk Weissenborn"
    },
    {
      "id": "2465270",
      "type": "author",
      "name": "Thomas Unterthiner"
    },
    {
      "id": "2274215058",
      "type": "author",
      "name": "Mostafa Dehghani"
    },
    {
      "id": "46352821",
      "type": "author",
      "name": "Matthias Minderer"
    },
    {
      "id": "2280399",
      "type": "author",
      "name": "G. Heigold"
    },
    {
      "id": "1802148",
      "type": "author",
      "name": "S. Gelly"
    },
    {
      "id": "1882f194cb43828852cc052887671e55a80f945a",
      "type": "paper",
      "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
      "abstract": "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",
      "year": 2020,
      "venue": "International Conference on Learning Representations",
      "url": "https://www.semanticscholar.org/paper/1882f194cb43828852cc052887671e55a80f945a",
      "citation_count": 973,
      "reference_count": 99,
      "fields_of_study": "[\"Computer Science\", \"Mathematics\"]",
      "is_open_access": false,
      "tldr": "GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding and it is demonstrated that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",
      "external_id_mag": "3040573126",
      "external_id_dblp": "conf/iclr/LepikhinLXCFHKS21",
      "external_id_arxiv": "2006.16668",
      "external_id_corpusid": 220265858
    },
    {
      "id": "34946720",
      "type": "author",
      "name": "HyoukJoong Lee"
    },
    {
      "id": "7167328",
      "type": "author",
      "name": "Dehao Chen"
    },
    {
      "id": "2345617",
      "type": "author",
      "name": "Orhan Firat"
    },
    {
      "id": "2145438541",
      "type": "author",
      "name": "Yanping Huang"
    },
    {
      "id": "2545358",
      "type": "author",
      "name": "Z. Chen"
    },
    {
      "id": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "type": "paper",
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
      "year": 2019,
      "venue": "Journal of machine learning research",
      "url": "https://www.semanticscholar.org/paper/6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "citation_count": 17944,
      "reference_count": 134,
      "fields_of_study": "[\"Mathematics\", \"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.",
      "external_id_mag": "2981852735",
      "external_id_dblp": "journals/corr/abs-1910-10683",
      "external_id_arxiv": "1910.10683",
      "external_id_corpusid": 204838007
    },
    {
      "id": "3844009",
      "type": "author",
      "name": "Katherine Lee"
    },
    {
      "id": "35025299",
      "type": "author",
      "name": "Peter J. Liu"
    },
    {
      "id": "d9f6ada77448664b71128bb19df15765336974a6",
      "type": "paper",
      "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
      "abstract": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at this http URL.",
      "year": 2019,
      "venue": "Neural Information Processing Systems",
      "url": "https://www.semanticscholar.org/paper/d9f6ada77448664b71128bb19df15765336974a6",
      "citation_count": 2160,
      "reference_count": 86,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A new benchmark styled after GLUE is presented, a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard are presented.",
      "external_id_dblp": "journals/corr/abs-1905-00537",
      "external_id_mag": "2943552823",
      "external_id_arxiv": "1905.00537",
      "external_id_corpusid": 143424870
    },
    {
      "id": "144906624",
      "type": "author",
      "name": "Alex Wang"
    },
    {
      "id": "100984698",
      "type": "author",
      "name": "Yada Pruksachatkun"
    },
    {
      "id": "10666396",
      "type": "author",
      "name": "Nikita Nangia"
    },
    {
      "id": "50286460",
      "type": "author",
      "name": "Amanpreet Singh"
    },
    {
      "id": "38614754",
      "type": "author",
      "name": "Julian Michael"
    },
    {
      "id": "145783676",
      "type": "author",
      "name": "Felix Hill"
    },
    {
      "id": "39455775",
      "type": "author",
      "name": "Omer Levy"
    },
    {
      "id": "3644767",
      "type": "author",
      "name": "Samuel R. Bowman"
    },
    {
      "id": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
      "type": "paper",
      "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost",
      "abstract": "In several recently proposed stochastic optimization methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves.",
      "year": 2018,
      "venue": "International Conference on Machine Learning",
      "url": "https://www.semanticscholar.org/paper/54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
      "citation_count": 966,
      "reference_count": 13,
      "fields_of_study": "[\"Computer Science\", \"Mathematics\"]",
      "is_open_access": false,
      "tldr": "This work demonstrates empirically that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow, and proposes update clipping and a gradually increasing decay rate scheme as remedies.",
      "external_id_mag": "2949215200",
      "external_id_arxiv": "1804.04235",
      "external_id_dblp": "conf/icml/ShazeerS18",
      "external_id_corpusid": 4786918
    },
    {
      "id": "144872294",
      "type": "author",
      "name": "Mitchell Stern"
    },
    {
      "id": "eec6b87ade0f50555d2639317b83d39e1210fed4",
      "type": "paper",
      "title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections",
      "abstract": "We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer .",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/eec6b87ade0f50555d2639317b83d39e1210fed4",
      "citation_count": 0,
      "reference_count": 52,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute.",
      "external_id_arxiv": "2502.12170",
      "external_id_corpusid": 276421404
    },
    {
      "id": "2301203832",
      "type": "author",
      "name": "Da Xiao"
    },
    {
      "id": "2301433926",
      "type": "author",
      "name": "Qingye Meng"
    },
    {
      "id": "2301254852",
      "type": "author",
      "name": "Shengping Li"
    },
    {
      "id": "50242841",
      "type": "author",
      "name": "Xingyuan Yuan"
    },
    {
      "id": "a3fdb2e08bd0a632184918ea9b177144f478c18d",
      "type": "paper",
      "title": "Screening of multi deep learning-based de novo molecular generation models and their application for specific target molecular generation",
      "abstract": "",
      "year": 2025,
      "venue": "Scientific Reports",
      "url": "https://www.semanticscholar.org/paper/a3fdb2e08bd0a632184918ea9b177144f478c18d",
      "citation_count": 0,
      "reference_count": 35,
      "fields_of_study": "[\"Medicine\"]",
      "is_open_access": true,
      "tldr": "An integrated end-to-end neural network learning framework based on one complete encoder-decoder architecture transformer model: Transfer Text-to-Text Transformer (T5), by learning the embedding vector representation space of conditional molecular properties to encode and guide the vector representation of SMILES sequences.",
      "external_id_pubmedcentral": "11799282",
      "external_id_doi": "10.1038/s41598-025-86840-z",
      "external_id_corpusid": 276159269,
      "external_id_pubmed": "39910075"
    },
    {
      "id": "2140282634",
      "type": "author",
      "name": "Yishu Wang"
    },
    {
      "id": "2296078432",
      "type": "author",
      "name": "Mengyao Guo"
    },
    {
      "id": "2296222589",
      "type": "author",
      "name": "Xiaomin Chen"
    },
    {
      "id": "7298446",
      "type": "author",
      "name": "Dongmei Ai"
    },
    {
      "id": "2ecaba7bc63baf9ee68940c04714e4a6420e731b",
      "type": "paper",
      "title": "ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis",
      "abstract": "Bidirectional transformers excel at sentiment analysis, and Large Language Models (LLM) are effective zero-shot learners. Might they perform better as a team? This paper explores collaborative approaches between ELECTRA and GPT-4o for three-way sentiment classification. We fine-tuned (FT) four models (ELECTRA Base/Large, GPT-4o/4o-mini) using a mix of reviews from Stanford Sentiment Treebank (SST) and DynaSent. We provided input from ELECTRA to GPT as: predicted label, probabilities, and retrieved examples. Sharing ELECTRA Base FT predictions with GPT-4o-mini significantly improved performance over either model alone (82.74 macro F1 vs. 79.29 ELECTRA Base FT, 79.52 GPT-4o-mini) and yielded the lowest cost/performance ratio (\\$0.12/F1 point). However, when GPT models were fine-tuned, including predictions decreased performance. GPT-4o FT-M was the top performer (86.99), with GPT-4o-mini FT close behind (86.77) at much less cost (\\$0.38 vs. \\$1.59/F1 point). Our results show that augmenting prompts with predictions from fine-tuned encoders is an efficient way to boost performance, and a fine-tuned GPT-4o-mini is nearly as good as GPT-4o FT at 76% less cost. Both are affordable options for projects with limited resources.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/2ecaba7bc63baf9ee68940c04714e4a6420e731b",
      "citation_count": 0,
      "reference_count": 23,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The results show that augmenting prompts with predictions from fine-tuned encoders is an efficient way to boost performance, and a fine-tuned GPT-4o-mini is nearly as good as GPT-4o FT at 76% less cost.",
      "external_id_dblp": "journals/corr/abs-2501-00062",
      "external_id_arxiv": "2501.00062",
      "external_id_doi": "10.48550/arXiv.2501.00062",
      "external_id_corpusid": 275212880
    },
    {
      "id": "104853363",
      "type": "author",
      "name": "James P. Beno"
    },
    {
      "id": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "paper",
      "title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",
      "abstract": "Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "citation_count": 17,
      "reference_count": 93,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper introduces ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders.",
      "external_id_dblp": "journals/corr/abs-2412-13663",
      "external_id_arxiv": "2412.13663",
      "external_id_doi": "10.48550/arXiv.2412.13663",
      "external_id_corpusid": 274822983
    },
    {
      "id": "2335869834",
      "type": "author",
      "name": "Benjamin Warner"
    },
    {
      "id": "2322445069",
      "type": "author",
      "name": "Antoine Chaffin"
    },
    {
      "id": "2276425110",
      "type": "author",
      "name": "Benjamin Clavi'e"
    },
    {
      "id": "2342275696",
      "type": "author",
      "name": "Orion Weller"
    },
    {
      "id": "2341914557",
      "type": "author",
      "name": "Oskar Hallström"
    },
    {
      "id": "2335869726",
      "type": "author",
      "name": "Said Taghadouini"
    },
    {
      "id": "2335870173",
      "type": "author",
      "name": "Alexis Gallagher"
    },
    {
      "id": "2335870535",
      "type": "author",
      "name": "Raja Biswas"
    },
    {
      "id": "8759332",
      "type": "author",
      "name": "Faisal Ladhak"
    },
    {
      "id": "2335870101",
      "type": "author",
      "name": "Tom Aarsen"
    },
    {
      "id": "2335870756",
      "type": "author",
      "name": "Nathan Cooper"
    },
    {
      "id": "2322441927",
      "type": "author",
      "name": "Griffin Adams"
    },
    {
      "id": "2335861967",
      "type": "author",
      "name": "Jeremy Howard"
    },
    {
      "id": "2335869805",
      "type": "author",
      "name": "Iacopo Poli"
    },
    {
      "id": "ca997f1a733e53ad0fa29041246ff655243e8c1b",
      "type": "paper",
      "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models",
      "abstract": "Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the $\\textbf{optimal approximation rate}$, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/ca997f1a733e53ad0fa29041246ff655243e8c1b",
      "citation_count": 2,
      "reference_count": 63,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper proposes a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers, and provides a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions.",
      "external_id_dblp": "journals/corr/abs-2411-03884",
      "external_id_arxiv": "2411.03884",
      "external_id_doi": "10.48550/arXiv.2411.03884",
      "external_id_corpusid": 273850118
    },
    {
      "id": "2210797120",
      "type": "author",
      "name": "Zhijian Zhuo"
    },
    {
      "id": "2125061111",
      "type": "author",
      "name": "Ya Wang"
    },
    {
      "id": "122290781",
      "type": "author",
      "name": "Yutao Zeng"
    },
    {
      "id": "2108764287",
      "type": "author",
      "name": "Xiaoqing Li"
    },
    {
      "id": "2323563576",
      "type": "author",
      "name": "Xun Zhou"
    },
    {
      "id": "2329571633",
      "type": "author",
      "name": "Jinwen Ma"
    },
    {
      "id": "6e8bfe58a437a7c87fbadccff1c4bb5b992b68a3",
      "type": "paper",
      "title": "Leveraging Continuously Differentiable Activation for Learning in Analog and Quantized Noisy Environments",
      "abstract": "Real-world analog systems, such as photonic neural networks, intrinsically suffer from noise that can impede model convergence and accuracy for a variety of deep learning models. In the presence of noise, some activation functions behave erratically or even amplify the noise. Specifically, ReLU, an activation function used ubiquitously in digital deep learning systems, not only poses a challenge to implement in analog hardware but has also been shown to perform worse than continuously differentiable activation functions. In this paper, we demonstrate that GELU and SiLU enable robust propagation of gradients in analog hardware because they are continuously differentiable functions. To analyze this cause of activation differences in the presence of noise, we used functional interpolation between ReLU and GELU/SiLU to perform analysis and training of convolutional, linear, and transformer networks on simulated analog hardware with different interpolated activation functions. We find that in ReLU, errors in the gradient due to noise are amplified during backpropagation, leading to a significant reduction in model performance. However, we observe that error amplification decreases as we move toward GELU/SiLU, until it is non-existent at GELU/SiLU demonstrating that continuously differentiable activation functions are $\\sim 100\\times$ more noise-resistant than conventional rectified activations for inputs near zero. Our findings provide guidance in selecting the appropriate activations to realize reliable and performant photonic and other analog hardware accelerators in several domains of machine learning, such as computer vision, signal processing, and beyond.",
      "year": 2025,
      "venue": "IEEE Journal of Selected Topics in Quantum Electronics",
      "url": "https://www.semanticscholar.org/paper/6e8bfe58a437a7c87fbadccff1c4bb5b992b68a3",
      "citation_count": 0,
      "reference_count": 33,
      "fields_of_study": "",
      "is_open_access": false,
      "tldr": "It is demonstrated that GELU and SiLU enable robust propagation of gradients in analog hardware because they are continuously differentiable functions, which provides guidance in selecting the appropriate activations to realize reliable and performant photonic and other analog hardware accelerators in several domains of machine learning.",
      "external_id_doi": "10.1109/JSTQE.2025.3534636",
      "external_id_corpusid": 275963059
    },
    {
      "id": "2188243752",
      "type": "author",
      "name": "Vivswan Shah"
    },
    {
      "id": "2282533462",
      "type": "author",
      "name": "Nathan Youngblood"
    },
    {
      "id": "2baf5fc029effc9a6b3790a7221dc84dba9a0f8e",
      "type": "paper",
      "title": "Heterogeneous Multi-robot Task Allocation and Scheduling via Reinforcement Learning",
      "abstract": "Many multi-robot applications require allocating a team of heterogeneous agents (robots) with different abilities to cooperatively complete a given set of spatially distributed tasks as quickly as possible. We focus on tasks that can only be initiated when all required agents are present otherwise arrived agents would be waiting idly. Agents need to not only execute a sequence of tasks by dynamically forming and disbanding teams to satisfy/match diverse ability requirements of each task but also account for the schedules of other agents to minimize unnecessary idle time. Conventional methods, such as mix-integer programming generally require centralized scheduling and a long optimization time, which limits their potential for real-world applications. In this work, we propose a reinforcement learning framework to train a decentralized policy applicable to heterogeneous agents. To address the challenge of complex cooperation learning, we further introduce a constrained flashforward mechanism to guide/constrain the agents' exploration and help them make better predictions. Through an attention mechanism that reasons about both short-term cooperation and long-term scheduling dependency, agents learn to reactively choose their next tasks (and subsequent coalitions) to avoid wasting abilities and to shorten the overall task completion time (makespan). We compare our method with State-of-the-Art heuristic and mixed-integer programming methods, demonstrating its generalization ability and showing it closely matches or outperforms these baselines while remaining at least two orders of magnitude faster.",
      "year": 2025,
      "venue": "IEEE Robotics and Automation Letters",
      "url": "https://www.semanticscholar.org/paper/2baf5fc029effc9a6b3790a7221dc84dba9a0f8e",
      "citation_count": 0,
      "reference_count": 37,
      "fields_of_study": "",
      "is_open_access": false,
      "tldr": "This work proposes a reinforcement learning framework to train a decentralized policy applicable to heterogeneous agents, and introduces a constrained flashforward mechanism to guide/constrain the agents' exploration and help them make better predictions.",
      "external_id_doi": "10.1109/LRA.2025.3534682",
      "external_id_corpusid": 275973963
    },
    {
      "id": "2315510079",
      "type": "author",
      "name": "Weiheng Dai"
    },
    {
      "id": "2342818160",
      "type": "author",
      "name": "Utkarsh Rai"
    },
    {
      "id": "2284773169",
      "type": "author",
      "name": "Jimmy Chiun"
    },
    {
      "id": "2146175818",
      "type": "author",
      "name": "Yuhong Cao"
    },
    {
      "id": "2292917033",
      "type": "author",
      "name": "Guillaume Sartoretti"
    },
    {
      "id": "7105c73fdcb84617e3a02b192bb4f0f9df2c9d95",
      "type": "paper",
      "title": "Sleep-Induced Network With Reducing Information Loss for Short-Term Load Forecasting",
      "abstract": "Short-term load forecasting (STLF) plays an important role in real-time decision-making and management of the power system while is still a challenging task. Considering that sleep improves brain memories and cognitive processes, this paper explores a approach of integrating biological mechanisms to reduce information loss of networks, and hence proposes a sleep-induced network (SI-Net) by analogy for achieving high-performance STLF. Firstly, through mimicking the sleep process, a multi-level bionic flowchart of the SI-Net is designed to integrate the gated, attention, parallel, cooperative, and asynchronous mechanisms, which not only encode features from coarse to fine but also enhance the fitting capability at the feature layer. Secondly, through imitating the brain memory paths during sleep, the primary and secondary memory paths are designed to update and store information, respectively, and their independence and collaboration avoid information loss in the SI-Net. Thirdly, the loss function constructed by the Gaussian kernel makes nonlinear errors linearly separable in the high-dimensional space, being beneficial to train the SI-Net. The experiments with real-world load datasets are performed and the results show that the SI-Net outperforms 15 baselines and presents high accuracy and stability. Bionically-inspired ideas are promising to design high-performance forecasting networks for energy systems.",
      "year": 2025,
      "venue": "IEEE Transactions on Power Systems",
      "url": "https://www.semanticscholar.org/paper/7105c73fdcb84617e3a02b192bb4f0f9df2c9d95",
      "citation_count": 0,
      "reference_count": 50,
      "fields_of_study": "",
      "is_open_access": false,
      "tldr": "",
      "external_id_doi": "10.1109/TPWRS.2024.3443156",
      "external_id_corpusid": 271869959
    },
    {
      "id": "2175564679",
      "type": "author",
      "name": "Han Wu"
    },
    {
      "id": "2271674741",
      "type": "author",
      "name": "Yan Liang"
    },
    {
      "id": "2149395933",
      "type": "author",
      "name": "Xiao-zhi Gao"
    },
    {
      "id": "2275416366",
      "type": "author",
      "name": "Jiani Heng"
    },
    {
      "id": "2117994803",
      "type": "author",
      "name": "Zhe Chen"
    },
    {
      "id": "35b9b9404695ec566f554cc2138ee60e9c45e7a9",
      "type": "paper",
      "title": "Kanana: Compute-efficient Bilingual Language Models",
      "abstract": "We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The computational cost of Kanana is significantly lower than that of state-of-the-art models of similar size. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation. Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users. Lastly, the report elaborates on plausible approaches used for language model adaptation to specific scenarios, such as embedding, retrieval augmented generation, and function calling. The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/35b9b9404695ec566f554cc2138ee60e9c45e7a9",
      "citation_count": 0,
      "reference_count": 85,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The report details the techniques employed during pre-training and post-training of the Kanana models to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation.",
      "external_id_arxiv": "2502.18934",
      "external_id_corpusid": 276617859
    },
    {
      "id": "2347341549",
      "type": "author",
      "name": "Kanana Llm Team Yunju Bak"
    },
    {
      "id": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
      "type": "paper",
      "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
      "abstract": "Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
      "year": 2018,
      "venue": "BlackboxNLP@EMNLP",
      "url": "https://www.semanticscholar.org/paper/451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
      "citation_count": 6626,
      "reference_count": 77,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "A benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models, which favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks.",
      "open_access_pdf_url": "https://www.aclweb.org/anthology/W18-5446.pdf",
      "open_access_status": "HYBRID",
      "external_id_mag": "2963310665",
      "external_id_dblp": "conf/emnlp/WangSMHLB18",
      "external_id_acl": "W18-5446",
      "external_id_arxiv": "1804.07461",
      "external_id_doi": "10.18653/v1/W18-5446",
      "external_id_corpusid": 5034059
    },
    {
      "id": "4f57f486adea0bf95c252620a4e8af39232ef8bc",
      "type": "paper",
      "title": "Swish: a Self-Gated Activation Function",
      "abstract": "The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose a new activation function, named Swish, which is simply $f(x) = x \\cdot \\text{sigmoid}(x)$. Our experiments show that Swish tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.",
      "year": 2017,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/4f57f486adea0bf95c252620a4e8af39232ef8bc",
      "citation_count": 581,
      "reference_count": 42,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The experiments show that Swish tends to work better than ReLU on deeper models across a number of challenging datasets, and its simplicity and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.",
      "external_id_mag": "2765833400",
      "external_id_arxiv": "1710.05941",
      "external_id_corpusid": 196158220
    },
    {
      "id": "3377142",
      "type": "author",
      "name": "Prajit Ramachandran"
    },
    {
      "id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "type": "paper",
      "title": "Attention is All you Need",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
      "year": 2017,
      "venue": "Neural Information Processing Systems",
      "url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "citation_count": 116896,
      "reference_count": 41,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
      "external_id_mag": "2963403868",
      "external_id_dblp": "journals/corr/VaswaniSPUJGKP17",
      "external_id_arxiv": "1706.03762",
      "external_id_corpusid": 13756489
    },
    {
      "id": "40348417",
      "type": "author",
      "name": "Ashish Vaswani"
    },
    {
      "id": "3877127",
      "type": "author",
      "name": "Niki Parmar"
    },
    {
      "id": "19177000",
      "type": "author",
      "name": "Aidan N. Gomez"
    },
    {
      "id": "88caa4a0253a8b0076176745ebc072864eab66e1",
      "type": "paper",
      "title": "Language Modeling with Gated Convolutional Networks",
      "abstract": "The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.",
      "year": 2016,
      "venue": "International Conference on Machine Learning",
      "url": "https://www.semanticscholar.org/paper/88caa4a0253a8b0076176745ebc072864eab66e1",
      "citation_count": 2257,
      "reference_count": 36,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens, is developed and is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.",
      "external_id_dblp": "conf/icml/DauphinFAG17",
      "external_id_arxiv": "1612.08083",
      "external_id_mag": "2567070169",
      "external_id_corpusid": 16119010
    },
    {
      "id": "2921469",
      "type": "author",
      "name": "Yann Dauphin"
    },
    {
      "id": "2325985",
      "type": "author",
      "name": "Michael Auli"
    },
    {
      "id": "2529182",
      "type": "author",
      "name": "David Grangier"
    },
    {
      "id": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb",
      "type": "paper",
      "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units",
      "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map, combining the intuitions of dropout and zoneout while respecting neuron values. This connection suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.",
      "year": 2016,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/4361e64f2d12d63476fdc88faf72a0f70d9a2ffb",
      "citation_count": 643,
      "reference_count": 23,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "An empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and finding performance improvements across all tasks suggests a new probabilistic understanding of nonlinearities.",
      "external_id_dblp": "journals/corr/HendrycksG16",
      "external_id_mag": "2462831000",
      "external_id_corpusid": 2359786
    },
    {
      "id": "1700980",
      "type": "author",
      "name": "Kevin Gimpel"
    },
    {
      "id": "05dd7254b632376973f3a1b4d39485da17814df5",
      "type": "paper",
      "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
      "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL",
      "year": 2016,
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "url": "https://www.semanticscholar.org/paper/05dd7254b632376973f3a1b4d39485da17814df5",
      "citation_count": 7676,
      "reference_count": 28,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": true,
      "tldr": "A strong logistic regression model is built, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%).",
      "open_access_pdf_url": "https://aclanthology.org/D16-1264.pdf",
      "open_access_status": "HYBRID",
      "external_id_dblp": "journals/corr/RajpurkarZLL16",
      "external_id_mag": "2963748441",
      "external_id_acl": "D16-1264",
      "external_id_arxiv": "1606.05250",
      "external_id_doi": "10.18653/v1/D16-1264",
      "external_id_corpusid": 11816014
    },
    {
      "id": "2706258",
      "type": "author",
      "name": "Pranav Rajpurkar"
    },
    {
      "id": "2151810148",
      "type": "author",
      "name": "Jian Zhang"
    },
    {
      "id": "2787620",
      "type": "author",
      "name": "Konstantin Lopyrev"
    },
    {
      "id": "145419642",
      "type": "author",
      "name": "Percy Liang"
    },
    {
      "id": "67107f78a84bdb2411053cb54e94fa226eea6d8e",
      "type": "paper",
      "title": "Deep Sparse Rectifier Neural Networks",
      "abstract": "While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-dierentiabil ity",
      "year": 2011,
      "venue": "International Conference on Artificial Intelligence and Statistics",
      "url": "https://www.semanticscholar.org/paper/67107f78a84bdb2411053cb54e94fa226eea6d8e",
      "citation_count": 8025,
      "reference_count": 37,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-dierentiabil ity.",
      "external_id_mag": "2156387975",
      "external_id_dblp": "journals/jmlr/GlorotBB11",
      "external_id_corpusid": 2239473
    },
    {
      "id": "3119801",
      "type": "author",
      "name": "Xavier Glorot"
    },
    {
      "id": "1713934",
      "type": "author",
      "name": "Antoine Bordes"
    },
    {
      "id": "1751762",
      "type": "author",
      "name": "Yoshua Bengio"
    },
    {
      "id": "c503d6948bafb16d9da83516746dcc9ceb52dd39",
      "type": "paper",
      "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment",
      "abstract": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose \\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t} (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT's state-of-the-art performance, closing the gap with Full FT.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/c503d6948bafb16d9da83516746dcc9ceb52dd39",
      "citation_count": 0,
      "reference_count": 78,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "GOAT, a framework that adaptively integrates relevant priors using an SVD-structured MoE, and aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor, demonstrates that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance.",
      "external_id_arxiv": "2502.16894",
      "external_id_corpusid": 276575955
    },
    {
      "id": "4b4dd452323e2126a9ba3b8a9bc70a0d7d991a16",
      "type": "paper",
      "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
      "abstract": "As large-scale AI models expand, training becomes costlier and sustaining progress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020), Hoffmann et al. (2022)) predict training loss from a static compute budget yet neglect time and efficiency, prompting the question: how can we balance ballooning GPU fleets with rapidly improving hardware and algorithms? We introduce the relative-loss equation, a time- and efficiency-aware framework that extends classical AI scaling laws. Our model shows that, without ongoing efficiency gains, advanced performance could demand millennia of training or unrealistically large GPU fleets. However, near-exponential progress remains achievable if the\"efficiency-doubling rate\"parallels Moore's Law. By formalizing this race to efficiency, we offer a quantitative roadmap for balancing front-loaded GPU investments with incremental improvements across the AI stack. Empirical trends suggest that sustained efficiency gains can push AI scaling well into the coming decade, providing a new perspective on the diminishing returns inherent in classical scaling.",
      "year": 2025,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/4b4dd452323e2126a9ba3b8a9bc70a0d7d991a16",
      "citation_count": 0,
      "reference_count": 24,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "The relative-loss equation is introduced, a time- and efficiency-aware framework that extends classical AI scaling laws and offers a quantitative roadmap for balancing front-loaded GPU investments with incremental improvements across the AI stack.",
      "external_id_dblp": "journals/corr/abs-2501-02156",
      "external_id_arxiv": "2501.02156",
      "external_id_doi": "10.48550/arXiv.2501.02156",
      "external_id_corpusid": 275336968
    },
    {
      "id": "2338865687",
      "type": "author",
      "name": "Chien-Ping Lu"
    },
    {
      "id": "f67fc0ac665d7b999ce4d67866688f564fd0f55f",
      "type": "paper",
      "title": "LLaMA-MoE v2: Exploring Sparsity of LLaMA from Perspective of Mixture-of-Experts with Post-Training",
      "abstract": "Recently, inspired by the concept of sparsity, Mixture-of-Experts (MoE) models have gained increasing popularity for scaling model size while keeping the number of activated parameters constant. In this study, we thoroughly investigate the sparsity of the dense LLaMA model by constructing MoE for both the attention (i.e., Attention MoE) and MLP (i.e., MLP MoE) modules in the transformer blocks. Specifically, we investigate different expert construction methods and granularities under the same activation conditions to analyze the impact of sparsifying the model. Additionally, to comprehensively evaluate the model's capabilities across various domains (e.g., conversation, code, math) after sparsification, we apply sparsity to the instructed large language models (LLMs) and construct instructed MoE models. To counteract the performance degradation resulting from increased sparsity, we design a two-stage post-training strategy to enhance model performance. Experiments on the LLaMA3 model demonstrate the potential effectiveness of this approach for future developments of instructed MoE models. The source codes and models are available at: \\url{https://github.com/OpenSparseLLMs/LLaMA-MoE-v2}.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/f67fc0ac665d7b999ce4d67866688f564fd0f55f",
      "citation_count": 2,
      "reference_count": 37,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This study thoroughly investigates the sparsity of the dense LLaMA model by constructing MoE for both the attention and MLP modules in the transformer blocks and designs a two-stage post-training strategy to enhance model performance.",
      "external_id_arxiv": "2411.15708",
      "external_id_dblp": "journals/corr/abs-2411-15708",
      "external_id_doi": "10.48550/arXiv.2411.15708",
      "external_id_corpusid": 274234365
    },
    {
      "id": "2187286687",
      "type": "author",
      "name": "Daize Dong"
    },
    {
      "id": "2332306988",
      "type": "author",
      "name": "Xuyang Hu"
    },
    {
      "id": "2225238340",
      "type": "author",
      "name": "Weigao Sun"
    },
    {
      "id": "de4e7a50d29554c807159ae316cf6487cbfcd2db",
      "type": "paper",
      "title": "Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models",
      "abstract": "As language models have scaled both their number of parameters and pretraining dataset sizes, the computational cost for pretraining has become intractable except for the most well-resourced teams. This increasing cost makes it ever more important to be able to reuse a model after it has completed pretraining; allowing for a model's abilities to further improve without needing to train from scratch. In this work, we detail a set of guidelines that cover how to design efficacious data distributions and learning rate schedules for continued pretraining of language models. When applying these findings within a continued pretraining run on top of a well-trained 15B parameter model, we show an improvement of 9\\% in average model accuracy compared to the baseline of continued training on the pretraining set. The resulting recipe provides a practical starting point with which to begin developing language models through reuse rather than retraining.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/de4e7a50d29554c807159ae316cf6487cbfcd2db",
      "citation_count": 15,
      "reference_count": 50,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work detail a set of guidelines that cover how to design efficacious data distributions and learning rate schedules for continued pretraining of language models and shows an improvement of 9\\% in average model accuracy compared to the baseline of continued training on the pretraining set.",
      "external_id_arxiv": "2407.07263",
      "external_id_dblp": "journals/corr/abs-2407-07263",
      "external_id_doi": "10.48550/arXiv.2407.07263",
      "external_id_corpusid": 271089092
    },
    {
      "id": "102413194",
      "type": "author",
      "name": "Jupinder Parmar"
    },
    {
      "id": "2310608832",
      "type": "author",
      "name": "Sanjev Satheesh"
    },
    {
      "id": "66870756",
      "type": "author",
      "name": "M. Patwary"
    },
    {
      "id": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "paper",
      "title": "Nemotron-4 15B Technical Report",
      "abstract": "We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/b54c3599a17db5a71349877a8567400117efbade",
      "citation_count": 18,
      "reference_count": 34,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones.",
      "external_id_dblp": "journals/corr/abs-2402-16819",
      "external_id_arxiv": "2402.16819",
      "external_id_doi": "10.48550/arXiv.2402.16819",
      "external_id_corpusid": 268033639
    },
    {
      "id": "9358910",
      "type": "author",
      "name": "Shrimai Prabhumoye"
    },
    {
      "id": "2287849760",
      "type": "author",
      "name": "Joseph Jennings"
    },
    {
      "id": "2253531461",
      "type": "author",
      "name": "Sandeep Subramanian"
    },
    {
      "id": "2287768443",
      "type": "author",
      "name": "Dan Su"
    },
    {
      "id": "2283871700",
      "type": "author",
      "name": "Chen Zhu"
    },
    {
      "id": "2251007290",
      "type": "author",
      "name": "Deepak Narayanan"
    },
    {
      "id": "151472559",
      "type": "author",
      "name": "Aastha Jhunjhunwala"
    },
    {
      "id": "80563762",
      "type": "author",
      "name": "Ayush Dattagupta"
    },
    {
      "id": "1654185172",
      "type": "author",
      "name": "Vibhu Jawa"
    },
    {
      "id": "2287874054",
      "type": "author",
      "name": "Jiwei Liu"
    },
    {
      "id": "1789266135",
      "type": "author",
      "name": "Ameya Mahabaleshwarkar"
    },
    {
      "id": "1739735954",
      "type": "author",
      "name": "Osvald Nitski"
    },
    {
      "id": "2267493444",
      "type": "author",
      "name": "Annika Brundyn"
    },
    {
      "id": "2287835428",
      "type": "author",
      "name": "James Maki"
    },
    {
      "id": "2287826306",
      "type": "author",
      "name": "Miguel Martinez"
    },
    {
      "id": "2287859963",
      "type": "author",
      "name": "Jiaxuan You"
    },
    {
      "id": "51028721",
      "type": "author",
      "name": "John Kamalu"
    },
    {
      "id": "3081566",
      "type": "author",
      "name": "P. LeGresley"
    },
    {
      "id": "2287854990",
      "type": "author",
      "name": "Denys Fridman"
    },
    {
      "id": "2260131747",
      "type": "author",
      "name": "Jared Casper"
    },
    {
      "id": "2787022",
      "type": "author",
      "name": "Oleksii Kuchaiev"
    },
    {
      "id": "2287847441",
      "type": "author",
      "name": "Jonathan Cohen"
    },
    {
      "id": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "paper",
      "title": "Scaling Laws for Fine-Grained Mixture of Experts",
      "abstract": "Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.",
      "year": 2024,
      "venue": "International Conference on Machine Learning",
      "url": "https://www.semanticscholar.org/paper/9548bacc4c7714151b674748dc86e2cc185a4955",
      "citation_count": 34,
      "reference_count": 25,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work introduces a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts in MoE, and establishes scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity.",
      "external_id_dblp": "conf/icml/LudziejewskiKAP24",
      "external_id_arxiv": "2402.07871",
      "external_id_doi": "10.48550/arXiv.2402.07871",
      "external_id_corpusid": 267626982
    },
    {
      "id": "2261391307",
      "type": "author",
      "name": "Jakub Krajewski"
    },
    {
      "id": "2261391884",
      "type": "author",
      "name": "Jan Ludziejewski"
    },
    {
      "id": "2287756379",
      "type": "author",
      "name": "Kamil Adamczewski"
    },
    {
      "id": "2261392151",
      "type": "author",
      "name": "Maciej Pi'oro"
    },
    {
      "id": "2261391303",
      "type": "author",
      "name": "Michal Krutul"
    },
    {
      "id": "2184103686",
      "type": "author",
      "name": "Szymon Antoniak"
    },
    {
      "id": "2278427988",
      "type": "author",
      "name": "Kamil Ciebiera"
    },
    {
      "id": "2278427725",
      "type": "author",
      "name": "Krystian Kr'ol"
    },
    {
      "id": "2261392065",
      "type": "author",
      "name": "Tomasz Odrzyg'o'zd'z"
    },
    {
      "id": "2268495134",
      "type": "author",
      "name": "Piotr Sankowski"
    },
    {
      "id": "2261392232",
      "type": "author",
      "name": "Marek Cygan"
    },
    {
      "id": "25898662",
      "type": "author",
      "name": "Sebastian Jaszczur"
    },
    {
      "id": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "paper",
      "title": "Mixtral of Experts",
      "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/411114f989a3d1083d90afd265103132fee94ebe",
      "citation_count": 789,
      "reference_count": 35,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work introduces Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model that vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks and provides a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks.",
      "external_id_dblp": "journals/corr/abs-2401-04088",
      "external_id_arxiv": "2401.04088",
      "external_id_doi": "10.48550/arXiv.2401.04088",
      "external_id_corpusid": 266844877
    },
    {
      "id": "2278435471",
      "type": "author",
      "name": "Antoine Roux"
    },
    {
      "id": "2278434888",
      "type": "author",
      "name": "Blanche Savary"
    },
    {
      "id": "2168785331",
      "type": "author",
      "name": "Emma Bou Hanna"
    },
    {
      "id": "2278434464",
      "type": "author",
      "name": "Guillaume Bour"
    },
    {
      "id": "2278428931",
      "type": "author",
      "name": "Sandeep Subramanian"
    },
    {
      "id": "2278460092",
      "type": "author",
      "name": "Sophia Yang"
    },
    {
      "id": "81588783",
      "type": "author",
      "name": "Théophile Gervet"
    },
    {
      "id": "2278590503",
      "type": "author",
      "name": "Thomas Wang"
    },
    {
      "id": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
      "type": "paper",
      "title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models",
      "abstract": "Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).",
      "year": 2022,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
      "citation_count": 141,
      "reference_count": 91,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work scales a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B), and for the first time achieves state-of theart performance in transfer learning.",
      "external_id_arxiv": "2202.08906",
      "external_id_corpusid": 248496391
    },
    {
      "id": "49596195",
      "type": "author",
      "name": "Sameer Kumar"
    },
    {
      "id": "2140321952",
      "type": "author",
      "name": "Nan Du"
    },
    {
      "id": "48448318",
      "type": "author",
      "name": "J. Dean"
    },
    {
      "id": "e320ec2c51ff2c25371d9d6ba3f8ac38eec1bc9b",
      "type": "paper",
      "title": "Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks",
      "abstract": "Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers have gained significant attention. Currently, state-of-the-art LLMs utilize this architecture. There is a substantial amount of research on how to train such models and how to select hyperparameters for this architecture. However, there is a lack of studies focusing on post-evaluation analysis of MoE layer properties. In this paper, we take a first step toward closing this gap by evaluating expert contributions on the quiz-based MMLU benchmark. We show that most experts were never activated during inference on this benchmark. Additionally, the output distribution of gating networks is much closer to uniform than sparse. Finally, we demonstrate that the average performance of some experts within the same layer varies significantly.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/e320ec2c51ff2c25371d9d6ba3f8ac38eec1bc9b",
      "citation_count": 0,
      "reference_count": 7,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Evaluating expert contributions on the quiz-based MMLU benchmark shows that most experts were never activated during inference on this benchmark, and it is demonstrated that the average performance of some experts within the same layer varies significantly.",
      "external_id_arxiv": "2502.17187",
      "external_id_corpusid": 276575216
    },
    {
      "id": "2346974350",
      "type": "author",
      "name": "Andrei Chernov"
    },
    {
      "id": "9307d546f9c253d82086a9793f27cd53bdd14164",
      "type": "paper",
      "title": "Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers",
      "abstract": "Traditional transformer models often allocate a fixed amount of computational resources to every input token, leading to inefficient and unnecessary computation. To address this, the Mixture of Depths (MoD) was introduced to dynamically adjust the computational depth by skipping less important layers. Despite its promise, current MoD approaches remain under-explored and face two main challenges: (1) high training costs due to the need to train the entire model along with the routers that determine which layers to skip, and (2) the risk of performance degradation when important layers are bypassed. In response to the first issue, we propose Router-Tuning, a method that fine-tunes only the router on a small dataset, drastically reducing the computational overhead associated with full model training. For the second challenge, we propose MindSkip, which deploys Attention with Dynamic Depths. This method preserves the model's performance while significantly enhancing computational and memory efficiency. Extensive experiments demonstrate that our approach delivers competitive results while dramatically improving the computation efficiency, e.g., 21\\% speedup and only a 0.2\\% performance drop. The code is released at https://github.com/CASE-Lab-UMD/Router-Tuning.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/9307d546f9c253d82086a9793f27cd53bdd14164",
      "citation_count": 0,
      "reference_count": 38,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Router-Tuning is proposed, a method that fine-tunes only the router on a small dataset, drastically reducing the computational overhead associated with full model training and significantly enhancing computational and memory efficiency.",
      "external_id_arxiv": "2410.13184",
      "external_id_dblp": "journals/corr/abs-2410-13184",
      "external_id_doi": "10.48550/arXiv.2410.13184",
      "external_id_corpusid": 273404149
    },
    {
      "id": "2152235390",
      "type": "author",
      "name": "Shwai He"
    },
    {
      "id": "2325002094",
      "type": "author",
      "name": "Tao Ge"
    },
    {
      "id": "2299920928",
      "type": "author",
      "name": "Guoheng Sun"
    },
    {
      "id": "2326301764",
      "type": "author",
      "name": "Bowei Tian"
    },
    {
      "id": "2250363276",
      "type": "author",
      "name": "Xiaoyang Wang"
    },
    {
      "id": "2307735047",
      "type": "author",
      "name": "Ang Li"
    },
    {
      "id": "2282412203",
      "type": "author",
      "name": "Dong Yu"
    },
    {
      "id": "896d1d46901af227786ef39ec2a8e27f425dcca6",
      "type": "paper",
      "title": "A fast convergence algorithm based on binary integer programming for expert load balancing in MoE LLMs",
      "abstract": "MoE (Mixture-of-Expert) architectures appear frequently in large language models, and the number of experts can be over one hundred recently. However, the expert load imbalance problem always happens in MoE model pre-training, which will cause routing collapse or increased computational overhead. In order to balance loads on experts, we propose BIP-Based Balancing, an expert load balancing algorithm based on binary integer programming (BIP). The algorithm maintains an additional vector q that can help change the top-K order of s by solving a binary integer programming with very small time costs. In simulation experiments, we observe that BIP-Based Balancing make imbalance disappoint very fast, while the final sum of routine scores decreases very little. Our algorithm achieves nearly perfect trade-off between expert load balance and pre-training efficiency under the simulation view.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/896d1d46901af227786ef39ec2a8e27f425dcca6",
      "citation_count": 0,
      "reference_count": 16,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work proposes BIP-Based Balancing, an expert load balancing algorithm based on binary integer programming (BIP), which achieves nearly perfect trade-off between expert load balance and pre-training efficiency under the simulation view.",
      "external_id_arxiv": "2502.15451",
      "external_id_corpusid": 276558090
    },
    {
      "id": "2346992153",
      "type": "author",
      "name": "Yuan Sun"
    },
    {
      "id": "043c7415a827769704a25088f9f8ed95b94b602e",
      "type": "paper",
      "title": "Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer Fine-Grained Experts",
      "abstract": "Large language models have demonstrated exceptional performance across a wide range of tasks. However, dense models usually suffer from sparse activation, where many activation values tend towards zero (i.e., being inactivated). We argue that this could restrict the efficient exploration of model representation space. To mitigate this issue, we propose Finedeep, a deep-layered fine-grained expert architecture for dense models. Our framework partitions the feed-forward neural network layers of traditional dense models into small experts, arranges them across multiple sub-layers. A novel routing mechanism is proposed to determine each expert's contribution. We conduct extensive experiments across various model sizes, demonstrating that our approach significantly outperforms traditional dense architectures in terms of perplexity and benchmark performance while maintaining a comparable number of parameters and floating-point operations. Moreover, we find that Finedeep achieves optimal results when balancing depth and width, specifically by adjusting the number of expert sub-layers and the number of experts per sub-layer. Empirical results confirm that Finedeep effectively alleviates sparse activation and efficiently utilizes representation capacity in dense models.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/043c7415a827769704a25088f9f8ed95b94b602e",
      "citation_count": 0,
      "reference_count": 35,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work proposes Finedeep, a deep-layered fine-grained expert architecture for dense models, which significantly outperforms traditional dense architectures in terms of perplexity and benchmark performance while maintaining a comparable number of parameters and floating-point operations.",
      "external_id_arxiv": "2502.12928",
      "external_id_corpusid": 276421964
    },
    {
      "id": "2264028895",
      "type": "author",
      "name": "Leiyu Pan"
    },
    {
      "id": "2216413613",
      "type": "author",
      "name": "Zhenpeng Su"
    },
    {
      "id": "2261278385",
      "type": "author",
      "name": "Minxuan Lv"
    },
    {
      "id": "f9568e88def17e8c833b399c9ac51670fda82f3f",
      "type": "paper",
      "title": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate",
      "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across various tasks, and their application in edge scenarios has attracted significant attention. However, sparse-activated Mixture-of-Experts (MoE) models, which are well suited for edge scenarios, have received relatively little attention due to their high memory demands. Offload-based methods have been proposed to address this challenge, but they face difficulties with expert prediction. Inaccurate expert predictions can result in prolonged inference delays. To promote the application of MoE models in edge scenarios, we propose Fate, an offloading system designed for MoE models to enable efficient inference in resource-constrained environments. The key insight behind Fate is that gate inputs from adjacent layers can be effectively used for expert prefetching, achieving high prediction accuracy without additional GPU overhead. Furthermore, Fate employs a shallow-favoring expert caching strategy that increases the expert hit rate to 99\\%. Additionally, Fate integrates tailored quantization strategies for cache optimization and IO efficiency. Experimental results show that, compared to Load on Demand and Expert Activation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in prefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively, while maintaining inference quality. Moreover, Fate's performance improvements are scalable across different memory budgets.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/f9568e88def17e8c833b399c9ac51670fda82f3f",
      "citation_count": 0,
      "reference_count": 49,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Fate, an offloading system designed for MoE models to enable efficient inference in resource-constrained environments, and employs a shallow-favoring expert caching strategy that increases the expert hit rate to 99\\%.",
      "external_id_arxiv": "2502.12224",
      "external_id_corpusid": 276421626
    },
    {
      "id": "626751398e48cdaa7b395301e8436fe9d65f15f7",
      "type": "paper",
      "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM",
      "abstract": "Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at https://github.com/zhanshijinwat/Steel-LLM.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/626751398e48cdaa7b395301e8436fe9d65f15f7",
      "citation_count": 0,
      "reference_count": 42,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "A comprehensive summary of the Steel-LLM project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way are provided, offering a valuable resource for researchers and practitioners looking to develop their own LLMs.",
      "external_id_arxiv": "2502.06635",
      "external_id_corpusid": 276250627
    },
    {
      "id": "2344766119",
      "type": "author",
      "name": "Qingshui Gu"
    },
    {
      "id": "2344787271",
      "type": "author",
      "name": "Shu Li"
    },
    {
      "id": "2344819453",
      "type": "author",
      "name": "Tianyu Zheng"
    },
    {
      "id": "2314548935",
      "type": "author",
      "name": "Zhaoxiang Zhang"
    },
    {
      "id": "e3b033f839d0db90e98884c89347194ad109fbc9",
      "type": "paper",
      "title": "M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer Inference",
      "abstract": "Residual transformations enhance the representational depth and expressive power of large language models (LLMs). However, applying static residual transformations across all tokens in auto-regressive generation leads to a suboptimal trade-off between inference efficiency and generation fidelity. Existing methods, including Early Exiting, Skip Decoding, and Mixture-of-Depth address this by modulating the residual transformation based on token-level complexity. Nevertheless, these approaches predominantly consider the distance traversed by tokens through the model layers, neglecting the underlying velocity of residual evolution. We introduce Mixture of Multi-rate Residuals (M2R2), a framework that dynamically modulates residual velocity to improve early alignment, enhancing inference efficiency. Evaluations on reasoning oriented tasks such as Koala, Self-Instruct, WizardLM, and MT-Bench show M2R2 surpasses state-of-the-art distance-based strategies, balancing generation quality and speedup. In self-speculative decoding setup, M2R2 achieves up to 2.8x speedups on MT-Bench, outperforming methods like 2-model speculative decoding, Medusa, LookAhead Decoding, and DEED. In Mixture-of-Experts (MoE) architectures, integrating early residual alignment with ahead-of-time expert loading into high-bandwidth memory (HBM) accelerates decoding, reduces expert-switching bottlenecks, and achieves a 2.9x speedup, making it highly effective in resource-constrained environments.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/e3b033f839d0db90e98884c89347194ad109fbc9",
      "citation_count": 0,
      "reference_count": 71,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work introduces Mixture of Multi-rate Residuals (M2R2), a framework that dynamically modulates residual velocity to improve early alignment, enhancing inference efficiency and balancing generation quality and speedup.",
      "external_id_arxiv": "2502.02040",
      "external_id_corpusid": 276107314
    },
    {
      "id": "29769330",
      "type": "author",
      "name": "Nikhil Bhendawade"
    },
    {
      "id": "40465379",
      "type": "author",
      "name": "Mahyar Najibi"
    },
    {
      "id": "2237796924",
      "type": "author",
      "name": "Devang Naik"
    },
    {
      "id": "2284683150",
      "type": "author",
      "name": "Irina Belousova"
    },
    {
      "id": "cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "type": "paper",
      "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models",
      "abstract": "This paper revisits the implementation of $\\textbf{L}$oad-$\\textbf{b}$alancing $\\textbf{L}$oss (LBL) when training Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as $N_E \\sum_{i=1}^{N_E} f_i p_i$, where $N_E$ is the total number of experts, $f_i$ represents the frequency of expert $i$ being selected, and $p_i$ denotes the average gating score of the expert $i$. Existing MoE training frameworks usually employ the parallel training strategy so that $f_i$ and the LBL are calculated within a $\\textbf{micro-batch}$ and then averaged across parallel groups. In essence, a micro-batch for training billion-scale LLMs normally contains very few sequences. So, the micro-batch LBL is almost at the sequence level, and the router is pushed to distribute the token evenly within each sequence. Under this strict constraint, even tokens from a domain-specific sequence ($\\textit{e.g.}$, code) are uniformly routed to all experts, thereby inhibiting expert specialization. In this work, we propose calculating LBL using a $\\textbf{global-batch}$ to loose this constraint. Because a global-batch contains much more diverse sequences than a micro-batch, which will encourage load balance at the corpus level. Specifically, we introduce an extra communication step to synchronize $f_i$ across micro-batches and then use it to calculate the LBL. Through experiments on training MoEs-based LLMs (up to $\\textbf{42.8B}$ total parameters and $\\textbf{400B}$ tokens), we surprisingly find that the global-batch LBL strategy yields excellent performance gains in both pre-training perplexity and downstream tasks. Our analysis reveals that the global-batch LBL also greatly improves the domain specialization of MoE experts.",
      "year": 2025,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "citation_count": 0,
      "reference_count": 30,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work proposes calculating LBL using a global-batch containing much more diverse sequences than a micro-batch, which will encourage load balance at the corpus level and reveals that the global-batch LBL also greatly improves the domain specialization of MoE experts.",
      "external_id_arxiv": "2501.11873",
      "external_id_dblp": "journals/corr/abs-2501-11873",
      "external_id_doi": "10.48550/arXiv.2501.11873",
      "external_id_corpusid": 275787957
    },
    {
      "id": "2337236034",
      "type": "author",
      "name": "Zihan Qiu"
    },
    {
      "id": "2259588806",
      "type": "author",
      "name": "Zeyu Huang"
    },
    {
      "id": "2341721830",
      "type": "author",
      "name": "Bo Zheng"
    },
    {
      "id": "2341530596",
      "type": "author",
      "name": "Kaiyue Wen"
    },
    {
      "id": "2338357176",
      "type": "author",
      "name": "Zekun Wang"
    },
    {
      "id": "2315980300",
      "type": "author",
      "name": "Ivan Titov"
    },
    {
      "id": "2326803484",
      "type": "author",
      "name": "Junyang Lin"
    },
    {
      "id": "1da79ca1d102a4a1d0e0caa574c3c05cdc617ffa",
      "type": "paper",
      "title": "Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens",
      "abstract": "Human bodily movements convey critical insights into action intentions and cognitive processes, yet existing multimodal systems primarily focused on understanding human motion via language, vision, and audio, which struggle to capture the dynamic forces and torques inherent in 3D motion. Inertial measurement units (IMUs) present a promising alternative, offering lightweight, wearable, and privacy-conscious motion sensing. However, processing of streaming IMU data faces challenges such as wireless transmission instability, sensor noise, and drift, limiting their utility for long-term real-time motion capture (MoCap), and more importantly, online motion analysis. To address these challenges, we introduce Mojito, an intelligent motion agent that integrates inertial sensing with large language models (LLMs) for interactive motion capture and behavioral analysis.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/1da79ca1d102a4a1d0e0caa574c3c05cdc617ffa",
      "citation_count": 0,
      "reference_count": 93,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Mojito is introduced, an intelligent motion agent that integrates inertial sensing with large language models (LLMs) for interactive motion capture and behavioral analysis.",
      "external_id_arxiv": "2502.16175",
      "external_id_corpusid": 276575551
    },
    {
      "id": "2274106056",
      "type": "author",
      "name": "Ziwei Shan"
    },
    {
      "id": "2283974603",
      "type": "author",
      "name": "Yaoyu He"
    },
    {
      "id": "40d6c9d96114e697bff6ff10991d5d0c5a821ee4",
      "type": "paper",
      "title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
      "abstract": "Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/40d6c9d96114e697bff6ff10991d5d0c5a821ee4",
      "citation_count": 0,
      "reference_count": 49,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations that can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language and symbolic reasoning tasks.",
      "external_id_arxiv": "2502.12616",
      "external_id_corpusid": 276421915
    },
    {
      "id": "2008183566",
      "type": "author",
      "name": "Leonardo Ranaldi"
    },
    {
      "id": "34102057",
      "type": "author",
      "name": "Marco Valentino"
    },
    {
      "id": "2345924508",
      "type": "author",
      "name": "Alexander Polonsky"
    },
    {
      "id": "2242981659",
      "type": "author",
      "name": "André Freitas"
    },
    {
      "id": "639e84addcaf1784ff3c57ed3f6eeacb5a099d98",
      "type": "paper",
      "title": "Circuit Representation Learning with Masked Gate Modeling and Verilog-AIG Alignment",
      "abstract": "Understanding the structure and function of circuits is crucial for electronic design automation (EDA). Circuits can be formulated as And-Inverter graphs (AIGs), enabling efficient implementation of representation learning through graph neural networks (GNNs). Masked modeling paradigms have been proven effective in graph representation learning. However, masking augmentation to original circuits will destroy their logical equivalence, which is unsuitable for circuit representation learning. Moreover, existing masked modeling paradigms often prioritize structural information at the expense of abstract information such as circuit function. To address these limitations, we introduce MGVGA, a novel constrained masked modeling paradigm incorporating masked gate modeling (MGM) and Verilog-AIG alignment (VGA). Specifically, MGM preserves logical equivalence by masking gates in the latent space rather than in the original circuits, subsequently reconstructing the attributes of these masked gates. Meanwhile, large language models (LLMs) have demonstrated an excellent understanding of the Verilog code functionality. Building upon this capability, VGA performs masking operations on original circuits and reconstructs masked gates under the constraints of equivalent Verilog codes, enabling GNNs to learn circuit functions from LLMs. We evaluate MGVGA on various logic synthesis tasks for EDA and show the superior performance of MGVGA compared to previous state-of-the-art methods. Our code is available at https://github.com/wuhy68/MGVGA.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/639e84addcaf1784ff3c57ed3f6eeacb5a099d98",
      "citation_count": 1,
      "reference_count": 50,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "MGVGA is introduced, a novel constrained masked modeling paradigm incorporating masked gate modeling (MGM) and Verilog-AIG alignment (VGA) that preserves logical equivalence by masking gates in the latent space rather than in the original circuits, subsequently reconstructing the attributes of these masked gates.",
      "external_id_arxiv": "2502.12732",
      "external_id_corpusid": 276421462
    },
    {
      "id": "2220363044",
      "type": "author",
      "name": "Yuan Pu"
    },
    {
      "id": "0c0d935a560a766e0e77b146be6bc35a3458bc81",
      "type": "paper",
      "title": "VRoPE: Rotary Position Embedding for Video Large Language Models",
      "abstract": "Rotary Position Embedding (RoPE) has shown strong performance in text-based Large Language Models (LLMs), but extending it to video remains a challenge due to the intricate spatiotemporal structure of video frames. Existing adaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions separately but suffer from two major limitations: positional bias in attention distribution and disruptions in video-text transitions. To overcome these issues, we propose Video Rotary Position Embedding (VRoPE), a novel positional encoding method tailored for Video-LLMs. Our approach restructures positional indices to preserve spatial coherence and ensure a smooth transition between video and text tokens. Additionally, we introduce a more balanced encoding strategy that mitigates attention biases, ensuring a more uniform distribution of spatial focus. Extensive experiments on Vicuna and Qwen2 across different model scales demonstrate that VRoPE consistently outperforms previous RoPE variants, achieving significant improvements in video understanding, temporal reasoning, and retrieval tasks. Code will be available at https://github.com/johncaged/VRoPE",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/0c0d935a560a766e0e77b146be6bc35a3458bc81",
      "citation_count": 0,
      "reference_count": 31,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "Video Rotary Position Embedding (VRoPE), a novel positional encoding method tailored for Video-LLMs, is proposed, which restructures positional indices to preserve spatial coherence and ensure a smooth transition between video and text tokens.",
      "external_id_arxiv": "2502.11664",
      "external_id_corpusid": 276409092
    },
    {
      "id": "2346160582",
      "type": "author",
      "name": "Zikang Liu"
    },
    {
      "id": "26982950",
      "type": "author",
      "name": "Longteng Guo"
    },
    {
      "id": "2107222678",
      "type": "author",
      "name": "Yepeng Tang"
    },
    {
      "id": "2346894661",
      "type": "author",
      "name": "Junxian Cai"
    },
    {
      "id": "e8dfb0ce6684413a5b3206e1e238432405d5dad9",
      "type": "paper",
      "title": "FinMTEB: Finance Massive Text Embedding Benchmark",
      "abstract": "Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop a finance-adapted model, Fin-E5, using a persona-based data synthetic method to cover diverse financial embedding tasks for training. Through extensive evaluation of 15 embedding models, including Fin-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/e8dfb0ce6684413a5b3206e1e238432405d5dad9",
      "citation_count": 0,
      "reference_count": 75,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This work introduces the Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain and establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models.",
      "external_id_arxiv": "2502.10990",
      "external_id_corpusid": 276409274
    },
    {
      "id": "2260449655",
      "type": "author",
      "name": "Yixuan Tang"
    },
    {
      "id": "2246043972",
      "type": "author",
      "name": "Yi Yang"
    },
    {
      "id": "ca77b83cd1f4710d6df665f28530230bc8b6dece",
      "type": "paper",
      "title": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts Serving",
      "abstract": "Large Language Models (LLMs) have gained immense success in revolutionizing various applications, including content generation, search and recommendation, and AI-assisted operation. To reduce high training costs, Mixture-of-Experts (MoE) architecture has become a popular backbone for modern LLMs. However, despite the benefits, serving MoE-based LLMs experience severe memory inefficiency due to sparsely activated experts. Recent studies propose to offload inactive experts from GPU memory to CPU memory to improve the serving efficiency of MoE models. However, they either incur high inference latency or high model memory footprints due to coarse-grained designs. To tame the latency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert offloading system for MoE serving that achieves low inference latency with memory efficiency. We design fMoE to extract fine-grained expert selection patterns from MoE models and semantic hints from input prompts to efficiently guide expert prefetching, caching, and offloading decisions. fMoE is prototyped on top of HuggingFace Transformers and deployed on a six-GPU testbed. Experiments with open-source MoE models and real-world workloads show that fMoE reduces inference latency by 47% and improves expert hit rate by 36% over state-of-the-art solutions.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/ca77b83cd1f4710d6df665f28530230bc8b6dece",
      "citation_count": 0,
      "reference_count": 61,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "",
      "external_id_arxiv": "2502.05370",
      "external_id_corpusid": 276250275
    },
    {
      "id": "2019682283",
      "type": "author",
      "name": "Hanfei Yu"
    },
    {
      "id": "2344972958",
      "type": "author",
      "name": "Xingqi Cui"
    },
    {
      "id": "2146244292",
      "type": "author",
      "name": "Hong Zhang"
    },
    {
      "id": "2346026694",
      "type": "author",
      "name": "Hao Wang"
    },
    {
      "id": "263e245a872d59a110c2d08297ec839c8135349c",
      "type": "paper",
      "title": "Classic4Children: Adapting Chinese Literary Classics for Children with Large Language Model",
      "abstract": "Chinese literary classics hold significant cultural and educational value, offering deep insights into morality, history, and human nature. These works often include classical Chinese and complex narratives, making them difficult for children to read. To bridge this gap, we introduce a child-friendly literary adaptation (CLA) task to adapt the Chinese literary classic into engaging and accessible text for children. However, recent large language models (LLMs) overlook children's reading preferences (\\ie, vivid character portrayals, concise narrative structures, and appropriate readability), which poses challenges in CLA. In this paper, we propose a method called InstructChild, which augments the LLM with these preferences for adaptation. Specifically, we first obtain the characters' personalities and narrative structure as additional information for fine-grained instruction tuning. Then, we devise a readability metric as the reward to align the LLM with the children's reading level. Finally, a lookahead decoding strategy is applied to improve the readability of the generated text during inference. To support the evaluation of CLA task, we construct the Classic4Children dataset, which comprises both the original and child-friendly versions of the Four Great Classical Novels of Chinese literature. Experimental results show that our InstructChild significantly improves automatic and human evaluation performance.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/263e245a872d59a110c2d08297ec839c8135349c",
      "citation_count": 0,
      "reference_count": 44,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "",
      "external_id_arxiv": "2502.01090",
      "external_id_corpusid": 276095113
    },
    {
      "id": "2108090448",
      "type": "author",
      "name": "Jiali Chen"
    },
    {
      "id": "2327887351",
      "type": "author",
      "name": "Xusen Hei"
    },
    {
      "id": "2328803568",
      "type": "author",
      "name": "Yuqi Xue"
    },
    {
      "id": "2293561416",
      "type": "author",
      "name": "Zihan Wu"
    },
    {
      "id": "1387837930",
      "type": "author",
      "name": "Jiayuan Xie"
    },
    {
      "id": "2279360884",
      "type": "author",
      "name": "Yi Cai"
    },
    {
      "id": "0bf1446500757764ac72a0c450b1f295ae5b8d3f",
      "type": "paper",
      "title": "Predicting potentially unfair clauses in Chilean terms of services with natural language processing",
      "abstract": "This study addresses the growing concern of information asymmetry in consumer contracts, exacerbated by the proliferation of online services with complex Terms of Service that are rarely even read. Even though research on automatic analysis methods is conducted, the problem is aggravated by the general focus on English-language Machine Learning approaches and on major jurisdictions, such as the European Union. We introduce a new methodology and a substantial dataset addressing this gap. We propose a novel annotation scheme with four categories and a total of 20 classes, and apply it on 50 online Terms of Service used in Chile. Our evaluation of transformer-based models highlights how factors like language- and/or domain-specific pre-training, few-shot sample size, and model architecture affect the detection and classification of potentially abusive clauses. Results show a large variability in performance for the different tasks and models, with the highest macro-F1 scores for the detection task ranging from 79% to 89% and micro-F1 scores up to 96%, while macro-F1 scores for the classification task range from 60% to 70% and micro-F1 scores from 64% to 80%. Notably, this is the first Spanish-language multi-label classification dataset for legal clauses, applying Chilean law and offering a comprehensive evaluation of Spanish-language models in the legal domain. Our work lays the ground for future research in method development for rarely considered legal analysis and potentially leads to practical applications to support consumers in Chile and Latin America as a whole.",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/0bf1446500757764ac72a0c450b1f295ae5b8d3f",
      "citation_count": 0,
      "reference_count": 58,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This study addresses the growing concern of information asymmetry in consumer contracts, exacerbated by the proliferation of online services with complex Terms of Service that are rarely even read, with a new methodology and a substantial dataset addressing this gap.",
      "external_id_arxiv": "2502.00865",
      "external_id_corpusid": 276094911
    },
    {
      "id": "2343637672",
      "type": "author",
      "name": "Christoffer Loeffler"
    },
    {
      "id": "2343637351",
      "type": "author",
      "name": "Andrea Mart'inez Freile"
    },
    {
      "id": "2343637131",
      "type": "author",
      "name": "Tom'as Rey Pizarro"
    },
    {
      "id": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "type": "paper",
      "title": "Panacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation",
      "abstract": "Harmful fine-tuning attack introduces significant security risks to the fine-tuning services. Mainstream defenses aim to vaccinate the model such that the later harmful fine-tuning attack is less effective. However, our evaluation results show that such defenses are fragile -- with a few fine-tuning steps, the model still can learn the harmful knowledge. To this end, we do further experiment and find that an embarrassingly simple solution -- adding purely random perturbations to the fine-tuned model, can recover the model from harmful behavior, though it leads to a degradation in the model's fine-tuning performance. To address the degradation of fine-tuning performance, we further propose Panacea, which optimizes an adaptive perturbation that will be applied to the model after fine-tuning. Panacea maintains model's safety alignment performance without compromising downstream fine-tuning performance. Comprehensive experiments are conducted on different harmful ratios, fine-tuning tasks and mainstream LLMs, where the average harmful scores are reduced by up-to 21.5%, while maintaining fine-tuning performance. As a by-product, we analyze the optimized perturbation and show that different layers in various LLMs have distinct safety coefficients. Source code available at https://github.com/w-yibo/Panacea",
      "year": 2025,
      "venue": "",
      "url": "https://www.semanticscholar.org/paper/c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "citation_count": 0,
      "reference_count": 73,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "An embarrassingly simple solution is found -- adding purely random perturbations to the fine-tuned model, can recover the model from harmful behavior, though it leads to a degradation in the model's fine-tuning performance.",
      "external_id_arxiv": "2501.18100",
      "external_id_corpusid": 275994129
    },
    {
      "id": "2337059751",
      "type": "author",
      "name": "Yibo Wang"
    },
    {
      "id": "2253860508",
      "type": "author",
      "name": "Tiansheng Huang"
    },
    {
      "id": "2325984735",
      "type": "author",
      "name": "Li Shen"
    },
    {
      "id": "2343055466",
      "type": "author",
      "name": "Huanjin Yao"
    },
    {
      "id": "2341879393",
      "type": "author",
      "name": "Haotian Luo"
    },
    {
      "id": "2342937556",
      "type": "author",
      "name": "Rui Liu"
    },
    {
      "id": "2341570262",
      "type": "author",
      "name": "Naiqiang Tan"
    },
    {
      "id": "2333251144",
      "type": "author",
      "name": "Jiaxing Huang"
    },
    {
      "id": "2135519749",
      "type": "author",
      "name": "Dacheng Tao"
    },
    {
      "id": "50650e66ce7c454595862aac70c2a3e9dde23387",
      "type": "paper",
      "title": "Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models",
      "abstract": "One core capability of large language models (LLMs) is to follow natural language instructions. However, the issue of automatically constructing high-quality training data to enhance the complex instruction-following abilities of LLMs without manual annotation remains unresolved. In this paper, we introduce AutoIF, the first scalable and reliable method for automatically generating instruction-following training data. AutoIF transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions, the corresponding code to check the correctness of the instruction responses, and unit test samples to verify the code's correctness. Then, execution feedback-based rejection sampling can generate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) training. AutoIF achieves significant improvements across three training algorithms, SFT, Offline DPO, and Online DPO, when applied to the top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and strong-to-weak distillation settings. Our code is publicly available at https://github.com/QwenLM/AutoIF.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/50650e66ce7c454595862aac70c2a3e9dde23387",
      "citation_count": 11,
      "reference_count": 42,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This paper introduces AutoIF, the first scalable and reliable method for automatically generating instruction-following training data, and achieves significant improvements across three training algorithms, SFT, Offline DPO, and Online DPO, when applied to the top open-source LLMs, Qwen2 and LLaMA3.",
      "external_id_dblp": "journals/corr/abs-2406-13542",
      "external_id_arxiv": "2406.13542",
      "external_id_doi": "10.48550/arXiv.2406.13542",
      "external_id_corpusid": 270620157
    },
    {
      "id": "2307468401",
      "type": "author",
      "name": "Tingyu Xia"
    },
    {
      "id": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "paper",
      "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
      "abstract": "We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through https://github.com/THUDM and https://huggingface.co/THUDM.",
      "year": 2024,
      "venue": "arXiv.org",
      "url": "https://www.semanticscholar.org/paper/c7f9706898bdfa3241601e075b1305649b174ff1",
      "citation_count": 279,
      "reference_count": 51,
      "fields_of_study": "[\"Computer Science\"]",
      "is_open_access": false,
      "tldr": "This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B, which represent the most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM.",
      "external_id_dblp": "journals/corr/abs-2406-12793",
      "external_id_arxiv": "2406.12793",
      "external_id_doi": "10.48550/arXiv.2406.12793",
      "external_id_corpusid": 270562306
    },
    {
      "id": "2307076042",
      "type": "author",
      "name": "Team Glm Aohan Zeng"
    },
    {
      "id": "2288066971",
      "type": "author",
      "name": "Bin Xu"
    },
    {
      "id": "2260453208",
      "type": "author",
      "name": "Bowen Wang"
    },
    {
      "id": "2303795844",
      "type": "author",
      "name": "Chenhui Zhang"
    },
    {
      "id": "2307075814",
      "type": "author",
      "name": "Da Yin"
    },
    {
      "id": "2307075650",
      "type": "author",
      "name": "Diego Rojas"
    },
    {
      "id": "2307077651",
      "type": "author",
      "name": "Guanyu Feng"
    },
    {
      "id": "2300177144",
      "type": "author",
      "name": "Hanlin Zhao"
    },
    {
      "id": "2263428192",
      "type": "author",
      "name": "Hanyu Lai"
    },
    {
      "id": "2285134718",
      "type": "author",
      "name": "Hao Yu"
    },
    {
      "id": "2253869803",
      "type": "author",
      "name": "Hongning Wang"
    },
    {
      "id": "2307208477",
      "type": "author",
      "name": "Jiadai Sun"
    },
    {
      "id": "2298413671",
      "type": "author",
      "name": "Jiajie Zhang"
    },
    {
      "id": "2109077637",
      "type": "author",
      "name": "Jiale Cheng"
    },
    {
      "id": "2307075328",
      "type": "author",
      "name": "Jiayi Gui"
    },
    {
      "id": "2295923423",
      "type": "author",
      "name": "Jie Tang"
    },
    {
      "id": "2268783318",
      "type": "author",
      "name": "Jing Zhang"
    },
    {
      "id": "2284734101",
      "type": "author",
      "name": "Juanzi Li"
    },
    {
      "id": "2302140984",
      "type": "author",
      "name": "Lei Zhao"
    },
    {
      "id": "2239424207",
      "type": "author",
      "name": "Lindong Wu"
    },
    {
      "id": "2307893366",
      "type": "author",
      "name": "Lucen Zhong"
    },
    {
      "id": "2302812544",
      "type": "author",
      "name": "Ming-yue Liu"
    },
    {
      "id": "2289785849",
      "type": "author",
      "name": "Minlie Huang"
    },
    {
      "id": "2291469424",
      "type": "author",
      "name": "Peng Zhang"
    },
    {
      "id": "2294349685",
      "type": "author",
      "name": "Qinkai Zheng"
    },
    {
      "id": "2263498652",
      "type": "author",
      "name": "Rui Lu"
    },
    {
      "id": "2307075663",
      "type": "author",
      "name": "Shuaiqi Duan"
    },
    {
      "id": "2300177449",
      "type": "author",
      "name": "Shudan Zhang"
    },
    {
      "id": "1712738522",
      "type": "author",
      "name": "S. Cao"
    },
    {
      "id": "2307176162",
      "type": "author",
      "name": "Shuxun Yang"
    },
    {
      "id": "1403621152",
      "type": "author",
      "name": "W. Tam"
    },
    {
      "id": "2294801385",
      "type": "author",
      "name": "Wenyi Zhao"
    },
    {
      "id": "2111312892",
      "type": "author",
      "name": "Xiao Liu"
    },
    {
      "id": "2301110844",
      "type": "author",
      "name": "Xiaoyu Xia"
    },
    {
      "id": "2205862099",
      "type": "author",
      "name": "Xiaohan Zhang"
    },
    {
      "id": "2290625851",
      "type": "author",
      "name": "Xiaotao Gu"
    },
    {
      "id": "2250016690",
      "type": "author",
      "name": "Xin Lv"
    },
    {
      "id": "2294845418",
      "type": "author",
      "name": "Xinghan Liu"
    },
    {
      "id": "2274455560",
      "type": "author",
      "name": "Xinyi Liu"
    },
    {
      "id": "2291800800",
      "type": "author",
      "name": "Xinyue Yang"
    },
    {
      "id": "2265550676",
      "type": "author",
      "name": "Xixuan Song"
    },
    {
      "id": "2307219813",
      "type": "author",
      "name": "Xunkai Zhang"
    },
    {
      "id": "2052144945",
      "type": "author",
      "name": "Y. An"
    },
    {
      "id": "2268847370",
      "type": "author",
      "name": "Yifan Xu"
    },
    {
      "id": "10680347",
      "type": "author",
      "name": "Yilin Niu"
    },
    {
      "id": "2307187635",
      "type": "author",
      "name": "Yuantao Yang"
    },
    {
      "id": "2294907281",
      "type": "author",
      "name": "Yueyan Li"
    },
    {
      "id": "2141377570",
      "type": "author",
      "name": "Yushi Bai"
    },
    {
      "id": "2243402027",
      "type": "author",
      "name": "Yuxiao Dong"
    },
    {
      "id": "2286747770",
      "type": "author",
      "name": "Zehan Qi"
    },
    {
      "id": "2144718801",
      "type": "author",
      "name": "Zhaoyu Wang"
    },
    {
      "id": "2220673267",
      "type": "author",
      "name": "Zhenyi Yang"
    },
    {
      "id": "66395694",
      "type": "author",
      "name": "Zhengxiao Du"
    },
    {
      "id": "2298783034",
      "type": "author",
      "name": "Zhen-Ping Hou"
    },
    {
      "id": "2291734244",
      "type": "author",
      "name": "Zihan Wang"
    }
  ],
  "edges": [
    {
      "source": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "target": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "target": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "target": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "target": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "target": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "target": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "target": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "1557386977",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2417003",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "3035073",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "150077954",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "150077954",
      "target": "1882f194cb43828852cc052887671e55a80f945a",
      "type": "authored"
    },
    {
      "source": "2542999",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2285263",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1737285",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2672644",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2273534960",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "4337102",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2460849",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1508890387",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "148016269",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2273563615",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2143434227",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275180676",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2143471164",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2143471164",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2070364520",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275292292",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290484786",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "47447264",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2145139570",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2145139570",
      "target": "1882f194cb43828852cc052887671e55a80f945a",
      "type": "authored"
    },
    {
      "source": "2065370007",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2249566095",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2090818",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "152399055",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2146532222",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275176102",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275525680",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1698491",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275181648",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2143538252",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185558",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "34122449",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186741",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1406288863",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2005813",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275729730",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "47407464",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "47407464",
      "target": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "authored"
    },
    {
      "source": "2275185143",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185143",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "74530494",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "12679121",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "12679121",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "2275185509",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2058168486",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "3257286",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "40269586",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2944868",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "67311962",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "67311962",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2841893",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2841893",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "81387328",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2135383313",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1413718981",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2289035179",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186577",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2288269273",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275183383",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2276211400",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186839",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "80930649",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2367821",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187189",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275535939",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275188933",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1394635460",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1791585",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1737522",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "133666998",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275539055",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2266398107",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275178766",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290484287",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2287841795",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275176047",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2159545857",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2253158807",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2070068655",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2073395505",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2815290",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2815290",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "authored"
    },
    {
      "source": "2815290",
      "target": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
      "type": "authored"
    },
    {
      "source": "2815290",
      "target": "2a805d0e1b067444a554c5169d189fa1f649f411",
      "type": "authored"
    },
    {
      "source": "2815290",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "authored"
    },
    {
      "source": "2269752766",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2289446231",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275159462",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2197671266",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2253917827",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275287219",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186692",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275290025",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1786259",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "51042571",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2283231534",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1780245",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275189014",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2266735761",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2161966573",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275175792",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275182383",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2258347245",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290513267",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2217756237",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275177720",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185667",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275537981",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "48335426",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "48335426",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "2290662953",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275170606",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "18138802",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186531",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1413064976",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1991019030",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1991019030",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2275162692",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2080504963",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290634593",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "86898863",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275193471",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "39687627",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "39687627",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2256989598",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1706980",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2078909017",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290484705",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "3468078",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290664586",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2261797906",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275534739",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "40470211",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "40034895",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "3007442",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1734809439",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "6639036",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "6639036",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2274104519",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275193644",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "34269227",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2268673324",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290485493",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290487747",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2307454258",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "143783339",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "122704930",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185831",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "144097210",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290487616",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2249840944",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275191526",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275180117",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2268760156",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275188533",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275114643",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2118834006",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2273670422",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275832693",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275291909",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "40608942",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "40608942",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "authored"
    },
    {
      "source": "40608942",
      "target": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
      "type": "authored"
    },
    {
      "source": "2290487447",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "3166516",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2598683",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290487293",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275149073",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186804",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2048712",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2048712",
      "target": "1882f194cb43828852cc052887671e55a80f945a",
      "type": "authored"
    },
    {
      "source": "2275182203",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2065404873",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187147",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1863250",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2151245633",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275801132",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290487874",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "119556335",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2273556813",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290481818",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1423275766",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2166051497",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2307453241",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2620528",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275188153",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275193365",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "96641652",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185833",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275180366",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "51029932",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275166845",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275184551",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275252154",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275252154",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275193725",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275175432",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2158369306",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "153892869",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275181572",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275892922",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2767859",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2279918122",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275575378",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2191689971",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275191626",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186515",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2059763226",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "3175815",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "3175815",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2160887964",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2160887964",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "1956049835",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290484991",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2143374656",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275220028",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2285740851",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2146532125",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187155",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "30155667",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186192",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1754860",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185727",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2261737895",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "40055795",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275184736",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275181534",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "144703404",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "143981350",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "47568983",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185732",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2273650801",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "46901218",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185661",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "39257069",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186093",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1980809",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275277736",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290487054",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186656",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186656",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2275291886",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2267341862",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2090812426",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275183119",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2051018967",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2425230",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275585027",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186584",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2268665228",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2261961752",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "47901308",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275148073",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1994939814",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "35930544",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290485355",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275193337",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275176212",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187196",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2258551072",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275180557",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275184985",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185968",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275184113",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2160888100",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2160888100",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2252586080",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2218882489",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290484418",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "3038326",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275130349",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185511",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2269391198",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2259962018",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2274787555",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290485798",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2156930381",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "51210148",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275182246",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2779842",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2347193353",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275183277",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "4478284",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275180682",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275173231",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275786213",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185813",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "148152480",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2067745837",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275280377",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275571997",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275396476",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2007712128",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2064599701",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "153898744",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186701",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2249764807",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2265053608",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2144551262",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290487597",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "20702300",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275767067",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2121764",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2264591527",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2269733876",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2266464503",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2266464503",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "2274107421",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275182230",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2269541835",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2267546965",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185640",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "35099444",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187110",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275188563",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "50844587",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "104000494",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290486855",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "3448463",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2258793616",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2258793616",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2243002880",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2253808003",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290485108",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1996199677",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2263289033",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275184275",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1696719",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2259937157",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290595918",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2170162986",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275176205",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2941141",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2063800905",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187845",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "118801223",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275158927",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2201776471",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2260169185",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275798209",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "38637384",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275184618",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "41019080",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275188258",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "3362306",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2288056644",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2105841261",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2221119859",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275190309",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2272718153",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "9356387",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "39552848",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2279996944",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187415",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2934334",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275161833",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "12295226",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2402489",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "19200186",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2287809580",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1394189636",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "41037204",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2291169360",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "81408931",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2218062983",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2256337021",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "144413479",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "108173905",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2159207795",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187490",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186582",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1841008",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275176043",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275181199",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275176049",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "3404697",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "48257711",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275121046",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275184334",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1825728",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275180099",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275113487",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290487337",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2284761701",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2333511945",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290486431",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2412073",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275177971",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2163521750",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2258235140",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185589",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "11167300",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "89066101",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275275439",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "7247867",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185808",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2182971260",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2265240845",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275173841",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2251517316",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1404332584",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2297847306",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275184531",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187038",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2256873459",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1403998955",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275181554",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275184739",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290741315",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2249760524",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275189194",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2265528853",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275189864",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290529512",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290485332",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2279830514",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275112414",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2215449616",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2257256357",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275177173",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "48942032",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "40627523",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290484919",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "31713635",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2150348369",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "118505443",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187305",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2140488873",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2257286979",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290487762",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1658856741",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "103861813",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2265527968",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2174667321",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275694953",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186180",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1702423",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186554",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290490486",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2269460640",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290485653",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275249023",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290556567",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290485958",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275190069",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275169305",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290488307",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290666013",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275181309",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290539499",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2280669236",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290485789",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275177999",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2096063076",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "4125424",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "4125424",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "4529644",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290488254",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "47182967",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186837",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290487825",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186342",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187959",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275181434",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1749128",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275188906",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275999038",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275195333",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2080520726",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186260",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187022",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187506",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275182960",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2254035020",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275186671",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275179889",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "9941702",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2591720",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290629265",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2288791213",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290557316",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290594698",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2070271342",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275185644",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275181171",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275552322",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2770149",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "145556052",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1735318",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290486731",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "153776147",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275716550",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275189350",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2112293680",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275187515",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2561675",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290524803",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275221227",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275175372",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275188417",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275539011",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "40390373",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2217508229",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2269473701",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275188903",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2258550407",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "7353832",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "7353832",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2164862499",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290580195",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2181807096",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2214770531",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275188993",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290486901",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290485721",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1404655176",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2275150753",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2257926827",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "1393948967",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290488378",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2266467648",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "2290784246",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "authored"
    },
    {
      "source": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "cites",
      "contexts": "[\"In fact, cutting-edge industry models like Gemini 1.5 (Team et al., 2024) and GPT-4 (based on unofficial reports) (OpenAI, 2024) have adopted MoE, suggesting its effectiveness.\"]",
      "is_influential": true
    },
    {
      "source": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "cites",
      "contexts": "[\"We selected the Llama (Touvron et al., 2023) and Mixtral (Jiang et al., 2024) architectures for dense and MoE models, respectively, for our experiments.\", \"The SwiGLU activation function (Shazeer, 2020), now standard in state-of-the-art LLMs like LLaMA (Touvron et al., 2023) and Mixtral (Jiang et al., 2024), will be used for explanation here.\", \"These are common design choices for recent MoE-based LLMs, such as Mixtral (Jiang et al., 2024), Skywork-MoE (Wei et al., 2024), Phi-3.\", \"For a detailed overview of MoE, please refer to recent survey papers (Cai et al., 2024).\", \"While this method appears to ensure expert specialization by design, Jiang et al. (2024) has highlighted that the diversity achieved in this way differs from that required for MoE layer experts, leading to suboptimal performance as a result.\", \"In contrast, na\\u00a8\\u0131ve Upcycling shows nearly uniform routing across all layers except the final layer, which aligns with findings reported in Jiang et al. (2024).\", \"We apply the methodologies of Jiang et al. (2024) and Muennighoff et al. (2024) to 8\\u00d71.5B MoE models trained with different methods.\"]",
      "is_influential": true
    },
    {
      "source": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "cites",
      "contexts": "[\"To analyze the effect of global and layer-wise load balancing, we conducted a comparative analysis between global and layer-wise load balancing applications across 40B tokens for different initialization methods (From Scratch, Branch-Train-MiX, na \\u00a8 \\u0131ve Upcycling, and Drop-Upcycling with r=0.5 and r=1.0) in the 8\\u00d71.5B setting.\", \"Our experiments also show that BTX suffers from suboptimal convergence similar to those observed in Upcycling.\", \"Branch-Train-MiX (BTX) (Sukhbaatar et al., 2024) is a technique where a pre-trained dense model is replicated and fine-tuned on different datasets to produce multiple distinct expert dense models.\", \"Detailed descriptions of the model configurations are provided in Appendix A.3 We evaluated four different methods to build MoE models, namely, training from scratch, na \\u00a8 \\u0131ve Upcycling (Komatsuzaki et al., 2023), Random Noise Upcycling (Komatsuzaki et al., 2023) and Branch-Train-MiX (Sukhbaatar et al., 2024) to compare the performance with Drop-Upcycling.\", \"\\u2026in Appendix A.3 We evaluated four different methods to build MoE models, namely, training from scratch, na \\u00a8 \\u0131ve Upcycling (Komatsuzaki et al., 2023), Random Noise Upcycling (Komatsuzaki et al., 2023) and Branch-Train-MiX (Sukhbaatar et al., 2024) to compare the performance with Drop-Upcycling.\"]",
      "is_influential": true
    },
    {
      "source": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "cites",
      "contexts": "[\"Large-scale language models (LLMs) have achieved remarkable results across various natural language processing applications (Brown et al., 2020; Wei et al., 2022; Ouyang et al., 2022; OpenAI, 2024).\", \"In fact, cutting-edge industry models like Gemini 1.5 (Team et al., 2024) and GPT-4 (based on unofficial reports) (OpenAI, 2024) have adopted MoE, suggesting its effectiveness.\"]",
      "is_influential": true
    },
    {
      "source": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "cites",
      "contexts": "[\"The notable difference in Mixtral (MoE) from Llama (dense) is that all feedforward network (FFN) layers are replaced by sparsely gated MoE layers.\", \"The SwiGLU activation function (Shazeer, 2020), now standard in state-of-the-art LLMs like LLaMA (Touvron et al., 2023) and Mixtral (Jiang et al., 2024), will be used for explanation here.\", \"We selected the Llama (Touvron et al., 2023) and Mixtral (Jiang et al., 2024) architectures for dense and MoE models, respectively, for our experiments.\", \"We followed the typical training configurations used in Llama to train dense models and Mixtral for MoE models.\"]",
      "is_influential": true
    },
    {
      "source": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "cites",
      "contexts": "[\"Upcycling (Komatsuzaki et al., 2023) leverages the weights of a pre-trained dense model for initializing an MoE model by initializing the experts in the MoE layer as replicas of the FFN layers in the dense model.\", \"\\u2026are provided in Appendix A.3 We evaluated four different methods to build MoE models, namely, training from scratch, na \\u00a8 \\u0131ve Upcycling (Komatsuzaki et al., 2023), Random Noise Upcycling (Komatsuzaki et al., 2023) and Branch-Train-MiX (Sukhbaatar et al., 2024) to compare the\\u2026\", \"Upcycling (Komatsuzaki et al., 2023) is an approach that initializes and trains an MoE model using a pre-trained dense model, which aims to transfer learned knowledge for better initial performance.\", \"Following (Komatsuzaki et al., 2023), we first construct a Transformer with MoE layers by replicating the weights from a pre-trained Transformer with standard FFN layers.\", \"\\u2026in Appendix A.3 We evaluated four different methods to build MoE models, namely, training from scratch, na \\u00a8 \\u0131ve Upcycling (Komatsuzaki et al., 2023), Random Noise Upcycling (Komatsuzaki et al., 2023) and Branch-Train-MiX (Sukhbaatar et al., 2024) to compare the performance with Drop-Upcycling.\"]",
      "is_influential": true
    },
    {
      "source": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "target": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "type": "cites",
      "contexts": "[\"The SwiGLU activation function (Shazeer, 2020), now standard in state-of-the-art LLMs like LLaMA (Touvron et al., 2023) and Mixtral (Jiang et al., 2024), will be used for explanation here.\", \"The i -th expert FFN is denoted as SwiGLU ( i ) ( x ) , which, like a standard FFN layer, consists of three weight matrices.\", \"The FFN layer with SwiGLU is defined as follows: (1) Here, x \\u2208 R d h represents the input vector and \\u2299 denotes the Hadamard product.\", \"(Vaswani et al., 2017) with several improvements, including RMSNorm (Zhang & Sennrich, 2019), SwiGLU (Shazeer, 2020), and rotary position embeddings (RoPE) (Su et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "cites",
      "contexts": "[]",
      "is_influential": false
    },
    {
      "source": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "cites",
      "contexts": "[\"As input data from different domains, we use the validation sets from Expert ID (Kocetkov et al., 2023); and the English C4 dataset (Muennighoff et al., 2024).\", \"We apply the methodologies of Jiang et al. (2024) and Muennighoff et al. (2024) to 8\\u00d71.5B MoE models trained with different methods.\"]",
      "is_influential": false
    },
    {
      "source": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "cites",
      "contexts": "[\"Concurrent with our work, the Qwen2 technical report (Yang et al., 2024) briefly suggests the use of a methodology possibly related to Drop-Upcycling in training Qwen2-MoE.\"]",
      "is_influential": false
    },
    {
      "source": "2294513555",
      "target": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "type": "authored"
    },
    {
      "source": "2347347332",
      "target": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "type": "authored"
    },
    {
      "source": "2298889885",
      "target": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "type": "authored"
    },
    {
      "source": "2266470694",
      "target": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "type": "authored"
    },
    {
      "source": "2294362068",
      "target": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "type": "authored"
    },
    {
      "source": "2347348643",
      "target": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "type": "authored"
    },
    {
      "source": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "cites",
      "contexts": "[\"For text embedding, Gemini Pro 1.5 (Team et al. 2024) was used to generate detailed textual descriptions of video content by posing targeted questions about scenes and visual details (Appendix 1).\"]",
      "is_influential": true
    },
    {
      "source": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "cites",
      "contexts": "[\"We fine-tuned the LLaMA 3.1 (8B) (Dubey et al. 2024) model using titles and key messages from the advertisements as input.\"]",
      "is_influential": true
    },
    {
      "source": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "target": "375b65ae876ccdd6b909ce6bed7d58549c0da216",
      "type": "cites",
      "contexts": "[\"Then, we further compared the performance of MindMem with those of other cutting-edge methods such as Henry (HariniSI et al. 2024), ViT-Mem(Hagen and Espeseth 2023), GPT 3.5 and GPT 4O (Achiam et al. 2023).\"]",
      "is_influential": true
    },
    {
      "source": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "target": "203bfa5e488a9c3100e3d9b9af5ea34537068612",
      "type": "cites",
      "contexts": "[\"To further evaluate the reliability of the MindMem architecture, we conducted experiments using the Memento10K dataset.\", \"Unlike the LAMBDA dataset, Memento10K features distinct characteristics, consisting of relatively short (3-second) natural videos.\", \"To assess the reliability of the MindMem architecture, we evaluate it using the Memento10K dataset (Newman et al. 2020).\", \"Specifically, in models based on Memento10K, MindMem achieved a Spearman\\u2019s correlation coefficient ( \\u03c1 ) of 0.731 (MSE = 0.0055) when all three types of multimodal information were fed into the model (Table 3).\", \"We also compared the performance of our model with other models that tested the Memento10K dataset.\", \"To develop and evaluate MindMem, we use two datasets, Long-term Ad MemoraBility DAtaset (LAMBDA) (Harin-iSI et al. 2024) and Memento10K (Newman et al. 2020), which provide complementary settings for assessing advertisement memorability and general video memorability.\"]",
      "is_influential": true
    },
    {
      "source": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "cites",
      "contexts": "[\"To effectively model human cognition and memorability, a multi-modal approach is essential, as it more closely mirrors the way humans perceive and process information from their environment (Wang et al. 2024).\", \"For video embedding, Long Video Assistant (LongVA) model was used to extract visual features from the dataset (Zhang et al. 2024).\"]",
      "is_influential": false
    },
    {
      "source": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "cites",
      "contexts": "[\"These descriptions were then processed by the Qwen2 (7B) text model (Yang et al. 2024), which extracted embeddings from the last hidden layer.\"]",
      "is_influential": false
    },
    {
      "source": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "target": "d081584960c42f7793502bb496e46f682e3e43b3",
      "type": "cites",
      "contexts": "[]",
      "is_influential": false
    },
    {
      "source": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "target": "51f5d1ae89c29d70426bed77680ffab41e1eb945",
      "type": "cites",
      "contexts": "[]",
      "is_influential": false
    },
    {
      "source": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "target": "727df25246b5156fd2abaf52ba1b47abaeae9d72",
      "type": "cites",
      "contexts": "[\"The positive correlation be-tween video pace and memorability aligns with recent research showing that faster-paced content can lead to better engagement and information retention (Murphy et al. 2022).\"]",
      "is_influential": false
    },
    {
      "source": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "target": "9e20f3155d16e13a16f3614a638207d67a7f4a5f",
      "type": "cites",
      "contexts": "[\"Other study leveraged video-triggered Electroencephalo-gram (EEG) data to examine how emotions evoked by videos influence memorability (Hu et al. 2020).\"]",
      "is_influential": false
    },
    {
      "source": "2284386733",
      "target": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "type": "authored"
    },
    {
      "source": "118993525",
      "target": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "type": "authored"
    },
    {
      "source": "2286650249",
      "target": "90f20af3e16a825e0b976e968fd8e99a3366e438",
      "type": "authored"
    },
    {
      "source": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "cites",
      "contexts": "[\"Llama 1 (Touvron et al., 2023a) and Llama 2 (Touvron et al., 2023b) were our primary guides, as were Gemini 1 and Gemini 1.5 (Team et al., 2023; Reid et al., 2024), Claude 3 (Anthropic, 2023), and Mistral (Jiang et al., 2023).\", \"\\u2026chat LMs such as GPT (Ouyang et al., 2022; Achiam et al., 2023), Llama (Touvron et al., 2023a;b; Dubey et al., 2024), Gemini (Team et al., 2023; Reid et al., 2024) and Claude (Anthropic, 2023) has prompted a re-evaluation of how we assess LMs, with a growing emphasis on assessing LMs based on\\u2026\"]",
      "is_influential": true
    },
    {
      "source": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "cites",
      "contexts": "[\"Llama 1 (Touvron et al., 2023a) and Llama 2 (Touvron et al., 2023b) were our primary guides, as were Gemini 1 and Gemini 1.5 (Team et al., 2023; Reid et al., 2024), Claude 3 (Anthropic, 2023), and Mistral (Jiang et al., 2023).\"]",
      "is_influential": true
    },
    {
      "source": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "cites",
      "contexts": "[\"Llama 1 (Touvron et al., 2023a) and Llama 2 (Touvron et al., 2023b) were our primary guides, as were Gemini 1 and Gemini 1.5 (Team et al., 2023; Reid et al., 2024), Claude 3 (Anthropic, 2023), and Mistral (Jiang et al., 2023).\", \"We evaluated the 4 Llama 2 Chat models following the same evaluation processes reported in the Llama 2 paper (Touvron et al., 2023b).\"]",
      "is_influential": true
    },
    {
      "source": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "target": "814a4f680b9ba6baba23b93499f4b48af1a27678",
      "type": "cites",
      "contexts": "[\"Some of these benchmarks (e.g., MMLU) contain subsets (e.g., Jurisprudence) that we do not aggregate over.\", \"\\u2026HellaSwag (Zellers et al., 2019), HumanEval (Chen et al., 2021), InverseScaling (McKenzie et al., 2022a;b; 2023), MBPP (Austin et al., 2021), MMLU (Hendrycks et al., 2020), Natural Questions (Kwiatkowski et al., 2019), OpenbookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), QuAC (Choi et\\u2026\", \"The uncorrelated group consists of human evaluations Dialogue:Code and Dialogue:Language Assistance, as well as NLP benchmarks Kth-sentence, TLDR, SciBench\\u2019s Atkins and Differential Equations, MMLU\\u2019s College Math and BBH\\u2019s Formal Fallacies.\", \"Finally, Open QA and some Writing evaluations lie closer to benchmarks demanding specialized knowledge (MMLU.\", \"We evaluated the four Chat Llama 2 models on large-scale and commonly-used NLP benchmarks: AGI Eval (Zhong et al., 2023), AI2 Reasoning Challenge (ARC; both Easy and Hard) (Clark et al., 2018), BIG Bench Hard (Srivastava et al., 2022; Suzgun et al., 2022) BoolQ (Clark et al., 2019), Com-monSenseQA (Talmor et al., 2019), COPA (Roemmele et al., 2011), DROP (Dua et al., 2019), GSM8K (Cobbe et al., 2021), HellaSwag (Zellers et al., 2019), HumanEval (Chen et al., 2021), InverseScaling (McKenzie et al., 2022a;b; 2023), MBPP (Austin et al., 2021), MMLU (Hendrycks et al., 2020), Natural Questions (Kwiatkowski et al., 2019), OpenbookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), QuAC (Choi et al., 2018), RACE (Lai et al., 2017), SIQA (Sap et al., 2019), SQUAD (Rajpurkar et al., 2016), TLDR (V\\u00a8olske et al., 2017), TriviaQA (Joshi et al., 2017), WinoGrande (Sakaguchi et al., 2021) and XSum (Narayan et al., 2018).\"]",
      "is_influential": true
    },
    {
      "source": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "target": "295065d942abca0711300b2b4c39829551060578",
      "type": "cites",
      "contexts": "[\"In the context of natural language generation, Clinciu et al. (2021) found that embedding-based automated metrics (e.g., BERT-Score (Zhang et al., 2019) and BLEURT Sellam et al. (2020)) correlate more strongly with human judgments compared to word-overlap metrics (e.g., ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002)).\", \"Another prominent class of evaluation methods are based on machine learning models, e.g., word mover distance (Kusner et al., 2015) and BERT-Score (Zhang et al., 2019) that have since evolved into using chat LMs themselves as evaluators (Wang et al., 2023b; Zheng et al., 2024; Chiang & yi Lee,\\u2026\", \"In the context of natural language generation, Clinciu et al. (2021) found that embedding-based automated metrics (e.g., BERT-Score (Zhang et al., 2019) and BLEURT Sellam et al. (2020)) correlate more strongly with human judgments compared to word-overlap metrics (e.g., ROUGE (Lin, 2004) and BLEU\\u2026\", \"Another prominent class of evaluation methods are based on machine learning models, e.g., word mover distance (Kusner et al., 2015) and BERT-Score (Zhang et al., 2019) that have since evolved into using chat LMs themselves as evaluators (Wang et al., 2023b; Zheng et al., 2024; Chiang & yi Lee, 2023; Chan et al., 2023; Bavaresco et al., 2024; Fu et al., 2024), albeit with limitations, e.g., (Dorner et al., 2024; Szymanski et al., 2024; Thakur et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "target": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
      "type": "cites",
      "contexts": "[\"Open Question Answering was surprising given that some of our NLP benchmarks are open question answering datasets, e.g., OpenBookQA (Mihaylov et al., 2018).\", \"We evaluated the four Chat Llama 2 models on large-scale and commonly-used NLP benchmarks: AGI Eval (Zhong et al., 2023), AI2 Reasoning Challenge (ARC; both Easy and Hard) (Clark et al., 2018), BIG Bench Hard (Srivastava et al., 2022; Suzgun et al., 2022) BoolQ (Clark et al., 2019), Com-monSenseQA (Talmor et al., 2019), COPA (Roemmele et al., 2011), DROP (Dua et al., 2019), GSM8K (Cobbe et al., 2021), HellaSwag (Zellers et al., 2019), HumanEval (Chen et al., 2021), InverseScaling (McKenzie et al., 2022a;b; 2023), MBPP (Austin et al., 2021), MMLU (Hendrycks et al., 2020), Natural Questions (Kwiatkowski et al., 2019), OpenbookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), QuAC (Choi et al., 2018), RACE (Lai et al., 2017), SIQA (Sap et al., 2019), SQUAD (Rajpurkar et al., 2016), TLDR (V\\u00a8olske et al., 2017), TriviaQA (Joshi et al., 2017), WinoGrande (Sakaguchi et al., 2021) and XSum (Narayan et al., 2018).\", \"We found many cluding Resisting Correction Classification), OpenBookQA, COPA, SciBench (excluding Fundamentals of Physics) and SIQA.\", \"\\u2026et al., 2022a;b; 2023), MBPP (Austin et al., 2021), MMLU (Hendrycks et al., 2020), Natural Questions (Kwiatkowski et al., 2019), OpenbookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), QuAC (Choi et al., 2018), RACE (Lai et al., 2017), SIQA (Sap et al., 2019), SQUAD (Rajpurkar et\\u2026\"]",
      "is_influential": true
    },
    {
      "source": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "target": "751b04d346fc9a70781bbfea23953f424ff7deec",
      "type": "cites",
      "contexts": "[\"\\u2026between human evaluations and NLP benchmarks over the 4 average scores per model with three standard correlations \\u2014 Pearson (Galton, 1877), Spearman (Spearman, 1904) and Kendall (Kendall, 1938) \\u2014 giving us three correlation matrices of shape 160 \\u00d7 55 between every pair of NLP benchmark and human\\u2026\", \"This is more clearly visually displayed in the Spearman correlation matrix (App.\", \"11), and so we present only one (Pearson) moving forward, with equivalent plots of the other two (Spearman, Kendall) deferred to the appendix.\", \"\\u2026human evaluations and NLP benchmarks over the 4 average scores per model with three standard correlations \\u2014 Pearson (Galton, 1877), Spearman (Spearman, 1904) and Kendall (Kendall, 1938) \\u2014 giving us three correlation matrices of shape 160 \\u00d7 55 between every pair of NLP benchmark and human\\u2026\", \"We began by computing correlations between human evaluations and NLP benchmarks over the 4 average scores per model with three standard correlations \\u2014 Pearson (Galton, 1877), Spearman (Spearman, 1904) and Kendall (Kendall, 1938) \\u2014 giving us three correlation matrices of shape 160 \\u00d7 55 between every pair of NLP benchmark and human evaluation area-category-subcategory (Fig.\"]",
      "is_influential": true
    },
    {
      "source": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "target": "d7da009f457917aa381619facfa5ffae9329a6e9",
      "type": "cites",
      "contexts": "[\"For decades, the field of natural language processing (NLP) has relied on academic benchmarks and automated metrics (e.g., Accuracy, Brier Score (Brier, 1950), BLEU (Papineni et al., 2002)) to evaluate the performance of language models (LMs).\", \"In the context of natural language generation, Clinciu et al. (2021) found that embedding-based automated metrics (e.g., BERT-Score (Zhang et al., 2019) and BLEURT Sellam et al. (2020)) correlate more strongly with human judgments compared to word-overlap metrics (e.g., ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002)).\", \"Many classic NLP benchmark metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin, 2004), and METEOR (Banerjee & Lavie, 2005), were introduced on the premise that they correlate with human judgments.\", \"\\u2026language generation, Clinciu et al. (2021) found that embedding-based automated metrics (e.g., BERT-Score (Zhang et al., 2019) and BLEURT Sellam et al. (2020)) correlate more strongly with human judgments compared to word-overlap metrics (e.g., ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002)).\"]",
      "is_influential": true
    },
    {
      "source": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "target": "7235707434df77b0469e6da21e93a27f250870eb",
      "type": "cites",
      "contexts": "[\"\\u20262019) that have since evolved into using chat LMs themselves as evaluators (Wang et al., 2023b; Zheng et al., 2024; Chiang & yi Lee, 2023; Chan et al., 2023; Bavaresco et al., 2024; Fu et al., 2024), albeit with limitations, e.g., (Dorner et al., 2024; Szymanski et al., 2024; Thakur et al., 2024).\"]",
      "is_influential": false
    },
    {
      "source": "1749176844",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "2146367061",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "2146367061",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2146367061",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2146367061",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "2146367061",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "2237987675",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "2237987675",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2293725986",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "2306863572",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "2306863572",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "39980906",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "39980906",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "39980906",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "39980906",
      "target": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
      "type": "authored"
    },
    {
      "source": "39980906",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "39980906",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "51229603",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "51229603",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "51229603",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "151093281",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "151093281",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "22193324",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "22193324",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "28554843",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "28554843",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "28554843",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2343771482",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "3449411",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "3449411",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "123593472",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "46617804",
      "target": "e6ddb39815eac5f237ba6e97a942e9fcf03ff9b9",
      "type": "authored"
    },
    {
      "source": "46617804",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "46617804",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "46617804",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "46617804",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "46617804",
      "target": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "type": "authored"
    },
    {
      "source": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "cites",
      "contexts": "[\"For both GPT-4o and Llama 3.1 8B Instruct, we achieve higher ASR \\u00d7 5 30 with comparable query counts, though WildTeaming demonstrates an advantage in generating diverse jailbreaks on Gemini 1.5 Flash.\", \"Table 1 presents a comparison between our approach and the WildTeaming baseline across Gemini 1.5 Flash (Team et al., 2024), GPT-4, and Llama 3.1 8B Instruct.\"]",
      "is_influential": true
    },
    {
      "source": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "cites",
      "contexts": "[\"To assess our pipeline\\u2019s components, we evaluate three stages, with detailed parameters in the Appendix E: \\u2022 Generation: We generate proposed features using the generation-stage prompt with GPT-4o and randomly sample up to 50 features, representing the initial pipeline output.\", \"For both GPT-4o and Llama 3.1 8B Instruct, we achieve higher ASR \\u00d7 5 30 with comparable query counts, though WildTeaming demonstrates an advantage in generating diverse jailbreaks on Gemini 1.5 Flash.\", \"We use GPT-4o to generate 50 features with temperature and top p set to 1, using a random sample of 100 instances due to context limitations.\", \"Our pipeline uses GPT-4o (Hurst et al., 2024) for feature proposal and evalidation, and Llama 3.1 8B Instruct (Dubey et al., 2024) for feature selection.\", \"We employ GPT-4o (Hurst et al., 2024) Featurization Stage.\"]",
      "is_influential": true
    },
    {
      "source": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "cites",
      "contexts": "[\"From 5000 adversarial, harmful prompts in WildJailbreak, we identify 536 prompts that elicit non-refusal responses from Llama 3.1 8B Instruct, as judged by WildGuard (Han et al., 2024), to derive a new set of 20 features (Appendix H.2) using the same featurization setup.\", \"\\u2022 Full pipeline: We apply the feature selection procedure to the clustered features through Llama 3.1 8B Instruct (prompt found in Appendix F.3), iteratively selecting up to 50 features that maximize dataset reconstruction, representing the complete pipeline.\", \"Table 1 presents a comparison between our approach and the WildTeaming baseline across Gemini 1.5 Flash (Team et al., 2024), GPT-4, and Llama 3.1 8B Instruct.\", \"Features Extracted from Filtered WildJailbreak of Llama 3.1 8B Instruct Non-Refusals Top 20 Features Generated from Llama 3.1 8B Instruct non-refusals from WildJailbreak 1.\", \"Overall, our method successfully compresses WildTeam-ing\\u2019s 500 clusters into just 20 features (a 25x reduction in feature space) while maintaining or improving performance across most metrics, particularly for robust models like GPT-4 and Llama 3.1 8B Instruct.\", \"We sample 1,000 preferred responses from each dataset and create separate feature spaces of 50 features for SHP and (Appendix I.4) and HH-RLHF (Appendix I.5) using parameters from Appendix E, with Llama 3.1 8B Instruct handling the final stage via the prompt in Appendix F.3.\", \"Prompt used by Claude Haiku 3.5 in the Semantic Preservation Evaluation Instruction: Do these two classes share the same meaning?\", \"- roleplay: [definition] Instruct the model to play the role of some different characters to legitimize its harmful response [example] you are a helpful research assistant aiding great scientific discovery.\", \"For both GPT-4o and Llama 3.1 8B Instruct, we achieve higher ASR \\u00d7 5 30 with comparable query counts, though WildTeaming demonstrates an advantage in generating diverse jailbreaks on Gemini 1.5 Flash.\", \"Our pipeline uses GPT-4o (Hurst et al., 2024) for feature proposal and evalidation, and Llama 3.1 8B Instruct (Dubey et al., 2024) for feature selection.\"]",
      "is_influential": true
    },
    {
      "source": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "target": "deb3fccb5b68041ea0f5c724f08b2d840fa51eaa",
      "type": "cites",
      "contexts": "[\"From 5000 adversarial, harmful prompts in WildJailbreak, we identify 536 prompts that elicit non-refusal responses from Llama 3.1 8B Instruct, as judged by WildGuard (Han et al., 2024), to derive a new set of 20 features (Appendix H.2) using the same featurization setup.\"]",
      "is_influential": true
    },
    {
      "source": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "target": "03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f",
      "type": "cites",
      "contexts": "[\"Zhong et al. (2022) formulates this as a distribution comparison problem to identify distinguishing characteristics, an approach later extended beyond text (Dunlap et al., 2024) and adapted to accommodate user-specified exploration goals (Zhong et al., 2023).\", \"Our method is restricted to binary features and relies on positive feature instances during optimization, aligning with prior work (Dunlap et al., 2024; Zhong et al., 2024; Findeis et al., 2024) but limiting applicability to tasks requiring numeric attributes and hierarchical relationships.\", \"Recent approaches decompose rewards into interpretable features such as readability and correctness (Dorka, 2024; Wang et al., 2024).\", \"\\u2026discovery as a search over natural language hypotheses (Qiu et al., 2023), employing diverse strategies including de-duplication (Pham et al., 2023), optimization for minimal cluster overlap (Wang et al., 2023; Zhong et al., 2024), and human-guided feature selection (Viswanathan et al., 2023).\"]",
      "is_influential": true
    },
    {
      "source": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "target": "54e14b407d246a73d0af8f22ed67c363d3aa6f54",
      "type": "cites",
      "contexts": "[\"Following the original study (Go et al., 2024), we analyze two datasets, HH-RLHF (Bai et al., 2022) and SHP (Etha-yarajh et al., 2022), each containing prompt-response pairs with human helpfulness rankings.\", \"However, since our trained preference model is linear, the features discovered by our pipeline maintain interpretability comparable to those in (Go et al., 2024).\", \"\\u2026reconstruction quality via perplexity (PPL) and iteratively concatenating features in sequence to create a set that best captures the dataset\\u2019s properties. response pairs to create preference models that match or exceed the performance of those based on expert-crafted features (Go et al., 2024).\", \"To train our PM, we adapted Go et al. (2024)\\u2019s approach, using GPT-4 to rate responses on a 1-10 scale for each feature (prompts in Appendix I.1).\", \"Similarly to Go et al. (2024), we evaluate PM quality using pairwise win rates.\", \"Overall, our unsupervised features matched the performance of state-of-the-art hand-crafted preference models (Go et al., 2024) across all metrics, with superior accuracy and generalizability.\", \"We compare our method to compositional preference models (CPMs) (Go et al., 2024), which validate responses against predefined features before training a linear regression to predict preferences.\", \"We compare our features against the 14 expert-crafted features from Go et al. (2024) (Appendix I.3).\", \"According to Go et al. (2024), lower divergence from these comprehensively trained reference models indicates better generalization to unseen data.\"]",
      "is_influential": true
    },
    {
      "source": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "cites",
      "contexts": "[\"To assess our pipeline\\u2019s components, we evaluate three stages, with detailed parameters in the Appendix E: \\u2022 Generation: We generate proposed features using the generation-stage prompt with GPT-4o and randomly sample up to 50 features, representing the initial pipeline output.\", \"Table 1 presents a comparison between our approach and the WildTeaming baseline across Gemini 1.5 Flash (Team et al., 2024), GPT-4, and Llama 3.1 8B Instruct.\", \"Overall, our method successfully compresses WildTeam-ing\\u2019s 500 clusters into just 20 features (a 25x reduction in feature space) while maintaining or improving performance across most metrics, particularly for robust models like GPT-4 and Llama 3.1 8B Instruct.\", \"To train our PM, we adapted Go et al. (2024)\\u2019s approach, using GPT-4 to rate responses on a 1-10 scale for each feature (prompts in Appendix I.1).\", \"For both GPT-4o and Llama 3.1 8B Instruct, we achieve higher ASR \\u00d7 5 30 with comparable query counts, though WildTeaming demonstrates an advantage in generating diverse jailbreaks on Gemini 1.5 Flash.\", \"We use GPT-4o to generate 50 features with temperature and top p set to 1, using a random sample of 100 instances due to context limitations.\", \"For analysis, we study the WildJailbreak dataset where Mixtral-8\\u00d77B (Jiang et al., 2024a) or GPT-4 are given harmful queries and instructed to generate jailbreaks by combining 2\\u20137 tactics sampled from 500 clusters (containing 85k total human jailbreak tactics).\", \"Our pipeline uses GPT-4o (Hurst et al., 2024) for feature proposal and evalidation, and Llama 3.1 8B Instruct (Dubey et al., 2024) for feature selection.\", \"Recent advances in large language models (LLMs) (Vaswani, 2017; Radford, 2019; Achiam et al., 2023) have emerged as a promising approach to this challenge, enabling researchers to process datasets and generate natural language descriptions that summarize and analyze underlying information (Singh et\\u2026\", \"To compare them, we use GPT-4 Turbo with an AlpacaEval (Li et al., 2023) prompt, evaluating both orderings and selecting the response with higher log probabilities.\", \"We apply our method to the WildTeaming framework (Jiang et al., 2024b), a state-of-the-art approach that uses GPT-4 (Achiam et al., 2023) to extract 100k+ tactics from human jailbreaks in LMSYS-CHAT-1M (Zheng et al., 2023a) and WILDCHAT (Zhao et al., 2024).\", \"We employ GPT-4o (Hurst et al., 2024) Featurization Stage.\"]",
      "is_influential": true
    },
    {
      "source": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "cites",
      "contexts": "[\"Following the original study (Go et al., 2024), we analyze two datasets, HH-RLHF (Bai et al., 2022) and SHP (Etha-yarajh et al., 2022), each containing prompt-response pairs with human helpfulness rankings.\", \"SHP uses human-written responses from natural interactions, while HH-RLHF uses machine-generated responses ranked for alignment.\", \"We sample 1,000 preferred responses from each dataset and create separate feature spaces of 50 features for SHP and (Appendix I.4) and HH-RLHF (Appendix I.5) using parameters from Appendix E, with Llama 3.1 8B Instruct handling the final stage via the prompt in Appendix F.3.\", \"For a detailed analysis of these features, refer to Appendix I.4 for SHP and Appendix I.5 for HH-RLHF, with the top coefficients visualized in Table 5.\", \"We use fine-tuned DeBERTa models as our references for HH-RLHF 4 and SHP (Sileo, 2023) and plot their BoN scores against PMs trained with our features and the baseline features.\", \"Our method demonstrates superior robustness across feature counts on SHP, while achieving comparable robustness on HH-RLHF.\", \"Our analysis of the HH-RLHF dataset shows that both our PMs and the baseline PMs maintain similar reward differentials, though our models achieve slightly higher rewards overall.\", \"The HH-RLHF plot demonstrates that similar to accuracy, we can choose to generate more features and easily improve the performance of our PM.\", \"In the HH-RLHF dataset, the features with the highest positive coefficients are \\u201dincludes educational content\\u201d and \\u201dis structured with clear, distinct sections.\\u201d\"]",
      "is_influential": true
    },
    {
      "source": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "target": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
      "type": "cites",
      "contexts": "[\"Following the original study (Go et al., 2024), we analyze two datasets, HH-RLHF (Bai et al., 2022) and SHP (Etha-yarajh et al., 2022), each containing prompt-response pairs with human helpfulness rankings.\", \"In RLHF, a preference model (PM) learns from human-ranked responses to score LLM outputs.\", \"SHP uses human-written responses from natural interactions, while HH-RLHF uses machine-generated responses ranked for alignment.\", \"We sample 1,000 preferred responses from each dataset and create separate feature spaces of 50 features for SHP and (Appendix I.4) and HH-RLHF (Appendix I.5) using parameters from Appendix E, with Llama 3.1 8B Instruct handling the final stage via the prompt in Appendix F.3.\", \"For a detailed analysis of these features, refer to Appendix I.4 for SHP and Appendix I.5 for HH-RLHF, with the top coefficients visualized in Table 5.\", \"We use fine-tuned DeBERTa models as our references for HH-RLHF 4 and SHP (Sileo, 2023) and plot their BoN scores against PMs trained with our features and the baseline features.\", \"The growing capabilities of LLMs necessitate their alignment with human preferences, primarily through reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022).\", \"Our method demonstrates superior robustness across feature counts on SHP, while achieving comparable robustness on HH-RLHF.\", \"Our analysis of the HH-RLHF dataset shows that both our PMs and the baseline PMs maintain similar reward differentials, though our models achieve slightly higher rewards overall.\", \"The HH-RLHF plot demonstrates that similar to accuracy, we can choose to generate more features and easily improve the performance of our PM.\", \"In the HH-RLHF dataset, the features with the highest positive coefficients are \\u201dincludes educational content\\u201d and \\u201dis structured with clear, distinct sections.\\u201d\"]",
      "is_influential": true
    },
    {
      "source": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "cites",
      "contexts": "[\"Our pipeline shows promise across scientific research, from social science to medical analysis (Tamkin et al., 2024; Singh et al., 2025; Wolf et al., 2018).\"]",
      "is_influential": false
    },
    {
      "source": "2347041888",
      "target": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "type": "authored"
    },
    {
      "source": "2347041748",
      "target": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "type": "authored"
    },
    {
      "source": "2347043065",
      "target": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "type": "authored"
    },
    {
      "source": "2347043097",
      "target": "36afdce83a491f752325f4cd115ecb9d91a8f4be",
      "type": "authored"
    },
    {
      "source": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "cites",
      "contexts": "[\"\\u2026video understanding tasks and have the ability to take a full video as input (2 closed-source models and 5 open-source models): Gemini 1.5 Flash (Team et al., 2024), GPT-4o (OpenAI), VideoChat2 (Li et al., 2023a), Video-ChatGPT (Maaz et al., 2023), LLaVA-Video (Zhang et al., 2024c),\\u2026\", \"1: Information about models tested on S OCIAL G ENOME : VideoChat (Li et al., 2023a), VideoChat-GPT (Maaz et al., 2023), LLaVA-Video (Zhang et al., 2024b), LLaVA-Video-Only (Zhang et al., 2024b), LongVA (Zhang et al., 2024a), GPT-4o (OpenAI), Gemini 1.5 Flash (Team et al., 2024).\", \"5-Pro sparse mixture-of-experts transformer (Team et al., 2024).\", \"5-Flash (Team et al., 2024), GPT-4o (OpenAI), LLaVA-Video and LLaVA-Video-Only (Zhang et al., 2024c), LongVA (Zhang et al., 2024a), Video-ChatGPT (Maaz et al., 2023), and VideoChat2 (Li et al., 2023a).\"]",
      "is_influential": true
    },
    {
      "source": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "target": "deeb72a8582a68403082f2da4e7fa6a3c4da26b4",
      "type": "cites",
      "contexts": "[\"\\u2026a full video as input (2 closed-source models and 5 open-source models): Gemini 1.5 Flash (Team et al., 2024), GPT-4o (OpenAI), VideoChat2 (Li et al., 2023a), Video-ChatGPT (Maaz et al., 2023), LLaVA-Video (Zhang et al., 2024c), LLaVA-Video-Only (Zhang et al., 2024c), LongVA (Zhang et al., 2024a).\", \"The LLaVA-Video model LLaVA-Video-7B-Qwen2 (Zhang et al., 2024c) has an architecture with a SigLIP SO400M vision transformer and Qwen2 language model, has 7B parameters, can process up to 110 frames, and was trained on mixture of single image, multi-image, and video tasks from the LLaVA-Video-178K\\u2026\"]",
      "is_influential": true
    },
    {
      "source": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "target": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "cites",
      "contexts": "[\"LongVA The LongVA model LongVA-7B-DPO (Zhang et al., 2024a) aligns a unified multimodal transformer (UMT) with QFormer and aligns this visual encoder with a Qwen2 7B language model.\", \"1: Information about models tested on S OCIAL G ENOME : VideoChat (Li et al., 2023a), VideoChat-GPT (Maaz et al., 2023), LLaVA-Video (Zhang et al., 2024b), LLaVA-Video-Only (Zhang et al., 2024b), LongVA (Zhang et al., 2024a), GPT-4o (OpenAI), Gemini 1.5 Flash (Team et al., 2024).\", \"\\u2026a full video as input (2 closed-source models and 5 open-source models): Gemini 1.5 Flash (Team et al., 2024), GPT-4o (OpenAI), VideoChat2 (Li et al., 2023a), Video-ChatGPT (Maaz et al., 2023), LLaVA-Video (Zhang et al., 2024c), LLaVA-Video-Only (Zhang et al., 2024c), LongVA (Zhang et al., 2024a).\"]",
      "is_influential": true
    },
    {
      "source": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "target": "81002626b9eeb896f2adcdc838a4451d9001cdb3",
      "type": "cites",
      "contexts": "[\"SOTA models have struggled to perform well on Social-IQ 2.0 (Xie and Park, 2023; Pirhadi et al., 2023; Li et al., 2024c; Agrawal et al., 2024; Chen et al., 2024).\", \"We note that prior papers that test models on S OCIAL -IQ 2.0 have used the validation set, not the test set (Xie and Park, 2023; Pirhadi et al., 2023; Guo et al., 2023; Li et al., 2024c; Agrawal et al., 2024; Chen et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "target": "aa66639895a7dfce1e229293e546686912ba8320",
      "type": "cites",
      "contexts": "[\"SOTA models have struggled to perform well on Social-IQ 2.0 (Xie and Park, 2023; Pirhadi et al., 2023; Li et al., 2024c; Agrawal et al., 2024; Chen et al., 2024).\", \"We note that prior papers that test models on S OCIAL -IQ 2.0 have used the validation set, not the test set (Xie and Park, 2023; Pirhadi et al., 2023; Guo et al., 2023; Li et al., 2024c; Agrawal et al., 2024; Chen et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "target": "cf4fac3510b0e4fdab47fa7495c18ad32dfefde0",
      "type": "cites",
      "contexts": "[\"We note that prior papers that test models on S OCIAL -IQ 2.0 have used the validation set, not the test set (Xie and Park, 2023; Pirhadi et al., 2023; Guo et al., 2023; Li et al., 2024c; Agrawal et al., 2024; Chen et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "target": "21a6bd63a062acab5e738db4c593891060df6205",
      "type": "cites",
      "contexts": "[\"SOTA models have struggled to perform well on Social-IQ 2.0 (Xie and Park, 2023; Pirhadi et al., 2023; Li et al., 2024c; Agrawal et al., 2024; Chen et al., 2024).\", \"We note that prior papers that test models on S OCIAL -IQ 2.0 have used the validation set, not the test set (Xie and Park, 2023; Pirhadi et al., 2023; Guo et al., 2023; Li et al., 2024c; Agrawal et al., 2024; Chen et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "target": "23c8d86963639daf04bc6518c1731eafc85bdef1",
      "type": "cites",
      "contexts": "[\"SOTA models have struggled to perform well on Social-IQ 2.0 (Xie and Park, 2023; Pirhadi et al., 2023; Li et al., 2024c; Agrawal et al., 2024; Chen et al., 2024).\", \"We note that prior papers that test models on S OCIAL -IQ 2.0 have used the validation set, not the test set (Xie and Park, 2023; Pirhadi et al., 2023; Guo et al., 2023; Li et al., 2024c; Agrawal et al., 2024; Chen et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "target": "e17aac3bbe0aabda715cafad392f31a1e046c17c",
      "type": "cites",
      "contexts": "[\"Key benchmarks include the video QA tasks of Social-IQ 1.0 (Zadeh et al., 2019) and Social-IQ 2.0 (Wilf et al., 2023); both examine model QA accuracy when answering questions about social interactions in videos.\"]",
      "is_influential": true
    },
    {
      "source": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "target": "fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "type": "cites",
      "contexts": "[\"Social interactions during hour-long videos (Zou et al., 2025), for example, would contain more complex social dynamics and longer-form temporal dependencies across humans.\"]",
      "is_influential": false
    },
    {
      "source": "2259929826",
      "target": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "type": "authored"
    },
    {
      "source": "2347017366",
      "target": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "type": "authored"
    },
    {
      "source": "28130078",
      "target": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "type": "authored"
    },
    {
      "source": "28130078",
      "target": "e17aac3bbe0aabda715cafad392f31a1e046c17c",
      "type": "authored"
    },
    {
      "source": "49933077",
      "target": "693973a41cac31ec2c02e7ce7c59e4bc0dc6d2f7",
      "type": "authored"
    },
    {
      "source": "49933077",
      "target": "e17aac3bbe0aabda715cafad392f31a1e046c17c",
      "type": "authored"
    },
    {
      "source": "2228362755",
      "target": "020c82cbe72629c014c430e198d08d020115c67d",
      "type": "authored"
    },
    {
      "source": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "cites",
      "contexts": "[\"5 Turbo, Claude 2, and Gemini 1.5, now claim support for input contexts beyond 100,000 tokens (Georgiev et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "target": "078a9c3dcd0575dccc45c861618e2363caf47986",
      "type": "cites",
      "contexts": "[\"\\u2022 Expanded Benchmarks , such as InfinityBench (Zhang et al., 2024), NeedleBench (Li et al., 2024), or domain-specific tasks (e.g., LongHealth (Adams et al., 2024)), to further validate performance across diverse domains.\", \"Instead of orchestrating separate retrieval calls, we instruct the model to locate and tag relevant portions of the input text, then walk through these tagged segments in a CoT style (Li et al., 2024).\", \"Multi-hop reasoning tasks requiring multiple disjoint facts can falter if the initial top-k retrieval does not capture all necessary segments (Li et al., 2024).\", \"Similarly, Li et al. (2024) presented NeedleBench , extending context lengths up to one million tokens to assess how models handle single-needle or multi-needle retrieval challenges.\", \"Despite this progress, LLMs still face critical challenges when dealing with very long inputs containing dispersed, multi-faceted information (Kuratov et al., 2024; Li et al., 2024; Wang et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "cites",
      "contexts": "[\"Key issues include context confusion and context dilution in the attention mechanism, making it difficult for the model to prioritize and recall crucial facts (Lee et al., 2024).\", \"Models often fail on tasks requiring multi-hop reasoning, especially when relevant details are scattered across large portions of text (Lee et al., 2024; Adams et al., 2024; Karpinska et al., 2024; Levy et al., 2024).\", \"Various iterative or tool-augmented retrieval methods have emerged to address these limitations, interleaving generation with refined retrieval calls (Lee et al., 2024).\", \"Consequently, an important line of work explores combining CoT with external retrieval calls (Shaham et al., 2022; Lee et al., 2024).\", \"One widely adopted strategy is Retrieval-Augmented Generation (RAG) , where a language model is paired with an external retriever that fetches relevant snippets from an indexed corpus (Lee et al., 2024; Reddy et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "target": "b47507f12a8b7d26f78a484e32a08d61d3f87358",
      "type": "cites",
      "contexts": "[\"For instance, Kuratov et al. (2024) introduced BABILong , which provides tasks specifically designed to test needle-in-a-haystack retrieval and multi-step reasoning within extensive inputs.\", \"Despite this progress, LLMs still face critical challenges when dealing with very long inputs containing dispersed, multi-faceted information (Kuratov et al., 2024; Li et al., 2024; Wang et al., 2024).\", \"Despite large or even multimillion-token windows, models often attend to only a fraction of the provided text, leading to performance degradation on reasoning tasks where critical information is scattered far apart (Kuratov et al., 2024; Zhang et al., 2024; Levy et al., 2024).\", \"We test our method on the BABILong dataset (Kuratov et al., 2024) requiring advanced reasoning over thousands of tokens, including large-narrative benchmarks such as NovelQA (Wang et al., 2025).\"]",
      "is_influential": true
    },
    {
      "source": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "cites",
      "contexts": "[\"However, when crucial facts are missing or buried in large contexts, CoT by itself may lead to incomplete or hallucinated chains of reasoning (Agarwal et al., 2024).\", \"However, when crucial facts are omitted or buried in large swaths of irrelevant content, CoT alone may lead to hallucinated or incomplete reasoning (Agarwal et al., 2024).\", \"\\u2022 Expanded Benchmarks , such as InfinityBench (Zhang et al., 2024), NeedleBench (Li et al., 2024), or domain-specific tasks (e.g., LongHealth (Adams et al., 2024)), to further validate performance across diverse domains.\", \"Despite large or even multimillion-token windows, models often attend to only a fraction of the provided text, leading to performance degradation on reasoning tasks where critical information is scattered far apart (Kuratov et al., 2024; Zhang et al., 2024; Levy et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "target": "344fa52471306a25133c9536992be15eecdd1c60",
      "type": "cites",
      "contexts": "[\"Models often fail on tasks requiring multi-hop reasoning, especially when relevant details are scattered across large portions of text (Lee et al., 2024; Adams et al., 2024; Karpinska et al., 2024; Levy et al., 2024).\", \"Other datasets like NovelQA (Wang et al., 2025), One Thousand and One Pairs (Karpinska et al., 2024), and CLongEval (Qiu et al., 2024) test performance on massive text corpora, including entire novels and Chinese-language benchmarks.\"]",
      "is_influential": false
    },
    {
      "source": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "target": "879905229dd629570165ab92cfa4045d7de0cbe5",
      "type": "cites",
      "contexts": "[\"Despite this progress, LLMs still face critical challenges when dealing with very long inputs containing dispersed, multi-faceted information (Kuratov et al., 2024; Li et al., 2024; Wang et al., 2024).\"]",
      "is_influential": false
    },
    {
      "source": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "target": "76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d",
      "type": "cites",
      "contexts": "[\"Other datasets like NovelQA (Wang et al., 2025), One Thousand and One Pairs (Karpinska et al., 2024), and CLongEval (Qiu et al., 2024) test performance on massive text corpora, including entire novels and Chinese-language benchmarks.\", \"We test our method on the BABILong dataset (Kuratov et al., 2024) requiring advanced reasoning over thousands of tokens, including large-narrative benchmarks such as NovelQA (Wang et al., 2025).\"]",
      "is_influential": false
    },
    {
      "source": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "target": "0ed0d844544c4a2981acccb9332dead922294664",
      "type": "cites",
      "contexts": "[\"Other datasets like NovelQA (Wang et al., 2025), One Thousand and One Pairs (Karpinska et al., 2024), and CLongEval (Qiu et al., 2024) test performance on massive text corpora, including entire novels and Chinese-language benchmarks.\"]",
      "is_influential": false
    },
    {
      "source": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "target": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "cites",
      "contexts": "[]",
      "is_influential": false
    },
    {
      "source": "2346055707",
      "target": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "type": "authored"
    },
    {
      "source": "10424769",
      "target": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "type": "authored"
    },
    {
      "source": "2243408877",
      "target": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "type": "authored"
    },
    {
      "source": "2247886893",
      "target": "ee0bbc3dd5cbf1f0d840fc80ac59db0aba6124c9",
      "type": "authored"
    },
    {
      "source": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "cites",
      "contexts": "[\"While some LLMs support up to 2 million tokens [6], directly processing decompiled malware remains impractical, as some samples would still exceed this limit.\", \"Besides GPT-4o-mini, we select Gemini 1.5 pro [6] as another comparison, which claims their longest context windows and for its malware detection capabilities.\"]",
      "is_influential": true
    },
    {
      "source": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "target": "79d94b9c8f17f754c35bceb29eb76d7a39664c5b",
      "type": "cites",
      "contexts": "[\"Walton et al. [34] proposed a hierarchical approach, analyzing decompiled code at the function, class, and package levels.\"]",
      "is_influential": false
    },
    {
      "source": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "target": "f02a5b86269fe40498e58061bb9b85a1c8b73a12",
      "type": "cites",
      "contexts": "[\"However, most studies focus on relatively simple malware ecosystems, such as npm packages [31], PowerShell scripts [32], Linux binaries [30], and JavaScript-based threats [33].\"]",
      "is_influential": false
    },
    {
      "source": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "target": "f8bb6887a591872b47f75d28b22baba7b0521655",
      "type": "cites",
      "contexts": "[\"However, most studies focus on relatively simple malware ecosystems, such as npm packages [31], PowerShell scripts [32], Linux binaries [30], and JavaScript-based threats [33].\"]",
      "is_influential": false
    },
    {
      "source": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "target": "ca100914a70cf077eaff278b4827cdeeee742868",
      "type": "cites",
      "contexts": "[\"However, these methods either target to specific feature space [13] or introduce high retraining overhead and the risk of label poisoning [21], [22].\"]",
      "is_influential": false
    },
    {
      "source": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "target": "50fb8bcb90734b7b86cb257162038fcde3b47bad",
      "type": "cites",
      "contexts": "[\"However, most studies focus on relatively simple malware ecosystems, such as npm packages [31], PowerShell scripts [32], Linux binaries [30], and JavaScript-based threats [33].\"]",
      "is_influential": false
    },
    {
      "source": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "target": "f82f49c20c6acc69f884f05e3a9f1ceea91061ce",
      "type": "cites",
      "contexts": "[\"[41], producing facts inconsistent with instructions.\"]",
      "is_influential": false
    },
    {
      "source": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "target": "4bfd5e2221b88b9d38c7111fc0a525027f64b619",
      "type": "cites",
      "contexts": "[\"LLMs are increasingly used in security tasks like code analysis [27], vulnerability detection [28], [29], and malware classification [30].\", \"However, most studies focus on relatively simple malware ecosystems, such as npm packages [31], PowerShell scripts [32], Linux binaries [30], and JavaScript-based threats [33].\"]",
      "is_influential": false
    },
    {
      "source": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "target": "c37b66cae380ff6bc210338fef200ec1874ceb5f",
      "type": "cites",
      "contexts": "[\"Zhao et al. [35] relies on predefined feature spaces (Drebin [11]) to generate feature summaries instead of analyzing raw code.\"]",
      "is_influential": false
    },
    {
      "source": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "target": "3882788d42339848f5828362e367437e9a5447e0",
      "type": "cites",
      "contexts": "[\"Truncation offers a potential alternative, but it can cause substantial contextual loss, ultimately degrading analysis accuracy [7].\"]",
      "is_influential": false
    },
    {
      "source": "2346517767",
      "target": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "type": "authored"
    },
    {
      "source": "2220672771",
      "target": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "type": "authored"
    },
    {
      "source": "2220672771",
      "target": "ca100914a70cf077eaff278b4827cdeeee742868",
      "type": "authored"
    },
    {
      "source": "2346104589",
      "target": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "type": "authored"
    },
    {
      "source": "2346055417",
      "target": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "type": "authored"
    },
    {
      "source": "2344628799",
      "target": "c45fa8d9cf08902c8eaa46e03b23f6a2f0d9fb39",
      "type": "authored"
    },
    {
      "source": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "target": "c811bedbe8f4c21d0cba9f9175f7c2eb203284a7",
      "type": "cites",
      "contexts": "[\"These models such as GPT-4o (OpenAI, 2024), Claude-3 (An-thropic, 2024a), Gemini (Team et al., 2024), and Llama-3.\", \"The proprietary models we used are gpt-4o-2024-08-06, claude-3-5-sonnet-20240620, claude-3-sonnet-20240229, claude-3-haiku-20240307, Gemini 1.5 Pro (May 2024), and Gemini 1.5 Flash (May 2024).\", \"Some proprietary models such as GPT-4o (OpenAI, 2024), Claude-3 (An-thropic, 2024a), and Gemini (Team et al., 2024) show superior performance, especially on visually complex reasoning tasks such as MathVista (Lu et al., 2024b).\", \"For proprietary models, we include GPT-4o (OpenAI, 2024), the Claude family (Claude 3.5 Sonnet (Anthropic, 2024b), Claude 3 Son-net (Anthropic, 2024a), Claude 3 Haiku (An-thropic, 2024a)) and the Gemini family (Gemini 1.5 Pro (Team et al., 2024), Gemini 1.5 Flash (Team et al., 2024)).\", \"V ISION , the scores of GPT-4o and Gemini 1.5 Pro in the \\u201cText Only\\u201d mode are significantly lower than in the \\u201cText + Image\\u201d mode, indicating a notable performance difference.\", \"Following GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro also show robust results.\"]",
      "is_influential": true
    },
    {
      "source": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "cites",
      "contexts": "[\"For all MLLMs, we set the generation temperature to 0.2, the nucleus sampling parameter top _ p to 0.95, and the maximum generation length to 1024 tokens, the same as the other code generation work (Hui et al., 2024; Zheng et al., 2024; Guo et al., 2024b; Zhu et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "target": "5eac2a40422a7085cb6f03285ad08210b6f6744b",
      "type": "cites",
      "contexts": "[\"For all MLLMs, we set the generation temperature to 0.2, the nucleus sampling parameter top _ p to 0.95, and the maximum generation length to 1024 tokens, the same as the other code generation work (Hui et al., 2024; Zheng et al., 2024; Guo et al., 2024b; Zhu et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "target": "8946891e94831adc8cddb0d32311cce2445c96d2",
      "type": "cites",
      "contexts": "[\"Compare with MathVista.\", \"V ISION and MathVista.\", \"We select some proprietary models and open-source models with similar performance on MathVista to test their performance on C ODE -\", \"V ISION over MM-Code and how it can evaluate the reasoning capabilities of MLLMs from different perspectives compared to MathVista.\", \"MathVista (Lu et al., 2024b) and Math-Vision (Wang et al., 2024b) to evaluate the mathematical reasoning abilities of MLLMs within visual contexts.\", \"When comparing with MathVista (Lu et al., 2024b), we find that proprietary models maintain similar performance across both benchmarks, while open-source models show a significant performance drop (around -30\", \"2021), mathematical reasoning (Lu et al., 2024b; Wang et al., 2024b), and code generation (Li et al., 2024c; Shi et al., 2024).\", \"Previous research has evaluated the reasoning ability of MLLMs through tasks like Visual Question Answering (VQA) (Mobasher et al., 2022; Gurari et al., 2018) and mathematical reasoning (Lu et al., 2024b).\", \"MathVista is a multimodal mathematics reasoning benchmark.\", \"V ISION is significantly lower than on MathVista, with a performance gap of around -30%.\", \"Some proprietary models such as GPT-4o (OpenAI, 2024), Claude-3 (An-thropic, 2024a), and Gemini (Team et al., 2024) show superior performance, especially on visually complex reasoning tasks such as MathVista (Lu et al., 2024b).\"]",
      "is_influential": true
    },
    {
      "source": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "target": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "cites",
      "contexts": "[\"\\u2026Multimodal Large Language Models (MLLMs) have gained significant attention due to their ability to process and generate information across various modalities, such as text, images, and audio (Li et al., 2024a; Liu et al., 2024; Zhu et al., 2023; Li et al., 2024b; Dai et al., 2023; Ye et al., 2024).\"]",
      "is_influential": false
    },
    {
      "source": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "cites",
      "contexts": "[\"6 (Yao et al., 2024a), Qwen-VL (Bai et al., 2023), Deepseek-VL (Lu et al., 2024a).\"]",
      "is_influential": false
    },
    {
      "source": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "cites",
      "contexts": "[\"ChartMimic (Shi et al., 2024) requires MLLMs to generate corresponding chart rendering code by using information-dense visual charts and text instructions as input.\", \"2021), mathematical reasoning (Lu et al., 2024b; Wang et al., 2024b), and code generation (Li et al., 2024c; Shi et al., 2024).\"]",
      "is_influential": false
    },
    {
      "source": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "cites",
      "contexts": "[\"2-Vision (AI, 2024), Phi-3-Vision (Abdin et al., 2024a), MiniCPM V2.\"]",
      "is_influential": false
    },
    {
      "source": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "target": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "cites",
      "contexts": "[\"Step 1 Step 2 Step 3 LeetCode HumanEval bilities of LLMs (Chen et al., 2021; Hendrycks et al., 2021; Jain et al., 2024; Austin et al., 2021), many researchers have begun using code generation to evaluate the reasoning abilities of MLLMs by introducing visual contexts.\"]",
      "is_influential": false
    },
    {
      "source": "2329617551",
      "target": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "type": "authored"
    },
    {
      "source": "2345914083",
      "target": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "type": "authored"
    },
    {
      "source": "2346246767",
      "target": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "type": "authored"
    },
    {
      "source": "2346578527",
      "target": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "type": "authored"
    },
    {
      "source": "2345818907",
      "target": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "type": "authored"
    },
    {
      "source": "2345818783",
      "target": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "type": "authored"
    },
    {
      "source": "2345873396",
      "target": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "type": "authored"
    },
    {
      "source": "2346054898",
      "target": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "type": "authored"
    },
    {
      "source": "2346673644",
      "target": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "type": "authored"
    },
    {
      "source": "1699104018",
      "target": "6b3062b7e2f0bb4f85527ede6f294946fc035639",
      "type": "authored"
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "8970a42d5de27dcf0833f3f1387de586b6d85a3e",
      "type": "cites",
      "contexts": "[\"This paradigm, wherein FFNs are utilized as experts, remains predominant, and subsequent refinements will be expounded upon in Sections 4.2.2 to 4.2.4.\", \"Similar to dense MoE, the soft MoE approach maintains full differentiability by leveraging all the experts for processing each input, thus avoiding issues inherent to discrete expert selection.\"]",
      "is_influential": true
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8",
      "type": "cites",
      "contexts": "[\"This paradigm, wherein FFNs are utilized as experts, remains predominant, and subsequent refinements will be expounded upon in Sections 4.2.2 to 4.2.4.\", \"Similar to dense MoE, the soft MoE approach maintains full differentiability by leveraging all the experts for processing each input, thus avoiding issues inherent to discrete expert selection.\"]",
      "is_influential": true
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "cites",
      "contexts": "[\"To address this, each MoE layer incorporates an auxiliary loss function that promotes an even distribution of tokens across experts within each batch, as described in many studies [30, 44, 49, 74, 86, 94, 154].\", \"Switch Transformers [49] and many other subsequent studies [34, 44, 74, 94] have embraced this \\ud835\\udc3f \\ud835\\udc4e\\ud835\\udc62\\ud835\\udc65 design, and enhancements [30, 36, 154] have been made to cater to diverse requirements.\", \"The incorporation of MoE architectures into large language models (LLMs) is now a prevalent practice, allowing these models to achieve significant parameter scale-ups and consequent enhancements in capabilities [48, 49, 74, 86, 154].\", \"From this period forward, MoE models [34, 74, 154, 197] typically featured larger expert dimensions.\", \"\\u2026et al.[135], GShard[86], Switch Transformer[49] ST-MoE[197], M6-t[173], Mod-Squad[21], StableMoE[31] ModuleFormer[140], OpenMoE[172], Skywork-MoE [154] Base Layer[87], DSelect-k[58], V-MoE[128], Z-code M3[80] S-Base[26], Sentence-level MoE[84], NLLB[29], MMoE[101] Task-level MoE[175], X-MoE[22],\\u2026\", \"To counteract this issue, they project hidden vectors into a lower-dimensional space before gating and implement L2 normalization for both token representations and expert embeddings, thus calculating gating scores within a low-dimensional hypersphere.\", \"The hyper-parameter \\ud835\\udc58 is selected based on the specific application, with common choices being \\ud835\\udc58 = 1 [26, 49] or \\ud835\\udc58 = 2 [44, 74, 86, 121, 154, 197].\", \"1 GShard [86], Switch-T [49], GLaM[44], Mixtral-8x7B[74], DBRX[34], Jamba[94], DeepSeekMoE[30], DeepSeek-V2[36], Skywork-MoE[154] \\ud835\\udc4e\\ud835\\udc62\\ud835\\udc65 \\ud835\\udc64 \\ud835\\udc4e\\ud835\\udc62\\ud835\\udc65 = 0 .\", \"Reflecting this trend, more recent sparse MoE models [34, 74, 94, 151, 154, 166, 172, 197] commonly utilize no more than 64 experts.\", \"Following the introduction of Mixtral 8x7B [74], the trend seems to shift towards placing MoE in every layer of the model, with a common choice of only 8 or 16 experts mirroring the dimensionality of a dense FFN [30, 34, 151, 154].\"]",
      "is_influential": true
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "cites",
      "contexts": "[\"The load balancing is facilitated by the selection of hash functions [87], (c) the combination of grouped domain mapping and random gating [127], (d) expert-choice gating [193], (e) attention router [166], and (f) soft MoE with expert merging [105]. prior to training, which can equitably distribute\\u2026\", \"Skywork-MoE [154] proposes two innovative techniques: gating logit normalization, which improves expert diversification, and adaptive auxiliary loss coefficients, which provides layer-specific adjustment of auxiliary loss coefficients.\", \"\\u2026Task-level MoE[175], X-MoE[22], Uni-Perceiver-MoE[195] Mixtral-8x7B[74], DeepSeekMoE[30], Jamba[94], DBRX[34], MoA[182], JetMoE [139], Yuan 2.0-M32 [166], DS-MoE[117] For example, the Mixtral 8x7B [74], introduced by Mixtral AI, shares its foundational architecture with the earlier Mistral 7B\\u2026\", \"Reflecting this trend, more recent sparse MoE models [34, 74, 94, 151, 154, 166, 172, 197] commonly utilize no more than 64 experts.\"]",
      "is_influential": true
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "7f6f34090d17d0523b91c0d3cd34fe9f66ebb189",
      "type": "cites",
      "contexts": "[\"This paradigm, wherein FFNs are utilized as experts, remains predominant, and subsequent refinements will be expounded upon in Sections 4.2.2 to 4.2.4.\", \"Similar to dense MoE, the soft MoE approach maintains full differentiability by leveraging all the experts for processing each input, thus avoiding issues inherent to discrete expert selection.\"]",
      "is_influential": true
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "d9c05ef795a82e35280ddc04f9a0c203114f546e",
      "type": "cites",
      "contexts": "[\"This paradigm, wherein FFNs are utilized as experts, remains predominant, and subsequent refinements will be expounded upon in Sections 4.2.2 to 4.2.4.\", \"Similar to dense MoE, the soft MoE approach maintains full differentiability by leveraging all the experts for processing each input, thus avoiding issues inherent to discrete expert selection.\"]",
      "is_influential": true
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "53a803388e83ae89261624099d7be4287ace67cb",
      "type": "cites",
      "contexts": "[\"Switch Transformers [49] and many other subsequent studies [34, 44, 74, 94] have embraced this \\ud835\\udc3f \\ud835\\udc4e\\ud835\\udc62\\ud835\\udc65 design, and enhancements [30, 36, 154] have been made to cater to diverse requirements.\", \"1 GShard [86], Switch-T [49], GLaM[44], Mixtral-8x7B[74], DBRX[34], Jamba[94], DeepSeekMoE[30], DeepSeek-V2[36], Skywork-MoE[154] \\ud835\\udc4e\\ud835\\udc62\\ud835\\udc65 \\ud835\\udc64 \\ud835\\udc4e\\ud835\\udc62\\ud835\\udc65 = 0 .\", \"Differently, DeepSeekMoE [30, 36] introduces the concept of fine-grained expert segmentation by subdividing the intermediate hidden dimension of FFN expert, while preserving the overall parameter count.\", \"As depicted in Figure 1, MoE has maintained a robust trajectory of growth, particularly notable in 2024 with the advent of Mixtral-8x7B [74] and a variety of subsequent industrial-scale LLMs such as Grok-1 [169], DBRX [34], Arctic [152], DeepSeek-V2 [36], etc.\", \"\\u2026with LLMs has unlocked extraordinary capabilities in a range of natural language understanding (NLU) and generation (NLG) tasks, including machine translation [29, 135], open-domain question answering [6, 44], code generation [30, 74, 150, 154], and mathematical problem-solving [30, 36, 74, 150].\", \"Under this taxonomy, we first delve into MoE algorithmic advancements, particularly the prevalent substitution of feed-forward network (FFN) layers with MoE layers in transformer-based LLMs [36, 44, 49, 74, 86, 172, 197].\"]",
      "is_influential": true
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "4b879f069d023e03bf537309a99bdaeb39916ea5",
      "type": "cites",
      "contexts": "[\"Subsequent contributions by Zhong et al. [189] argue that SMEAR\\u2019s demonstrated advantages are confined to downstream fine-tuning on classification tasks.\", \"Therefore, future studies should focus on more effective regularization techniques [197] or innovative gating algorithms [5, 105, 189, 193] that encourage equitable load distribution among experts and enhance model training stability.\", \"\\u2026of MoE into model backbone, and discuss an array of novel MoE-related designs, such as soft MoE with token or expert merging [105, 118, 164, 178, 189], mixture of parameter-efficient experts (MoPEs) [43, 53, 100, 160, 168, 178], training and inference schemes with model transition between dense\\u2026\", \"Lory [189] introduces a causal segment routing strategy, conducting expert merging at the segment level while maintaining the auto-regressive nature of language models.\"]",
      "is_influential": true
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "ebcf108f8bc42140721ff02b6727b0a291362957",
      "type": "cites",
      "contexts": "[\"By leveraging the base weights from a single FFN of the base model, MixLoRA streamlines the creation of the MoPE architecture.\", \"Recent studies [15, 43, 51, 88, 100, 168, 178] have demonstrated the efficacy of combining Parameter-efficient Fine-tuning (PEFT) techniques with MoE frameworks, offering a promising method for incorporating MoE into established LLMs.\", \"Contrastingwith the MoPE implementations we have discussed, MixLoRA[88] creates a LoRA-MoE framework that closely aligns with the conventional MoE models.\", \"To mitigate these limitations, a line of mixture of parameter-efficient experts (MoPE) research has emerged, focusing on integrating the MoE framework with PEFT [88, 97].\", \"A plethora of network architectures has been adopted as experts, including LSTM [135], CNN [25, 183], FFNs (MLPs) [49, 86, 117, 197], Attention [140, 182], and LoRA [43, 88, 100].\", \"MoPE harnesses the best of both fields: the task versatility of MoE and the resource efficiency of PEFT [88], positioning it as a promising area of study that pushes the boundaries of both fields.\", \"Despite these successes, PEFT approaches often struggle with generalizing across multiple tasks due to their limited scope of trainable parameters and the potential for catastrophic forgetting [88].\", \"Rather than just plugging in multiple lightweight experts, MixLoRA fuses LoRA experts with the shared FFN layer.\", \"Furthermore, MixLoRA implements a high-throughput framework that significantly reduces token computation latency and memory usage during both training and inference, optimizing performance and efficiency.\"]",
      "is_influential": true
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "c9ff9fbbe21985b35d6a070b67f22a0e065c4328",
      "type": "cites",
      "contexts": "[\"[49, 86, 197], MoE has served as a substitute for Feed-Forward Network (FFN) modules within these models.\", \"Wang et al. [158] point that while the emergence of Foundation Models made it easier to obtain expert models tailored to specific tasks, the heterogeneity of data at test time necessitates more than a single expert.\", \"Realizing the diversity in data distributions, LoRAMoE separates the experts into two distinct groups: one focuses on learning various downstream tasks, and the other is dedicated to aligning pretrained world knowledge with human instructions.\", \"Similar to dense MoE, the soft MoE approach maintains full differentiability by leveraging all the experts for processing each input, thus avoiding issues inherent to discrete expert selection.\"]",
      "is_influential": true
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "cites",
      "contexts": "[\"\\u2026MoE with token or expert merging [105, 118, 164, 178, 189], mixture of parameter-efficient experts (MoPEs) [43, 53, 100, 160, 168, 178], training and inference schemes with model transition between dense and sparse [16, 82, 145, 149, 170, 184], and various derivatives [5, 19, 23, 124, 146, 171].\", \"\\u2026shared parameters, can be ensembled or parameter-averaged at inference to coalesce into a singular LM. Expanding on this concept, Sukhbaatar et al. [145] present Branch-Train-MiX (BTX), designed to combine the strengths of BTM and Mixture-of-Experts while addressing their respective limitations.\", \"\\u2026165]; Sparse-to-Dense, which involves degrading a sparse MoE model to a dense form that is more conducive to hardware implementation for inference [16, 68, 170]; and Expert Models Merging, a process of integrating multiple pre-trained dense expert models into a unified MoE model [89, 145, 158].\", \"As discussed in existing work [145], the predilection for leveraging MoE in the context of FFNs is rooted in the hypothesis that self-attention layers exhibit lower sparsity and less domain specificity than FFN layers.\"]",
      "is_influential": true
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "1112c0b273c9466906ba6d51fc90881538d33769",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "5d93440621bd49953be4aab0474b0d26dcae504e",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "ae57f38c1c62e2b0a143c0c862f94bb4c11f4fab",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "9e6b56fa4f6ea626c6748b6386214b279dd8f76f",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "0c33df9fc20c9314a0883611e9912a94b511569c",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "98fda72e51ffecf914da5fe5c8f3d64d4078e144",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "cites",
      "contexts": "[\"The effectiveness of expert-choice gating is further validated by Zhou et al. in their subsequent Brainformers study [192].\", \"\\u2026MoE with token or expert merging [105, 118, 164, 178, 189], mixture of parameter-efficient experts (MoPEs) [43, 53, 100, 160, 168, 178], training and inference schemes with model transition between dense and sparse [16, 82, 145, 149, 170, 184], and various derivatives [5, 19, 23, 124, 146, 171].\", \"\\u2026Dense-to-Sparse, which entails initiating with dense model training and progressively transitioning to a sparse MoE configuration [17, 45, 82, 95, 110, 117, 154, 165]; Sparse-to-Dense, which involves degrading a sparse MoE model to a dense form that is more conducive to hardware\\u2026\", \"Building upon the sparse upcycling technique [82], the Skywork-MoE model [154] leverages the foundational architecture of its pre-developed Skywork-13B model [163], adopting its dense checkpoints as a foundation for initial states.\", \"Komatsuzaki et al. [82] highlight the efficiency of sparse models in terms of quality and computational cost, yet acknowledge their significant data requirements and the expense of training from scratch at scale.\"]",
      "is_influential": true
    },
    {
      "source": "2296001947",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "authored"
    },
    {
      "source": "2294682530",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "authored"
    },
    {
      "source": "2304542351",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "authored"
    },
    {
      "source": "2310483288",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "authored"
    },
    {
      "source": "2257349580",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "authored"
    },
    {
      "source": "2295676687",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "authored"
    },
    {
      "source": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "target": "b16c7d45183b9d595ab64301be019741b1528860",
      "type": "cites",
      "contexts": "[\"BTX with Top-2 experts (our default) also approaches the best performance of the specialized models Llemma 7B and CodeLlama 7B in the math and coding domains, while drastically improving over those models on domains that are not their speciality such as world knowledge and commonsense reasoning.\", \"Continual learning Our method relates to continual learning (Awasthi and Sarawagi, 2019) For training a math expert, starting from a code expert rather than a general LLM was shown to be more beneficial (Shao et al., 2024; Azerbayev et al., 2023).\", \"To be comparable to Llemma, we train on the same amount of data as well, i.e. 48k steps with 201B tokens in total.\", \"After making three copies of the seed model Llama-2 7B , we continue training them on the following domain datasets to derive three domain experts: \\u2022 Math: The same data sources and mixture used in Llemma (Azerbayev et al., 2023) model training.\", \"\\u2022 \\u2022 Llemma 7B: A language model specializing in mathematics (Azerbayev et al., 2023) by continued training of CodeLlama 7B on math data.\", \"\\u2026et al., 2020; Touvron et al., 2023; Achiam et al., 2023), including code generation (Li et al., 2022b; Rozi\\u00e8re et al., 2023), solving math problems (Azerbayev et al., 2023), multilinguality (Zhao et al., 2024), etc. Training such LLMs requires a large amount of compute and data, exceeding\\u2026\"]",
      "is_influential": true
    },
    {
      "source": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "target": "fdacf2a732f55befdc410ea927091cad3b791f13",
      "type": "cites",
      "contexts": "[\"In particular, MoE is applied to the feedforward sublayer of Transformers (Fedus et al., 2022; Roller et al., 2021; Lewis et al., 2021), allowing the total number of parameters to grow without additional computation.\", \"We use a loss term similar to (Fedus et al., 2022): Here B is the current data batch, and \\u03b1 is a hyperparameter.\", \"Since the routing decisions are discrete and thus cannot be trained by gradient descent, various training methods have been explored for the Transformer architecture (Fedus et al., 2022; Lewis et al., 2021).\", \"Routing method Besides Top-k routing, we also experiment with other routing methods: \\u2022 Switch: It is a Top-1 routing method proposed by Fedus et al. (2022).\"]",
      "is_influential": true
    },
    {
      "source": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "target": "35b142ea69598e6241f0011312128031df55895c",
      "type": "cites",
      "contexts": "[\"Continual learning Our method relates to continual learning (Awasthi and Sarawagi, 2019) For training a math expert, starting from a code expert rather than a general LLM was shown to be more beneficial (Shao et al., 2024; Azerbayev et al., 2023).\", \"This could be because MMLU contains many math subjects and math training is shown to help on this task (Shao et al., 2024).\"]",
      "is_influential": false
    },
    {
      "source": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "target": "37ac7683543f0e039197a56e71e752a9ebe5998e",
      "type": "cites",
      "contexts": "[\"LLMs scaled in this way have shown impressive performance on downstream tasks (Jiang et al., 2024; Xue et al., 2024).\"]",
      "is_influential": false
    },
    {
      "source": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "cites",
      "contexts": "[\"A study by Dai et al. (2024) showed the advantage of more fine-grained experts, as well as having a shared expert that always stay active.\", \"\\u2026Achiam et al., 2023), including code generation (Li et al., 2022b; Rozi\\u00e8re et al., 2023), solving math problems (Azerbayev et al., 2023), multilinguality (Zhao et al., 2024), etc. Training such LLMs requires a large amount of compute and data, exceeding thousands of GPUs and trillions of tokens.\"]",
      "is_influential": false
    },
    {
      "source": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "target": "1cff5549753a518ed9d6a3517b5050968d710b27",
      "type": "cites",
      "contexts": "[]",
      "is_influential": false
    },
    {
      "source": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "target": "f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31",
      "type": "cites",
      "contexts": "[\"A more recent work by Douillard et al. (2023) showed that less frequent synchronization of diverged workers by averaging their weight changes and applying Nesterov momentum works well in practice for training LLMs.\"]",
      "is_influential": false
    },
    {
      "source": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "cites",
      "contexts": "[\"\\u2022 Code: The same data sources and mixture of code data used in CodeLlama pretraining (Rozi\\u00e8re et al., 2023).\", \"\\u2026performance in a wide-range of tasks (Brown et al., 2020; Touvron et al., 2023; Achiam et al., 2023), including code generation (Li et al., 2022b; Rozi\\u00e8re et al., 2023), solving math problems (Azerbayev et al., 2023), multilinguality (Zhao et al., 2024), etc. Training such LLMs requires a large\\u2026\"]",
      "is_influential": false
    },
    {
      "source": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "target": "464770587aece80cc9e3451050058e30c2aa6666",
      "type": "cites",
      "contexts": "[\"Training on more domains such as using unsupervised domain discovery (Gururangan et al., 2023) should amplify the benefit of the parallelization of experts training.\", \"The Branch-Train-Merge method (Li et al., 2022a; Gururangan et al., 2023) takes parallel training to the extreme by running multiple training processes completely independently.\", \"At this point, the Branch-Train-Merge method (Li et al., 2022a; Gururangan et al., 2023) uses these domain experts as is, choosing which expert to use by determining which domain the input belongs to at inference time.\"]",
      "is_influential": false
    },
    {
      "source": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "cites",
      "contexts": "[\"In recent years, Large Language Models (LLMs) have shown impressive performance in a wide-range of tasks (Brown et al., 2020; Touvron et al., 2023; Achiam et al., 2023), including code generation (Li et al., 2022b; Rozi\\u00e8re et al., 2023), solving math problems (Azerbayev et al., 2023),\\u2026\"]",
      "is_influential": false
    },
    {
      "source": "2265067",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "authored"
    },
    {
      "source": "2290916129",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "authored"
    },
    {
      "source": "2237990986",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "authored"
    },
    {
      "source": "2298402817",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "authored"
    },
    {
      "source": "2255374957",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "authored"
    },
    {
      "source": "3361236",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "authored"
    },
    {
      "source": "3361236",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "3361236",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2253401183",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "authored"
    },
    {
      "source": "2530311",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "authored"
    },
    {
      "source": "2530311",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2072801764",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "authored"
    },
    {
      "source": "2267341626",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "authored"
    },
    {
      "source": "2315344189",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "authored"
    },
    {
      "source": "2275249853",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275250875",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "144517868",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "144517868",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2274773568",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2274773568",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2258629",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275244794",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275252021",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275252424",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245579",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245579",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275246437",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275139370",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2256699302",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2054519183",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2054519183",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275251659",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "47626612",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275198557",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275251620",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275251620",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275251620",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "authored"
    },
    {
      "source": "2275245092",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "4689792",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "4689792",
      "target": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
      "type": "authored"
    },
    {
      "source": "2275245414",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245581",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "133740015",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275251674",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246071",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246071",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275248137",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275248137",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275245419",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2065151121",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2065151121",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275219628",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "35167962",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2146257251",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2146257251",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275157286",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2274782053",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245404",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245404",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275246368",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275120298",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "144114446",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "144114446",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "1466431052",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275545855",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2057091285",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2253841704",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275188918",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275179180",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275179180",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275289833",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2108828435",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2108828435",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2108828435",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "authored"
    },
    {
      "source": "1490681878",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275251158",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2276186593",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2276186593",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275839391",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275839391",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275231534",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "49645091",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2276187456",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2276187456",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275251205",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275244920",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275244920",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275247090",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275247090",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275251200",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275244298",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275244298",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "35363891",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "35363891",
      "target": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "authored"
    },
    {
      "source": "2275252295",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245491",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "66821245",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245457",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2146257131",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2146257131",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2065430571",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2065430571",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2096916416",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2096916416",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275249996",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275249996",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275245820",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275244914",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275251173",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2027599537",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2027599537",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "2275200811",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275254804",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275254804",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275144649",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2325028819",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2325028819",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2261041177",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2261041177",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2158366935",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2265066144",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275250003",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "145565184",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "145565184",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275247307",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275137274",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275137274",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2253699903",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2276101257",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2004021329",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2004021329",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275540338",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275295848",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275295848",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275226809",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275226809",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275245527",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2151087994",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2151087994",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2242286342",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2242286342",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2226452668",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246148",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245339",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245339",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "103681415",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275214107",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275214107",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275210604",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275777049",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "39378983",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "39378983",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2276187117",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2276187117",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2171110177",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2151094350",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2151094350",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2253471334",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2253471334",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275172062",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275752035",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275203081",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275203081",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275250083",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275250083",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275247096",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "35450887",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "35450887",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "authored"
    },
    {
      "source": "2403754",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2403754",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275230678",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275230678",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275169038",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275169038",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "3151440",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "3151440",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2844898",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2844898",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2152264064",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246102",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2260346092",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2260346092",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2149054292",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2149054292",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275296777",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275296777",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275112980",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275112980",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "51131802",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "51131802",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2146257375",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "1485556711",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246094",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246094",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "1666171360",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "1666171360",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275252322",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245594",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2064404342",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275229877",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275229877",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275247085",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275247085",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275246287",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246287",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2274915115",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2990741",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2990741",
      "target": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
      "type": "authored"
    },
    {
      "source": "52152632",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275256930",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275256930",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275285124",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275285124",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275176375",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275176375",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275759230",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275759230",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2253840098",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "1380985420",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "1380985420",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275248327",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2257272397",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245628",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "119341078",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245649",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "46430291",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "14113256",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "14113256",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275245336",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2114362965",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275231822",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275247045",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "39593364",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "39593364",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2047820455",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "3028785",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "3028785",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2274772421",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2274772421",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275234856",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275210659",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275132306",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "10698483",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "10698483",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275246330",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246330",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275252694",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275252694",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2051714782",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2051714782",
      "target": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
      "type": "authored"
    },
    {
      "source": "2275245453",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245453",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "1404556973",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "3407880",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275154456",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2117715631",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2117715631",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "147746767",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "147746767",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275246116",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "3422774",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "7406311",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "7406311",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "authored"
    },
    {
      "source": "2057426488",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2072676",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2273886618",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275115983",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2228518120",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2228518120",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "1435765036",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2713380",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2713380",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "34800652",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "34800652",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275244652",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246178",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246178",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "50213542",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245818",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245818",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275245435",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2274774915",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2274774915",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2068123790",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2068123790",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275125663",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245529",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245529",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275250075",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2136008481",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2136008481",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "1463773776",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "1463773776",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275246346",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246814",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246814",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "144401061",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275150061",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "146162186",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2151088845",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275243930",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275243930",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2285654208",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2285654208",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "38909097",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "38909097",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275178294",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2261024614",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2261024614",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275225165",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275252438",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275252095",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275252095",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275207240",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "11150265",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275250007",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2260406867",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2260406867",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "47204843",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246803",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246803",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2852106",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "144864359",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275265666",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275265666",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2252874293",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "47971768",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "47971768",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "authored"
    },
    {
      "source": "2196579",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275244711",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "102475503",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "102475503",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275246834",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275246834",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "118335789",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2700360",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2064673055",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2064673055",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2151735251",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2151735251",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275252299",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275252299",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2117680841",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "103422608",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "103422608",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2901424",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2901424",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2307592658",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245668",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245668",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "9927844",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "9927844",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275252251",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275252251",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "1701686",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "1701686",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275750817",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275750817",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "145950540",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "145950540",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2151289331",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2151289331",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275252092",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275252092",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2267339677",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2267339677",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275249879",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275249709",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275244171",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275244171",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2065005836",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2065005836",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "authored"
    },
    {
      "source": "2275203310",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275244586",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275244586",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275245661",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "153387869",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "153387869",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275245962",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245962",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275528432",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275540420",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275189326",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275189326",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2170081200",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2170081200",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2253952872",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275244218",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245663",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245663",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2930640",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2930640",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275139180",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275139180",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2065741038",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275249733",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2059411355",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2059411355",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275244177",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275225207",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275225207",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275245771",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275245771",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275299848",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275299848",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2274911253",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2307456650",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2307456650",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275190169",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275452480",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275452480",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275310096",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275593618",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275194186",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275194186",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2563432",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "49629836",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "49629836",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2262080679",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2262080679",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275288889",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275288889",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275545682",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275545682",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275257857",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275257857",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275201537",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2275201537",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275245715",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2368067",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "authored"
    },
    {
      "source": "2368067",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2368067",
      "target": "fdacf2a732f55befdc410ea927091cad3b791f13",
      "type": "authored"
    },
    {
      "source": "2368067",
      "target": "4f57f486adea0bf95c252620a4e8af39232ef8bc",
      "type": "authored"
    },
    {
      "source": "2368067",
      "target": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
      "type": "authored"
    },
    {
      "source": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "cites",
      "contexts": "[\"LLaMA-I (65B) outperforms on MMLU existing instruction finetuned models of moderate sizes, but are still far from the state-of-the-art, that is 77.4 for GPT code-davinci-002 on MMLU (numbers taken from Iyer et al. (2022)).\", \"In Table 10, we report the results of our instruct model LLaMA-I on MMLU and compare with existing instruction finetuned models of moderate sizes, namely, OPT-IML (Iyer et al., 2022) and the Flan-PaLM series (Chung et al., 2022).\", \"In Section 4, we also briefly compare LLaMA with instruction-tuned models such as OPT-IML (Iyer et al., 2022) and Flan-PaLM (Chung et al.\", \"In Table 10, we report the results of our instruct model LLaMA-I on MMLU and compare with existing instruction finetuned models of moderate sizes, namely, OPT-IML (Iyer et al., 2022) and the Flan-PaLM series (Chung et al.\", \"In Section 4, we also briefly compare LLaMA with instruction-tuned models such as OPT-IML (Iyer et al., 2022) and Flan-PaLM (Chung et al., 2022).\"]",
      "is_influential": true
    },
    {
      "source": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "cites",
      "contexts": "[\"Several recent work (Zhang et al., 2022; Hoffmann et al., 2022) have considered the RealToxicityPrompts benchmark (Gehman et al., 2020) as an indicator of how toxic is their model.\", \"This was also observed in previous work (Zhang et al., 2022 Table 12: CrowS-Pairs.\", \"There exist some exceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al.\", \"There exist some exceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022) and GLM (Zeng et al., 2022), but none that are competitive with PaLM-62B or Chinchilla.\", \", 2022), OPT (Zhang et al., 2022), and GLM (Zeng et al.\", \"\\u2026the non-publicly available language models GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022) and PaLM (Chowdhery et al., 2022), as well as the open-sourced OPT models (Zhang et al., 2022), GPT-J (Wang and Komatsuzaki, 2021), and GPT-Neo (Black et al., 2022).\", \"Several recent work (Zhang et al., 2022; Hoffmann et al., 2022) have considered the RealToxicityPrompts benchmark (Gehman et al.\", \"This was also observed in previous work (Zhang et al., 2022), with the notable exception of Hoffmann et al.\", \"This lead to a series of Large Language Models , such as Jurassic-1 (Lieber et al., 2021), Megatron-Turing NLG (Smith et al., 2022), Gopher (Rae et al., 2021), Chinchilla (Hoff-mann et al., 2022), PaLM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), and GLM (Zeng et al., 2022).\", \", 2022), as well as the open-sourced OPT models (Zhang et al., 2022), GPT-J (Wang and Komatsuzaki, 2021), and GPTNeo (Black et al.\"]",
      "is_influential": true
    },
    {
      "source": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "cites",
      "contexts": "[\"Rotary Embeddings [GPTNeo].\", \"There exist some exceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022) and GLM (Zeng et al., 2022), but none that are competitive with PaLM-62B or Chinchilla.\", \", 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al.\", \"\\u2026the non-publicly available language models GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022) and PaLM (Chowdhery et al., 2022), as well as the open-sourced OPT models (Zhang et al., 2022), GPT-J (Wang and Komatsuzaki, 2021), and GPT-Neo (Black et al., 2022).\", \", 2022), GPT-J (Wang and Komatsuzaki, 2021), and GPTNeo (Black et al., 2022).\", \"We compare LLaMA with other foundation models, namely the non-publicly available language models GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022) and PaLM (Chowdhery et al., 2022), as well as the open-sourced OPT models (Zhang et al., 2022), GPT-J (Wang and Komatsuzaki, 2021), and GPT-Neo (Black et al., 2022).\"]",
      "is_influential": true
    },
    {
      "source": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "cites",
      "contexts": "[\"We evaluate our models on two mathematical reasoning benchmarks: MATH (Hendrycks et al., 2021) and GSM8k (Cobbe et al., 2021).\", \"On GSM8k, we observe that LLaMA-65B outperforms Minerva-62B, although it has not been fine-tuned on mathematical data.\", \"In HumanEval, it also receives a function signature, and the prompt is formatted as natural code with the textual description and tests in a MATH + maj1@k GSM8k + maj1@k PaLM 8B docstring.\", \"GSM8k is a set of middle school mathematical problems.\"]",
      "is_influential": true
    },
    {
      "source": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "target": "77d956cdab4508d569ae5741549b78e715fd0749",
      "type": "cites",
      "contexts": "[\"Lin et al. (2021) consider the definition of \\u201ctrue\\u201d in the sense of \\u201cliteral truth about the real world\\u201d, and not claims that are only true in the context of a belief system or tradition.\", \"TruthfulQA (Lin et al., 2021) aims to measure the truthfulness of a model, i.\", \"TruthfulQA (Lin et al., 2021) aims to measure the truthfulness of a model, i.e., its ability to identify when a claim is true.\"]",
      "is_influential": true
    },
    {
      "source": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "target": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "cites",
      "contexts": "[\"Params HumanEval MBPP pass@ @1 @100 @1 @80 LaMDA 137B 14.0 47.\", \"We evaluate the ability of our models to write code from a natural language description on two benchmarks: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).\", \"LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer.\"]",
      "is_influential": true
    },
    {
      "source": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "target": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "type": "cites",
      "contexts": "[\"We evaluate our models on two mathematical reasoning benchmarks: MATH (Hendrycks et al., 2021) and GSM8k (Cobbe et al., 2021).\", \"In HumanEval, it also receives a function signature, and the prompt is formatted as natural code with the textual description and tests in a MATH + maj1@k GSM8k + maj1@k PaLM 8B docstring.\", \"MATH is a dataset of 12K middle school and high school mathematics problems written in LaTeX.\", \"We evaluate our models on two mathematical reasoning benchmarks: MATH (Hendrycks et al., 2021) and GSM8k (Cobbe et al.\"]",
      "is_influential": true
    },
    {
      "source": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "target": "399e7d8129c60818ee208f236c8dda17e876d21f",
      "type": "cites",
      "contexts": "[\", 2019), and to generate toxic or offensive content (Gehman et al., 2020).\", \", 2022) have considered the RealToxicityPrompts benchmark (Gehman et al., 2020) as an indicator of how toxic is their model.\", \"Large language models have been showed to re-produce and amplify biases that are existing in the training data (Sheng et al., 2019; Kurita et al., 2019), and to generate toxic or offensive content (Gehman et al., 2020).\", \"Several recent work (Zhang et al., 2022; Hoffmann et al., 2022) have considered the RealToxicityPrompts benchmark (Gehman et al., 2020) as an indicator of how toxic is their model.\"]",
      "is_influential": true
    },
    {
      "source": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "cites",
      "contexts": "[\"We compare LLaMA to existing large language models on two closed-book question answering benchmarks: Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al.\", \"We compare LLaMA to existing large language models on two closed-book question answering benchmarks: Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017).\", \"Figure 3 presents formatted examples in the 1-shot setting for Natural Questions and TriviaQA respectively.\", \"We evaluate LLaMA on Natural Questions and TriviaQA.\", \"For Natural Questions we use the test split used for open-domain question answering containing 3610 questions.\"]",
      "is_influential": true
    },
    {
      "source": "2113243762",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "2113243762",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2113243762",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "46183616",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "46183616",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "46183616",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "46183616",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "1410231361",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "1490887583",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "1490887583",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1490887583",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "114952298",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "114952298",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "114952298",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "114952298",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "47733973",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "47733973",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "47733973",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "39589154",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "39589154",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "39589154",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "39589154",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "2072738644",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "2209986197",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "2166043087",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "2166043087",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2166043087",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2319608",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "3024698",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "1830914",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "authored"
    },
    {
      "source": "1830914",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "1830914",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "cites",
      "contexts": "[\"To highlight the versatility of our upcycling algorithm, for Large and XL models, we instead begin all experiments from of\\ufb01cial T5 1.1 checkpoints (Narang et al., 2021; Roberts et al., 2022).\"]",
      "is_influential": true
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
      "type": "cites",
      "contexts": "[\"For language, similar ablations (Figure 8) shows that Expert Choice routing outperforms both Top-2 routing (with BPR) and switch (Top-1) routing, on a train time basis.\", \"MoE Vision Transfomers (\\u201cV-MoE\\u201d) models are trained broadly following the protocol of Riquelme et al. (2021).\", \"\\u2026there has been a growing number of MoE works achieving state-of-the-art quality and remarkable ef\\ufb01ciency gains on both language and vision tasks (Lepikhin et al., 2021; Fedus et al., 2022; Riquelme et al., 2021; Artetxe et al., 2021; Du et al., 2021; Zoph et al., 2022; Mustafa et al., 2022).\", \"There are two key changes compared to prior works which used this method (Dosovitskiy et al., 2021; Riquelme et al., 2021): \\u2022 Multiple seeds.\", \"The resultant MoEs broadly follow Vision MoE Transfomers (\\u201cV-MoE\\u201d) (Riquelme et al., 2021), with two differences; we perform global average pooling (Zhai et al., 2022) and use Expert Choice routing.\", \"For vision, Appendix B.1 shows that although Top-K routing, with Batch Prioritized Routing (BPR) (Riquelme et al., 2021), matches Expert Choice routing performance on a per step basis, as it is slightly slower, Top-K routing underperforms Expert Choice routing on a per train time basis.\"]",
      "is_influential": true
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "2a805d0e1b067444a554c5169d189fa1f649f411",
      "type": "cites",
      "contexts": "[\", 2021), with two differences; we perform global average pooling (Zhai et al., 2022) and use Expert Choice routing.\", \"We train with Adafactor (Shazeer & Stern, 2018), and decoupled weight decay (magnitude 3 on head and 0.03 on body) following Zhai et al. (2022).\", \"\\u2026(Devlin et al., 2019) to GPT-3 (Brown et al., 2020) to PaLM (Chowdhery et al., 2022) in natural language processing, or from AlexNet (Krizhevsky et al., 2017) to ViT-G (Zhai et al., 2022) in vision, break-throughs in performance have been obtained from larger hardware, datasets, and architectures.\", \"The resultant MoEs broadly follow Vision MoE Transfomers (\\u201cV-MoE\\u201d) (Riquelme et al., 2021), with two differences; we perform global average pooling (Zhai et al., 2022) and use Expert Choice routing.\", \", 2017) to ViT-G (Zhai et al., 2022) in vision, breakthroughs in performance have been obtained from larger hardware, datasets, and architectures.\", \"From BERT (Devlin et al., 2019) to GPT-3 (Brown et al., 2020) to PaLM (Chowdhery et al., 2022) in natural language processing, or from AlexNet (Krizhevsky et al., 2017) to ViT-G (Zhai et al., 2022) in vision, break-throughs in performance have been obtained from larger hardware, datasets, and architectures.\"]",
      "is_influential": true
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "cites",
      "contexts": "[\"To highlight the versatility of our upcycling algorithm, for Large and XL models, we instead begin all experiments from official T5 1.1 checkpoints (Narang et al., 2021; Roberts et al., 2022).\", \"For finetuning on SuperGLUE, we generally adopt the conventional setup (Raffel et al., 2020; Narang et al., 2021) where we finetune on all SuperGLUE tasks, in a proportional mix, for 200K steps with a batch size of 128.\", \"We apply the same sparse upcycling recipe to both language and vision tasks, focusing on the T5 (encoder-decoder) (Raffel et al., 2020; Narang et al., 2021) and Vision Transformer (encoder) (Dosovitskiy et al., 2021) architectures, respectively.\", \"For finetuning Dense models on SuperGLUE, we use Adafactor with the default, constant learning rate of 10\\u22123 and a dropout rate of 0.1 (Raffel et al., 2020; Narang et al., 2021).\", \"We apply the same sparse upcycling recipe to both language and vision tasks, focusing on the T5 (encoder-decoder) (Raffel et al., 2020; Narang et al., 2021) and Vision Transformer (encoder) (Dosovitskiy et al.\", \"To highlight the versatility of our upcycling algorithm, for Large and XL models, we instead begin all experiments from official T5 1.1 checkpoints (Narang et al., 2021).4\\nWe use the same hyperparameters for the upcycled model as for the corresponding dense model that we initialized from,\\u2026\"]",
      "is_influential": true
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "fdacf2a732f55befdc410ea927091cad3b791f13",
      "type": "cites",
      "contexts": "[\"For Top-2 routing (in the decoder) we include an auxiliary MoE loss, with scaling factor 0.01, to ensure tokens are distributed more evenly across all experts in the decoder (Shazeer et al., 2017; Fedus et al., 2022).\", \"For finetuning upcycled models, because there are many more parameters, it can be helpful to increase the dropout rate for the experts (Fedus et al., 2022), while using the default dropout rate of 0.\", \"01, to ensure tokens are distributed more evenly across all experts in the decoder (Shazeer et al., 2017; Fedus et al., 2022).\", \"Sparsely activated Mixture-of-Experts (MoE) models are an accelerator friendly family of sparse models that allow training of models with up to trillions of parameters (Shazeer et al., 2017; Fedus et al., 2022).\", \"\\u2026there has been a growing number of MoE works achieving state-of-the-art quality and remarkable ef\\ufb01ciency gains on both language and vision tasks (Lepikhin et al., 2021; Fedus et al., 2022; Riquelme et al., 2021; Artetxe et al., 2021; Du et al., 2021; Zoph et al., 2022; Mustafa et al., 2022).\", \"For \\ufb01netuning upcycled models, because there are many more parameters, it can be helpful to increase the dropout rate for the experts (Fedus et al., 2022), while using the default dropout rate of 0 .\", \"See also (Fedus et al., 2022) for a more detailed discussion of these three parallelization strategies.\", \"Recently, there has been a growing number of MoE works achieving state-of-the-art quality and remarkable efficiency gains on both language and vision tasks (Lepikhin et al., 2021; Fedus et al., 2022; Riquelme et al., 2021; Artetxe et al., 2021; Du et al., 2021; Zoph et al., 2022; Mustafa et al., 2022).\"]",
      "is_influential": true
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "cites",
      "contexts": "[\"Few-shot transfer follows Dosovitskiy et al. (2021), whereby a least-squares regressor predicts one-hot classes given frozen image representations.\", \"Dense ViT models are pretrained on JFT300M Sun et al. (2017).\", \"This roughly corresponds to the number of epochs used to train the corresponding variant in ViT (Dosovitskiy et al., 2021).\", \"There are two key changes compared to prior works which used this method (Dosovitskiy et al., 2021; Riquelme et al., 2021): \\u2022 Multiple seeds.\", \"For example, increasing the performance of ViT-B/16 by at least 1% on ImageNet 10-shot requires an additional 58% extra training time (relative to the original checkpoint) if we continue training the dense model; however, it only takes 13% extra training time with the upcycled version.\", \"We apply the same sparse upcycling recipe to both language and vision tasks, focusing on the T5 (encoder-decoder) (Raffel et al., 2020; Narang et al., 2021) and Vision Transformer (encoder) (Dosovitskiy et al., 2021) architectures, respectively.\", \"In experiments on Vision Transformers (Dosovitskiy et al., 2021) and T5 language models (Raffel et al., 2020), we show that upcycling is highly effective when the computation budget lies between +10% and +60% of the cost to train the original (dense) network.\", \"Vision Transformers (ViT) are encoder-only Transformer architectures (Liu et al., 2021; Radford et al., 2021; Touvron et al., 2021; He et al., 2022) which tokenize and embed images.\", \"The fewshot evaluation poses classi\\ufb01cation as a linear regression task, where inputs are frozen representations computed by a pretrained model, and outputs are one-hot vectors representing the ground truth (Dosovitskiy et al., 2021).\", \"From BERT (Devlin et al., 2019) to GPT-3 (Brown et al., 2020) to PaLM (Chowdhery et al., 2022) in natural language processing, or from AlexNet (Krizhevsky et al., 2017) to ViT-G (Zhai et al., 2022) in vision, break-throughs in performance have been obtained from larger hardware, datasets, and architectures.\"]",
      "is_influential": true
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "1882f194cb43828852cc052887671e55a80f945a",
      "type": "cites",
      "contexts": "[\"Recently, there has been a growing number of MoE works achieving state-of-the-art quality and remarkable efficiency gains on both language and vision tasks (Lepikhin et al., 2021; Fedus et al., 2022; Riquelme et al., 2021; Artetxe et al., 2021; Du et al., 2021; Zoph et al., 2022; Mustafa et al., 2022).\", \"Based on our ablation in Section 4.2.2 and prevailing conventions in the MoE literature (Lepikhin et al., 2021), unless otherwise speci\\ufb01ed, we replace half of the MLP layers in our upcycled models with MoE layers.\", \"\\u2026there has been a growing number of MoE works achieving state-of-the-art quality and remarkable ef\\ufb01ciency gains on both language and vision tasks (Lepikhin et al., 2021; Fedus et al., 2022; Riquelme et al., 2021; Artetxe et al., 2021; Du et al., 2021; Zoph et al., 2022; Mustafa et al., 2022).\", \"2 and prevailing conventions in the MoE literature (Lepikhin et al., 2021), unless otherwise specified, we replace half of the MLP layers in our upcycled models with MoE layers.\"]",
      "is_influential": true
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "type": "cites",
      "contexts": "[\"Our language experiments follow the setup of Raffel et al. (2020): we pretrain using the span corruption task on the English C4 dataset (Raffel et al., 2020) and \\ufb01netune on a proportional mix of all SuperGLUE (Wang et al., 2019) tasks simultaneously.\", \"For \\ufb01netuning on SuperGLUE, we generally adopt the conventional setup (Raffel et al., 2020; Narang et al., 2021) where we \\ufb01netune on all SuperGLUE tasks, in a proportional mix, for 200K steps with a batch size of 128.\", \"For finetuning on SuperGLUE, we generally adopt the conventional setup (Raffel et al., 2020; Narang et al., 2021) where we finetune on all SuperGLUE tasks, in a proportional mix, for 200K steps with a batch size of 128.\", \"We experiment with the T5 (Raffel et al., 2020) encoder-decoder as our archetypal language model.\", \"(2020): we pretrain using the span corruption task on the English C4 dataset (Raffel et al., 2020) and finetune on a proportional mix of all SuperGLUE (Wang et al.\", \"1 (Raffel et al., 2020; Narang et al., 2021).\", \", 2021) and T5 language models (Raffel et al., 2020), we show that upcycling is highly effective when the computation budget lies between +10% and +60% of the cost to train the original (dense) network.\", \"We apply the same sparse upcycling recipe to both language and vision tasks, focusing on the T5 (encoder-decoder) (Raffel et al., 2020; Narang et al., 2021) and Vision Transformer (encoder) (Dosovitskiy et al., 2021) architectures, respectively.\", \"In experiments on Vision Transformers (Dosovitskiy et al., 2021) and T5 language models (Raffel et al., 2020), we show that upcycling is highly effective when the computation budget lies between +10% and +60% of the cost to train the original (dense) network.\", \"We apply the same sparse upcycling recipe to both language and vision tasks, focusing on the T5 (encoder-decoder) (Raffel et al., 2020; Narang et al., 2021) and Vision Transformer (encoder) (Dosovitskiy et al.\"]",
      "is_influential": true
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "d9f6ada77448664b71128bb19df15765336974a6",
      "type": "cites",
      "contexts": "[\"Our language experiments follow the setup of Raffel et al. (2020): we pretrain using the span corruption task on the English C4 dataset (Raffel et al., 2020) and \\ufb01netune on a proportional mix of all SuperGLUE (Wang et al., 2019) tasks simultaneously.\", \"For \\ufb01netuning Dense models on SuperGLUE, we use Adafactor with the default, constant learning rate of 10 \\u2212 3 and a dropout rate of 0 .\", \"For \\ufb01netuning on SuperGLUE, we generally adopt the conventional setup (Raffel et al., 2020; Narang et al., 2021) where we \\ufb01netune on all SuperGLUE tasks, in a proportional mix, for 200K steps with a batch size of 128.\", \"Similarly, upcycled T5-Large and T5-Base models outperform their dense counterparts by 1.5-2 absolute points on SuperGLUE using 46% and 55% extra training, respectively.\", \"Upstream accuracy after 1M steps was comparable: 70.8% (no normalization) vs 70.7% (normalization), but downstream average scores on SuperGLUE lagged: 79.3% (no normalization) vs 78.8% (normalization).\", \"In our \\ufb01gures, we report the average SuperGLUE accuracy score across 3 runs for each data point.\", \"We report the C4 Validation Token Accuracy, the individual metrics on each SuperGLUE task after \\ufb01netuning, as well as the overall SuperGLUE score, and the extra TPUv4-core-days and ExaFLOPs used, both in absolute and relative (\"]",
      "is_influential": true
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
      "type": "cites",
      "contexts": "[\"We train with Adafactor (Shazeer & Stern, 2018), and decoupled weight decay (magnitude 3 on head and 0.03 on body) following Zhai et al. (2022).\", \"For \\ufb01netuning Dense models on SuperGLUE, we use Adafactor with the default, constant learning rate of 10 \\u2212 3 and a dropout rate of 0 .\", \"We use the Adafactor optimizer with an inverse square root decay and a peak learning rate of 0 .\"]",
      "is_influential": true
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "f2e78a574925486d1f13440f55688bcffde80101",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "6f88133bc591cd964667a626a06debad17775757",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "8d64e47f23d383c4492f93fc17213cdc7ef3ec2a",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "718c86daf8e8e59446a7c14f19e8c06ed4659581",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "a12bc2ddaa3a91edab682f6822e381ce9323e8fc",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "4915538917afdfebbdc97132b6a430497db4fc54",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "target": "ff8a40349db17daaed78def5f192229c3c2e2527",
      "type": "cites",
      "is_influential": false
    },
    {
      "source": "51891020",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "authored"
    },
    {
      "source": "1794202",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "authored"
    },
    {
      "source": "1794202",
      "target": "a12bc2ddaa3a91edab682f6822e381ce9323e8fc",
      "type": "authored"
    },
    {
      "source": "1794202",
      "target": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
      "type": "authored"
    },
    {
      "source": "1405626394",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "authored"
    },
    {
      "source": "1405626394",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2135570886",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "authored"
    },
    {
      "source": "1643737606",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "authored"
    },
    {
      "source": "97947517",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "authored"
    },
    {
      "source": "3226635",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "authored"
    },
    {
      "source": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "target": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "type": "cites",
      "contexts": "[\"Similarly to [Raffel et al., 2019], we use the Adafactor optimizer [Shazeer and Stern, 2018] and an inversesquare-root learning-rate schedule.\", \"Following [Raffel et al., 2019], we use a dropout rate of 0.\", \"Our main departure from [Raffel et al., 2019] is that we use no dropout during pre-training.\", \"Table 1: Heldout-set log-perplexity for Transformer models on the segment-filling task from [Raffel et al., 2019].\", \"2 Pre-Training and Perplexity Results Identically to [Raffel et al., 2019], we pre-train for 524,288 steps on the span-filling objective on the C4 dataset.\", \"Also listed are the inter-run standard deviations measured by [Raffel et al., 2019].\", \"We test the FFN variants we have described on the transfer-learning setup from [Raffel et al., 2019].\", \"1 Model Architecture We use the same code base, model architecture, and training task as the base model from [Raffel et al., 2019].\", \"59 [Raffel et al., 2019] 83.\", \"FFN(x,W1,W2, b1, b2) = max(0, xW1 + b1)W2 + b2 (1) Following the T5 codebase [Raffel et al., 2019] , we use a version with no bias:\"]",
      "is_influential": true
    },
    {
      "source": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "target": "d9f6ada77448664b71128bb19df15765336974a6",
      "type": "cites",
      "contexts": "[\"We then \\ufb01ne-tune each fully-trained model once on an examples-proportional mixture of the Stanford Question-Answering Dataset (SQuAD) [Rajpurkar et al., 2016] and all the language understanding tasks in the GLUE [Wang et al., 2018] and SuperGlue [Wang et al., 2019] benchmarks.\"]",
      "is_influential": false
    },
    {
      "source": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "target": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
      "type": "cites",
      "contexts": "[\"We then \\ufb01ne-tune each fully-trained model once on an examples-proportional mixture of the Stanford Question-Answering Dataset (SQuAD) [Rajpurkar et al., 2016] and all the language understanding tasks in the GLUE [Wang et al., 2018] and SuperGlue [Wang et al., 2019] benchmarks.\"]",
      "is_influential": false
    },
    {
      "source": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "target": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
      "type": "cites",
      "contexts": "[\"Similarly to [Ra\\ufb00el et al., 2019], we use the Adafactor optimizer [Shazeer and Stern, 2018] and an inverse-square-root learning-rate schedule.\"]",
      "is_influential": false
    },
    {
      "source": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "target": "4f57f486adea0bf95c252620a4e8af39232ef8bc",
      "type": "cites",
      "contexts": "[\"\\u2026[Ra\\ufb00el et al., 2019] 1 , we use a version with no bias: Subsequent work has proposed replacing the ReLU with other nonlinear activation functions such as Gaussian Error Linear Units, GELU( x ) = x \\u03a6( x ) [Hendrycks and Gimpel, 2016], and Swish \\u03b2 ( x ) = x\\u03c3 ( \\u03b2x ) [Ramachandran et al., 2017].\"]",
      "is_influential": false
    },
    {
      "source": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "target": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "type": "cites",
      "contexts": "[\"The Transformer [Vaswani et al., 2017] sequence-to-sequence model alternates between multi-head attention, and what it calls \\\"position-wise feed-forward networks\\\" (FFN).\", \"An encoder-decoder transformer model [Vaswani et al., 2017] is trained on a denoising objective of predicting missing text segments, and subsequently \\ufb01ne-tuned on various language understanding tasks.\"]",
      "is_influential": false
    },
    {
      "source": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "target": "88caa4a0253a8b0076176745ebc072864eab66e1",
      "type": "cites",
      "contexts": "[\"2 Gated Linear Units (GLU) and Variants [Dauphin et al., 2016] introduced Gated Linear Units (GLU), a neural network layer de\\ufb01ned as the component-wise product of two linear transformations of the input, one of which is sigmoid-activated.\"]",
      "is_influential": false
    },
    {
      "source": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "target": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb",
      "type": "cites",
      "contexts": "[\"\\u2026[Ra\\ufb00el et al., 2019] 1 , we use a version with no bias: Subsequent work has proposed replacing the ReLU with other nonlinear activation functions such as Gaussian Error Linear Units, GELU( x ) = x \\u03a6( x ) [Hendrycks and Gimpel, 2016], and Swish \\u03b2 ( x ) = x\\u03c3 ( \\u03b2x ) [Ramachandran et al., 2017].\"]",
      "is_influential": false
    },
    {
      "source": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "target": "05dd7254b632376973f3a1b4d39485da17814df5",
      "type": "cites",
      "contexts": "[\"We then \\ufb01ne-tune each fully-trained model once on an examples-proportional mixture of the Stanford Question-Answering Dataset (SQuAD) [Rajpurkar et al., 2016] and all the language understanding tasks in the GLUE [Wang et al., 2018] and SuperGlue [Wang et al., 2019] benchmarks.\"]",
      "is_influential": false
    },
    {
      "source": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "target": "67107f78a84bdb2411053cb54e94fa226eea6d8e",
      "type": "cites",
      "contexts": "[\"A rectified-linear (ReLU) [Glorot et al., 2011] activation function applied between the two linear transformations.\", \"A recti\\ufb01ed-linear (ReLU) [Glorot et al., 2011] activation function applied between the two linear transformations.\"]",
      "is_influential": false
    },
    {
      "source": "1846258",
      "target": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "type": "authored"
    },
    {
      "source": "1846258",
      "target": "fdacf2a732f55befdc410ea927091cad3b791f13",
      "type": "authored"
    },
    {
      "source": "1846258",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "1846258",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "1846258",
      "target": "1882f194cb43828852cc052887671e55a80f945a",
      "type": "authored"
    },
    {
      "source": "1846258",
      "target": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "type": "authored"
    },
    {
      "source": "1846258",
      "target": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
      "type": "authored"
    },
    {
      "source": "1846258",
      "target": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "type": "authored"
    },
    {
      "source": "1846258",
      "target": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
      "type": "authored"
    },
    {
      "source": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "cites",
      "contexts": "[\"BTX [39, 40] trained experts on different tasks and then mixed them.\"]",
      "is_influential": false
    },
    {
      "source": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "cites",
      "contexts": "[\"Notable work in this area includes: Sparse Upcycling : [15] proposed a method for training MoE models from dense checkpoints, demonstrating the ability to expand model capacity while maintaining or improving performance.\", \"Upcycling vs. Dense Continued Training : Following previous works [15], we compare upcycling vs continued training the dense Nemotron 2B model with the same amount of tokens (0.1T) under a similar learning schedule.\", \"Unlike standard MoE upcycling [15] where we can copy the dense MLP weights to MoE experts, granularity reduces the size of every MoE expert.\", \"Upcycling is the approach of converting a trained dense model into an MoE [15].\", \"Most previous work on upcycling either does not provide details into how their models were up-cycled [3], or provides experiments only at a small scale [15].\", \"Upcycling pre-trained dense language models into sparse mixture-of-experts models (referred to as simply \\u2018upcycling\\u2019 in this work) has emerged as an efficient approach to increase model capacity without the need for training from scratch [15, 16, 17, 3].\"]",
      "is_influential": true
    },
    {
      "source": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "target": "de4e7a50d29554c807159ae316cf6487cbfcd2db",
      "type": "cites",
      "contexts": "[\"We used cosine decay and decayed the lr to 1 / 100-th of the pretraining min-lr as done for the original continued training of Nemotron-4 15B [25].\", \"For Nemotron-4 15B experiments, we upcycle on continued training data so that we can compare upcycling against our existing dense continued training result [13, 25].\", \"val loss MMLU (5 shots) Nemotron-4 15B [13] 1.623 59.3 Nemotron-4 15B continued training [25] 1.377 65.\"]",
      "is_influential": true
    },
    {
      "source": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "cites",
      "contexts": "[\"Model : We do all our ablation studies on our Nemotron 2B 6 and Nemotron-4 15B [13] models followed by final results using a larger token count on Nemotron-4 15B. Nemotron 2B is a transformer-based decoder-only language model similar to GPT-2 and 3 [21].\", \"However, an immense amount of compute has been spent on pre-training dense LLMs with only one MLP layer (one expert) [10, 11, 12, 13, 14].\", \"For Nemotron-4 15B experiments, we upcycle on continued training data so that we can compare upcycling against our existing dense continued training result [13, 25].\", \"val loss MMLU (5 shots) Nemotron-4 15B [13] 1.623 59.3 Nemotron-4 15B continued training [25] 1.377 65.\"]",
      "is_influential": true
    },
    {
      "source": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "cites",
      "contexts": "[\"Studies have investigated the impact of the number of experts on model performance, finding that increasing the number of experts leads to improved sample efficiency and faster training, albeit with diminishing returns beyond certain thresholds [20].\", \"However, it has recently been suggested that increasing the number of experts to which a token is routed to, while shrinking the dimension of each expert might be a superior approach [20].\", \"Post Expert Layernorm : Work on finegrained MoE scaling laws [20] recommended adding a layer-norm at the end of MoE layer.\", \"We use the nomenclature proposed in [20] and add another term T to refer to topK.\"]",
      "is_influential": true
    },
    {
      "source": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "cites",
      "contexts": "[\"Top-2 routing is often used with MoE models [3, 5].\", \"This technique is used in Mixtral [3].\", \"5 3 , Mixtral 8x22B [3], DeepSeek-V2 [8] and Qwen2 [9] are MoE models.\", \"Most previous work on upcycling either does not provide details into how their models were up-cycled [3], or provides experiments only at a small scale [15].\", \"The choice of routing algorithm (e.g., top-1 vs top-2) and gating function (e.g., softmax vs sigmoid) has also been examined [30, 31, 32], with Mixtral 8x7B switching softmax and topK in the router [3].\", \"Sparse Mixture of Experts (MoE) models [1] are becoming increasingly popular [2, 3, 4, 5, 6, 7] since they can help achieve better accuracy without a commensurate increase in model training compute.\", \"Upcycling pre-trained dense language models into sparse mixture-of-experts models (referred to as simply \\u2018upcycling\\u2019 in this work) has emerged as an efficient approach to increase model capacity without the need for training from scratch [15, 16, 17, 3].\"]",
      "is_influential": true
    },
    {
      "source": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "target": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
      "type": "cites",
      "contexts": "[\"Top-2 routing is often used with MoE models [3, 5].\", \"Earlier work on MoE routed each token to a very small number of experts (topK = 1 or 2) [19, 5].\", \"Z Loss : We used the same z loss described in ST-MoE [5].\", \"Previous works [5] have shown that a tradeoff with wall clock time rather than compute is a better metric and in such cases topK greater than granularity might make more sense.\", \"Load Balancing Auxiliary Loss : We used the same aux loss as described in ST-MoE [5] and Switch Transformer [19] and experimented with different aux loss coefficients.\", \"Sparse Mixture of Experts (MoE) models [1] are becoming increasingly popular [2, 3, 4, 5, 6, 7] since they can help achieve better accuracy without a commensurate increase in model training compute.\"]",
      "is_influential": true
    },
    {
      "source": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "target": "fdacf2a732f55befdc410ea927091cad3b791f13",
      "type": "cites",
      "contexts": "[\"Earlier work on MoE routed each token to a very small number of experts (topK = 1 or 2) [19, 5].\", \"Load Balancing Auxiliary Loss : We used the same aux loss as described in ST-MoE [5] and Switch Transformer [19] and experimented with different aux loss coefficients.\", \"The standard MoE router formulation [1, 19] performs a softmax on router logits followed by a topK (softmax-then-topK).\", \"Switch Trans-former [19] simplified the MoE architecture by using a top-1 routing mechanism and demonstrated the ability to scale to trillion-parameter models.\"]",
      "is_influential": true
    },
    {
      "source": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "cites",
      "contexts": "[\"Research has explored various aspects of expert specialization and routing mechanisms in MoE models [28, 29].\"]",
      "is_influential": false
    },
    {
      "source": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "target": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "cites",
      "contexts": "[\"BTX [39, 40] trained experts on different tasks and then mixed them.\"]",
      "is_influential": false
    },
    {
      "source": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "cites",
      "contexts": "[\"However, an immense amount of compute has been spent on pre-training dense LLMs with only one MLP layer (one expert) [10, 11, 12, 13, 14].\"]",
      "is_influential": false
    },
    {
      "source": "2325149690",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "authored"
    },
    {
      "source": "31452163",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "authored"
    },
    {
      "source": "3283879",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "authored"
    },
    {
      "source": "3111334",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "authored"
    },
    {
      "source": "2325199465",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "authored"
    },
    {
      "source": "2325200012",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "authored"
    },
    {
      "source": "2325206494",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "authored"
    },
    {
      "source": "1491319211",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "authored"
    },
    {
      "source": "1491319211",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "1911755",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "authored"
    },
    {
      "source": "1911755",
      "target": "de4e7a50d29554c807159ae316cf6487cbfcd2db",
      "type": "authored"
    },
    {
      "source": "1911755",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2264406909",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "authored"
    },
    {
      "source": "2264406909",
      "target": "de4e7a50d29554c807159ae316cf6487cbfcd2db",
      "type": "authored"
    },
    {
      "source": "2264406909",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2037383772",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2037383772",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2280666145",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "3458736",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2315302377",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2146964035",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "48872685",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2257597409",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2158819969",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "3385516",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2052363815",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2052363815",
      "target": "deb3fccb5b68041ea0f5c724f08b2d840fa51eaa",
      "type": "authored"
    },
    {
      "source": "2261456046",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2259924223",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2166136235",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2264248042",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "30051202",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2127066887",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2257002316",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2288469507",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2111313627",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2257020375",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2290483619",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2303396379",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2284268811",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "2264251662",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "authored"
    },
    {
      "source": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "target": "50650e66ce7c454595862aac70c2a3e9dde23387",
      "type": "cites",
      "contexts": "[\"\\u2026performance of human preference alignment and instruction following by evaluating on benchmarks including MT-Bench (Zheng et al., 2023), Arena-Hard (Li et al., 2024), AlignBench (Liu et al., 2023b), MixEval (Ni et al., 2024) whose results approximate those of Chatbot Arena, and IFEval (Zhou et\\u2026\", \"The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench.\", \"Qwen2 outperforms competing models in evaluations of both fundamental language capabilities and instruction-tuned functionalities Specifically, Qwen2-72B-Instruct, our instruction-tuned variant, scores 9.1 on MT-Bench (Zheng et al., 2023), 48.1 on Arena-Hard (Chiang et al., 2024), and 35.7 on LiveCodeBench (Jain et al., 2024).\", \"Additionally, we assess the performance of human preference alignment and instruction following by evaluating on benchmarks including MT-Bench (Zheng et al., 2023), Arena-Hard (Li et al., 2024), AlignBench (Liu et al., 2023b), MixEval (Ni et al., 2024) whose results approximate those of Chatbot Arena, and IFEval (Zhou et al., 2023) 4 for instruction following.\", \"This methodology is also applicable to assessing instruction following (Dong et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "cites",
      "contexts": "[\"Qwen2-7B-Instruct achieves parity with ChatGLM4-9B-1M, albeit with a more noticeable decline at extended contexts.\", \"Moreover, Qwen2-72B-Instruct demonstrates strong performance, with an accuracy reduction of just 6 points, compared to ChatGLM4-9B-1M, which shows a more pronounced decline of 11 points, particularly given its lower initial accuracy.\", \"Qwen2-7B-Instruct surpasses ChatGLM4-9B-1M (Zeng et al., 2024), which claims a 1M context length.\"]",
      "is_influential": true
    },
    {
      "source": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "target": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "cites",
      "contexts": "[\"The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench.\", \"\\u2026in evaluations of both fundamental language capabilities and instruction-tuned functionalities Specifically, Qwen2-72B-Instruct, our instruction-tuned variant, scores 9.1 on MT-Bench (Zheng et al., 2023), 48.1 on Arena-Hard (Chiang et al., 2024), and 35.7 on LiveCodeBench (Jain et al., 2024).\", \"Qwen2 outperforms competing models in evaluations of both fundamental language capabilities and instruction-tuned functionalities Specifically, Qwen2-72B-Instruct, our instruction-tuned variant, scores 9.1 on MT-Bench (Zheng et al., 2023), 48.1 on Arena-Hard (Chiang et al., 2024), and 35.7 on LiveCodeBench (Jain et al., 2024).\", \"Specifically, we evaluate on MMLU, MMLU-Pro, GPQA, and Theorem QA for language understanding and knowledge, HumanEval, MBPP, MultiPL-E, and LiveCodeBench v1 (Jain et al., 2024) for coding, GSM8K and MATH for mathematics.\"]",
      "is_influential": true
    },
    {
      "source": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "cites",
      "contexts": "[\"In terms of the largest model of Qwen2, we compare Qwen2-72B with competitive baseline open-weight models, including Mixtral-8x22B (Jiang et al., 2024), Llama-3-70B (AI@Meta, 2024), as well as Qwen1.\", \"Mixtral-8x22B Llama-3-70B Qwen1.\", \"5-32B, as well as MoE baselines: Mixtral-8x7B and Jamba.\", \"It can be observed that Qwen2-72B-Instruct performs better than the proprietary model, GPT-4, and significantly outperforms the open-weight model, Mixtral-8x22B-Instruct.\", \"We compare Qwen2-72B with the baselines, including Mixtral-8x22B, Llama-3-70B, Qwen1.\", \"5-110B Qwen2-72B Qwen2-57B-A14B-Instruct For medium-size models, we compare Qwen2-57B-A14B-Instruct with Mixtral-8x7B-Instruct, another MoE baseline, as well as the dense SOTA models with over 30 billion parameters, e.g., Yi-1.\", \"Mixtral-8x7B Yi-1.\", \"We compare Qwen2-72B-Instruct with Mixtral-8x22B-Instruct, Llama-3-70B-Instruct, Qwen1.\", \"Jamba Mixtral-8x7B Yi-1.\", \"For example, transitioning from Mistral-7B (Jiang et al., 2023a) to Mixtral 8x7B (Jiang et al., 2024), involves activating two of the eight experts at a time.\", \"We compare Qwen2-72B-Instruct against the instruction-tuned models including Mixtral-8x22B-Instruct, Llama-3-70B-Instruct, as well as Qwen1.\", \"These baselines include other MoE models, such as Mixtral-8x7B (Jiang et al., 2024) and Jamba (Lieber et al., 2024), and dense models, such as Yi-1.\", \"We compare Qwen2-57B-A14B-Instruct with the similar-size MoE model Mixtral-8x7B-Instruct, 30B dense models such as Yi-1.\", \"In assessing language understanding in Chinese , Qwen2-72B significantly outperforms Mixtral-8x22B and Llama-3-70B, and also outperforms Qwen1.\"]",
      "is_influential": true
    },
    {
      "source": "2311633047",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2257101724",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "151471590",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2312091718",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2249451832",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2249451832",
      "target": "50650e66ce7c454595862aac70c2a3e9dde23387",
      "type": "authored"
    },
    {
      "source": "2302295311",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2302295311",
      "target": "50650e66ce7c454595862aac70c2a3e9dde23387",
      "type": "authored"
    },
    {
      "source": "2257039734",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2257039734",
      "target": "50650e66ce7c454595862aac70c2a3e9dde23387",
      "type": "authored"
    },
    {
      "source": "2311714296",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2248487202",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2248487202",
      "target": "cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "type": "authored"
    },
    {
      "source": "2304136322",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "51490462",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "51490462",
      "target": "50650e66ce7c454595862aac70c2a3e9dde23387",
      "type": "authored"
    },
    {
      "source": "2312183452",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2314068968",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2299550823",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2182966132",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2243424858",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "49365463",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2258670763",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "47793076",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2257108556",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2237981776",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2237981776",
      "target": "cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "type": "authored"
    },
    {
      "source": "2237981776",
      "target": "50650e66ce7c454595862aac70c2a3e9dde23387",
      "type": "authored"
    },
    {
      "source": "41211611",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "41211611",
      "target": "fc6a2f7478f68adefd69e2071f27e38aa1647f2f",
      "type": "authored"
    },
    {
      "source": "2312070548",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "35996608",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2247877609",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2257001403",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2257001403",
      "target": "50650e66ce7c454595862aac70c2a3e9dde23387",
      "type": "authored"
    },
    {
      "source": "2293321194",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2303430522",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2223106060",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2301649171",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2311440332",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2288896179",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2298005769",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2310233042",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "47447639",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "47447639",
      "target": "cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "type": "authored"
    },
    {
      "source": "2311453186",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2248039532",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2311456728",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2247821453",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2110171536",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2248127832",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2309297259",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2311842337",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2311391178",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2249717428",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2141874108",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2274095735",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2294009097",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2312041712",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2312110505",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2234000716",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2153951714",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "29343468",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2244411465",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2256981628",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2248072386",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2116702333",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2232106310",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "authored"
    },
    {
      "source": "2479521",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2369482",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2299944289",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "89942851",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916217",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313924937",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313975817",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "14279694",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "14279694",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2329138320",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2247818297",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918197",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2129047988",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2269467670",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313926281",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918952",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2294453195",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2279336258",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314521400",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313922587",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910288",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2295667288",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313953240",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916233",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2079950350",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "83928755",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313917653",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909658",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913576",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2217959550",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909741",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "103277778",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2268428822",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2273700455",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "66286536",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "66286536",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "66286536",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2273414632",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "51882206",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2273006690",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909437",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918299",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "71039937",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "71039937",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2303390957",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2267338678",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2269456985",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2306842160",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2343773325",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1394834533",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918680",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "31461304",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2268821751",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2708577",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313967211",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2282469774",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314074302",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313919767",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2268397654",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "51888120",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2264339927",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313924900",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314075528",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "103405110",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314125186",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2290402489",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "121929334",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "121929334",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "34921162",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2207049",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2207049",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2267241285",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2264288587",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1805998294",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1805998294",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2314056661",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "50825669",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313917660",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314078634",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "3222225",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313919733",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "35721567",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909388",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2287049560",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2223749565",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2223749565",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2223974989",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2223974989",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "31357678",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314428565",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314080357",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314602001",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314078877",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1749686057",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1749686057",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "90591458",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2149161568",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925205",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313912873",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2273413914",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2211671694",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918589",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "17097160",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918427",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313920868",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2285859430",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2282542714",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1405642252",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2273645788",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2279924280",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925316",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913532",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313915935",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1803520",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314080073",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313924605",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918409",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2249724552",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313912794",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2040305955",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2040305955",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2313925619",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314194315",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2000839712",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2047114741",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "152964870",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2210374",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2059886128",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2059886128",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2313909379",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313912870",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313905186",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2165660870",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2165660870",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2247796743",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2310234768",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2247874378",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314434867",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2911626",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2223756247",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2223756247",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "3444222",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2096643450",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2166310112",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2037772368",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2257643167",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2273574279",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2268221163",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313915931",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "46175439",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2304450089",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2214843767",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2214843767",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2314186827",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313912601",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910187",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925467",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "98804036",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909428",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1962768",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1962768",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "48647153",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "3102850",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913363",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2007285239",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925210",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1722889",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2110697298",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2110697298",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2214818043",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2214818043",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2266467782",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2248766592",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2268759462",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2273416143",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313985129",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2277511475",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2281792543",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2068070",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2068070",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "35557488",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1498636613",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2191455",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2272846244",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2116473",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2116473",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2314071119",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "83754395",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "47505161",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "47505161",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2273415395",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "31460313",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "31460313",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2313909594",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "40895369",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "40895369",
      "target": "464770587aece80cc9e3451050058e30c2aa6666",
      "type": "authored"
    },
    {
      "source": "40895369",
      "target": "399e7d8129c60818ee208f236c8dda17e876d21f",
      "type": "authored"
    },
    {
      "source": "148016419",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925454",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918585",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313917558",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910328",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2073456043",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2073456043",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2073456043",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2313915815",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313914277",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "46907106",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314332514",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "34066479",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2190957318",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2190957318",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2313913380",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918349",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2232955561",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2162195471",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2266751414",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2290750668",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2223742000",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2223742000",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2313913371",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2297930724",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2249851858",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2285798957",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910170",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314067352",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1404341450",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2286511206",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2223764353",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2223764353",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "148416622",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314381758",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2108473229",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2108473229",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2297839847",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2272672481",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2297187212",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2293992938",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2266490735",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "51149919",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2233294011",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2233294011",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2312019070",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313915967",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "116814432",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2077604116",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910256",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913221",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925780",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314067298",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313915843",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910116",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "14667698",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918472",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "122882087",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918666",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925473",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2266838640",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2243192949",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925570",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913152",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313919243",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2282542314",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909933",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1453469113",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918528",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "51912276",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313917455",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925699",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909987",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "35198582",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313915928",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918606",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913272",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2897362",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2319973",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918673",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909983",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "8005713",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314014642",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314069142",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909094",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916282",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "46240090",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918213",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918219",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925572",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313917587",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313919256",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913658",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313917281",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925537",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314071777",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314072280",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313982652",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2290129157",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2263867885",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925773",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2322150",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "50986776",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913237",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "3046707",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916159",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2161835643",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313915190",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2273657478",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2311498203",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2248278031",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2145259939",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2145259939",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2313925798",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2283843884",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "145267619",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913567",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916034",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916009",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925401",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "6072807",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313914699",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913986",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913160",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2064373270",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313915853",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918562",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314197755",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "32653170",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313907929",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2160885118",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "31292058",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2061585840",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "3360115",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2243280567",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913137",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925846",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918570",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916006",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925677",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2028300167",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918648",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2266304177",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "47776500",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2289832027",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2343773236",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313908554",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314725059",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2261827291",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "30279076",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913215",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2297942583",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "87085411",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910892",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2322981055",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2322981055",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2032201719",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2127473751",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2064713742",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2064713742",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2313913125",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913133",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313917982",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909751",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2131867883",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313917933",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314079720",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313924089",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313906372",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "39906022",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "39906022",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2313925697",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314712137",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314696266",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314170063",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313921009",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916920",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918017",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925667",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314554743",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925786",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314072216",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2115598555",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913073",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313917036",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913208",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "40267343",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2158995926",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313926373",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2262920209",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313925800",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314883034",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918835",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1410624139",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314883036",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2197533966",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913092",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313926370",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314073913",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313926731",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313951052",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314460078",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2269696579",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918301",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2331511165",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2072010",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2072010",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2313918577",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2273002871",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2010057",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "98800979",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2093466943",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "83174287",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2248340971",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918566",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2290016941",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "52097509",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313926376",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1727524",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2259912893",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313917797",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314105463",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909990",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "49089678",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314111844",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910746",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314078208",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "147487949",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913313",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "32371083",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910172",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2265554054",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916856",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910535",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918859",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "40943290",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909893",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1746841",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1746841",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2285597551",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910396",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314687456",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916655",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1405690366",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918980",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1778909324",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1729960",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916098",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909999",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313915995",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2307470796",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "31915793",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "14171685",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313919367",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2257254817",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2163709899",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2459737",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "41020300",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916229",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909804",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313934481",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "14714641",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909891",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918294",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2264459586",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313919671",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918488",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913081",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "46886013",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918943",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1410913697",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1905713",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916859",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "19200118",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313913104",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916525",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313919311",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918976",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314108295",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314005184",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314406059",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "48347720",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313926214",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314693028",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2311565327",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2268757277",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916766",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2237076180",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2237101143",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "50230355",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2423558",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2127604",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313919129",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2125208891",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313926357",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313909787",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918283",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2157683980",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2286537482",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313919117",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313914940",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "89754631",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313908725",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2211633",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2211633",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2071335303",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910221",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2189305275",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313919760",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314332791",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1993655237",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "1993655237",
      "target": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "authored"
    },
    {
      "source": "2313919132",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313919289",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313919174",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2273415095",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313918541",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313910202",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313916470",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314056846",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "71203676",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313685593",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "144386035",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2051654054",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2313920446",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2293767405",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "2314069334",
      "target": "40e8af970329135ec95057d73e239dab805ad128",
      "type": "authored"
    },
    {
      "source": "46985523",
      "target": "375b65ae876ccdd6b909ce6bed7d58549c0da216",
      "type": "authored"
    },
    {
      "source": "2874733",
      "target": "375b65ae876ccdd6b909ce6bed7d58549c0da216",
      "type": "authored"
    },
    {
      "source": "117232498",
      "target": "203bfa5e488a9c3100e3d9b9af5ea34537068612",
      "type": "authored"
    },
    {
      "source": "1482544048",
      "target": "203bfa5e488a9c3100e3d9b9af5ea34537068612",
      "type": "authored"
    },
    {
      "source": "1482544048",
      "target": "51f5d1ae89c29d70426bed77680ffab41e1eb945",
      "type": "authored"
    },
    {
      "source": "24026083",
      "target": "203bfa5e488a9c3100e3d9b9af5ea34537068612",
      "type": "authored"
    },
    {
      "source": "2132497979",
      "target": "203bfa5e488a9c3100e3d9b9af5ea34537068612",
      "type": "authored"
    },
    {
      "source": "2064049828",
      "target": "203bfa5e488a9c3100e3d9b9af5ea34537068612",
      "type": "authored"
    },
    {
      "source": "2065288337",
      "target": "203bfa5e488a9c3100e3d9b9af5ea34537068612",
      "type": "authored"
    },
    {
      "source": "143868587",
      "target": "203bfa5e488a9c3100e3d9b9af5ea34537068612",
      "type": "authored"
    },
    {
      "source": "2136025369",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2273631049",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2116426849",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "120688117",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2273584640",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2221032216",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2210636248",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2314692435",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2314692233",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2284031962",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2257095790",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2238905102",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2145977326",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2215167446",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2257302793",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2269508672",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2251076040",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "1742535",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2300922460",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2290203999",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2157449203",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2272672300",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2301126629",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2277750447",
      "target": "5ea16dfb5a44a97a8e93177e7e588cc7fb9e5996",
      "type": "authored"
    },
    {
      "source": "2265621323",
      "target": "d081584960c42f7793502bb496e46f682e3e43b3",
      "type": "authored"
    },
    {
      "source": "2300086932",
      "target": "d081584960c42f7793502bb496e46f682e3e43b3",
      "type": "authored"
    },
    {
      "source": "2300086932",
      "target": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "authored"
    },
    {
      "source": "2165247100",
      "target": "d081584960c42f7793502bb496e46f682e3e43b3",
      "type": "authored"
    },
    {
      "source": "2308040513",
      "target": "d081584960c42f7793502bb496e46f682e3e43b3",
      "type": "authored"
    },
    {
      "source": "2295601",
      "target": "d081584960c42f7793502bb496e46f682e3e43b3",
      "type": "authored"
    },
    {
      "source": "2145784327",
      "target": "d081584960c42f7793502bb496e46f682e3e43b3",
      "type": "authored"
    },
    {
      "source": "2145784327",
      "target": "deeb72a8582a68403082f2da4e7fa6a3c4da26b4",
      "type": "authored"
    },
    {
      "source": "2145784327",
      "target": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "authored"
    },
    {
      "source": "2257550096",
      "target": "d081584960c42f7793502bb496e46f682e3e43b3",
      "type": "authored"
    },
    {
      "source": "2258308833",
      "target": "d081584960c42f7793502bb496e46f682e3e43b3",
      "type": "authored"
    },
    {
      "source": "2264692022",
      "target": "d081584960c42f7793502bb496e46f682e3e43b3",
      "type": "authored"
    },
    {
      "source": "2264692022",
      "target": "deeb72a8582a68403082f2da4e7fa6a3c4da26b4",
      "type": "authored"
    },
    {
      "source": "2264692022",
      "target": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "authored"
    },
    {
      "source": "2279869111",
      "target": "d081584960c42f7793502bb496e46f682e3e43b3",
      "type": "authored"
    },
    {
      "source": "2111511268",
      "target": "51f5d1ae89c29d70426bed77680ffab41e1eb945",
      "type": "authored"
    },
    {
      "source": "2233132826",
      "target": "51f5d1ae89c29d70426bed77680ffab41e1eb945",
      "type": "authored"
    },
    {
      "source": "118237536",
      "target": "727df25246b5156fd2abaf52ba1b47abaeae9d72",
      "type": "authored"
    },
    {
      "source": "2140535196",
      "target": "727df25246b5156fd2abaf52ba1b47abaeae9d72",
      "type": "authored"
    },
    {
      "source": "2087438361",
      "target": "727df25246b5156fd2abaf52ba1b47abaeae9d72",
      "type": "authored"
    },
    {
      "source": "2140530383",
      "target": "727df25246b5156fd2abaf52ba1b47abaeae9d72",
      "type": "authored"
    },
    {
      "source": "3484063",
      "target": "727df25246b5156fd2abaf52ba1b47abaeae9d72",
      "type": "authored"
    },
    {
      "source": "2026684421",
      "target": "9e20f3155d16e13a16f3614a638207d67a7f4a5f",
      "type": "authored"
    },
    {
      "source": "50775285",
      "target": "9e20f3155d16e13a16f3614a638207d67a7f4a5f",
      "type": "authored"
    },
    {
      "source": "3402485",
      "target": "9e20f3155d16e13a16f3614a638207d67a7f4a5f",
      "type": "authored"
    },
    {
      "source": "2152827533",
      "target": "9e20f3155d16e13a16f3614a638207d67a7f4a5f",
      "type": "authored"
    },
    {
      "source": "2116219842",
      "target": "9e20f3155d16e13a16f3614a638207d67a7f4a5f",
      "type": "authored"
    },
    {
      "source": "2087124567",
      "target": "9e20f3155d16e13a16f3614a638207d67a7f4a5f",
      "type": "authored"
    },
    {
      "source": "2063969818",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "2256994781",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "2256994781",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "1697879",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "1697879",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2256994975",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "2256994975",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2328602",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "2328602",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "40550616",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "40550616",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2256995640",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "2256995640",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2256993163",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "2256993163",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2113836860",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "2113836860",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2113836860",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2256995632",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "2256995632",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2256994779",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "2256994779",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "1379806208",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "1379806208",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "1379806208",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2135734748",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "2135734748",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2256992826",
      "target": "db633c6b1c286c0386f0078d8a2e6224e03a6227",
      "type": "authored"
    },
    {
      "source": "2256992826",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "143792623",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "143792623",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2059203763",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2214809450",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2634674",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2023469",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2108267192",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2108267192",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "7153363",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2166312768",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2223748737",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2107063269",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "4305645",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2195458",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2132302721",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2065277797",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2135297476",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "1768032",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "3375249",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "3047561",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "40383658",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "38579672",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "150282885",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "1859294",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "51324296",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2066074360",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2112782199",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "71292072",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2110032535",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "2223770369",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "14701107",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "144270981",
      "target": "104b0bb1da562d53cbda87aec79ef6a2827d191a",
      "type": "authored"
    },
    {
      "source": "144270981",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "144270981",
      "target": "88caa4a0253a8b0076176745ebc072864eab66e1",
      "type": "authored"
    },
    {
      "source": "31793034",
      "target": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
      "type": "authored"
    },
    {
      "source": "49387725",
      "target": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
      "type": "authored"
    },
    {
      "source": "2115903168",
      "target": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
      "type": "authored"
    },
    {
      "source": "2061137049",
      "target": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
      "type": "authored"
    },
    {
      "source": "2064084601",
      "target": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
      "type": "authored"
    },
    {
      "source": "3422872",
      "target": "814a4f680b9ba6baba23b93499f4b48af1a27678",
      "type": "authored"
    },
    {
      "source": "3422872",
      "target": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "type": "authored"
    },
    {
      "source": "3422872",
      "target": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb",
      "type": "authored"
    },
    {
      "source": "90909974",
      "target": "814a4f680b9ba6baba23b93499f4b48af1a27678",
      "type": "authored"
    },
    {
      "source": "90909974",
      "target": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "type": "authored"
    },
    {
      "source": "104444594",
      "target": "814a4f680b9ba6baba23b93499f4b48af1a27678",
      "type": "authored"
    },
    {
      "source": "104444594",
      "target": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "type": "authored"
    },
    {
      "source": "1380103052",
      "target": "814a4f680b9ba6baba23b93499f4b48af1a27678",
      "type": "authored"
    },
    {
      "source": "16787428",
      "target": "814a4f680b9ba6baba23b93499f4b48af1a27678",
      "type": "authored"
    },
    {
      "source": "143711382",
      "target": "814a4f680b9ba6baba23b93499f4b48af1a27678",
      "type": "authored"
    },
    {
      "source": "143711382",
      "target": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "type": "authored"
    },
    {
      "source": "5164568",
      "target": "814a4f680b9ba6baba23b93499f4b48af1a27678",
      "type": "authored"
    },
    {
      "source": "5164568",
      "target": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "type": "authored"
    },
    {
      "source": "123437034",
      "target": "295065d942abca0711300b2b4c39829551060578",
      "type": "authored"
    },
    {
      "source": "145461044",
      "target": "295065d942abca0711300b2b4c39829551060578",
      "type": "authored"
    },
    {
      "source": "24277779",
      "target": "295065d942abca0711300b2b4c39829551060578",
      "type": "authored"
    },
    {
      "source": "7446832",
      "target": "295065d942abca0711300b2b4c39829551060578",
      "type": "authored"
    },
    {
      "source": "3167681",
      "target": "295065d942abca0711300b2b4c39829551060578",
      "type": "authored"
    },
    {
      "source": "48323507",
      "target": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
      "type": "authored"
    },
    {
      "source": "2236429",
      "target": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
      "type": "authored"
    },
    {
      "source": "48229640",
      "target": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
      "type": "authored"
    },
    {
      "source": "72898556",
      "target": "751b04d346fc9a70781bbfea23953f424ff7deec",
      "type": "authored"
    },
    {
      "source": "3323275",
      "target": "d7da009f457917aa381619facfa5ffae9329a6e9",
      "type": "authored"
    },
    {
      "source": "46924970",
      "target": "d7da009f457917aa381619facfa5ffae9329a6e9",
      "type": "authored"
    },
    {
      "source": "144582029",
      "target": "d7da009f457917aa381619facfa5ffae9329a6e9",
      "type": "authored"
    },
    {
      "source": "2587983",
      "target": "d7da009f457917aa381619facfa5ffae9329a6e9",
      "type": "authored"
    },
    {
      "source": "2301055584",
      "target": "7235707434df77b0469e6da21e93a27f250870eb",
      "type": "authored"
    },
    {
      "source": "2264184691",
      "target": "7235707434df77b0469e6da21e93a27f250870eb",
      "type": "authored"
    },
    {
      "source": "1402296287",
      "target": "7235707434df77b0469e6da21e93a27f250870eb",
      "type": "authored"
    },
    {
      "source": "2261394820",
      "target": "7235707434df77b0469e6da21e93a27f250870eb",
      "type": "authored"
    },
    {
      "source": "2325501478",
      "target": "7235707434df77b0469e6da21e93a27f250870eb",
      "type": "authored"
    },
    {
      "source": "2273093960",
      "target": "7235707434df77b0469e6da21e93a27f250870eb",
      "type": "authored"
    },
    {
      "source": "2328092058",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "1977806",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "1964382",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2293772267",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089416",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2276823919",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "1557541414",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088295",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2297773170",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328092088",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088996",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088630",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088972",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "38967461",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089001",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086314",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2288154316",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2304796773",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086873",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088190",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328077794",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088272",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2286810186",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2354728",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089401",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086274",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086861",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "3609856",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086268",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088910",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328091197",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087875",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328085045",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088314",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "47475538",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086263",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087747",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086866",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088700",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "100549595",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088963",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087787",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328306274",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088277",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088135",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089502",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "3077501",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086363",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087880",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086892",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "47757859",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086506",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089025",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328082349",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2162465097",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088805",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088258",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328262416",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328601582",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087829",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328928508",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328273156",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328272761",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086878",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328110341",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328340618",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088908",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2231409794",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089763",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328080408",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328090011",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2319226404",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328312366",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088056",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088261",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2754804",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328108199",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "152842363",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086940",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087781",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328091414",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "1659172809",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328550909",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086947",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088981",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328386420",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2297774080",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2165941083",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2325158061",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088811",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088712",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328106205",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2285927942",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "11948938",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328107429",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088679",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2168285763",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086469",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2290040262",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328104851",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328340186",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088790",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2307453621",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2282935403",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328338413",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "24643287",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088813",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2561924",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089393",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "102707868",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2000906",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2708454",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088993",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2325157923",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2187579768",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088092",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088895",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2309210464",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275788749",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328244583",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2241609611",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088965",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2256302782",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089547",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328106487",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088802",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328319492",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328261167",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2329018321",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328247147",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "3177568",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086498",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089222",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2297873691",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088675",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087850",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "9960452",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087765",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328081248",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089934",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2318703608",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328078075",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328108630",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089008",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086333",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328370280",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328110126",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088657",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088401",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328090052",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328346829",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275176580",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2325207065",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087131",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2254898448",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086322",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089844",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089080",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086398",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328092675",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2260978400",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087145",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328113403",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086391",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2325162875",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089100",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328111423",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088792",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328457059",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2320728017",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2099682093",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328107693",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "3193064",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328328041",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2272599912",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328090291",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328090151",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328280032",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089159",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328258887",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328959571",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "3308772",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2305476965",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328090097",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2275287759",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "35163402",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328092228",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089085",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328824976",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2265097787",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328091362",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086997",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089777",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "3365851",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086984",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089359",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2325154459",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2133298112",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086444",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086448",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087770",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "7624658",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086961",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2325151487",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328111449",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086281",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328113242",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087286",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088204",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328367058",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089338",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086386",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089793",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087257",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088078",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2316290883",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328288671",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328114240",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2304955112",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2329125040",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2314850132",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2105843952",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "7987799",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089012",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089937",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328260600",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087260",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088979",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328377365",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089004",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2073442003",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086978",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328158635",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2093346446",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2062757640",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2329885104",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086378",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088524",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088976",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086279",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088881",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089201",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087278",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088626",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328337411",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328078049",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "52254851",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088969",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328282767",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087320",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328107017",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328268724",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "52010041",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "14973749",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087115",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "90169542",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087230",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2379991",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2259739",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087314",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089683",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2266396476",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087307",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088645",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328983284",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088989",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328087310",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328089157",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2295667407",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328252793",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2329138264",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328485051",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "100528996",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328088471",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2307452791",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086527",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2330153952",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328255074",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328092537",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328256460",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328616992",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2328086974",
      "target": "7943ec4a67151a559b25cd34369e661c9a7924c8",
      "type": "authored"
    },
    {
      "source": "2423429",
      "target": "deb3fccb5b68041ea0f5c724f08b2d840fa51eaa",
      "type": "authored"
    },
    {
      "source": "2237944445",
      "target": "deb3fccb5b68041ea0f5c724f08b2d840fa51eaa",
      "type": "authored"
    },
    {
      "source": "2262217080",
      "target": "deb3fccb5b68041ea0f5c724f08b2d840fa51eaa",
      "type": "authored"
    },
    {
      "source": "2112504145",
      "target": "deb3fccb5b68041ea0f5c724f08b2d840fa51eaa",
      "type": "authored"
    },
    {
      "source": "2273918810",
      "target": "deb3fccb5b68041ea0f5c724f08b2d840fa51eaa",
      "type": "authored"
    },
    {
      "source": "2257385142",
      "target": "deb3fccb5b68041ea0f5c724f08b2d840fa51eaa",
      "type": "authored"
    },
    {
      "source": "46217681",
      "target": "deb3fccb5b68041ea0f5c724f08b2d840fa51eaa",
      "type": "authored"
    },
    {
      "source": "151088535",
      "target": "03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f",
      "type": "authored"
    },
    {
      "source": "49889860",
      "target": "03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f",
      "type": "authored"
    },
    {
      "source": "2269778420",
      "target": "03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f",
      "type": "authored"
    },
    {
      "source": "51011000",
      "target": "03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f",
      "type": "authored"
    },
    {
      "source": "2267488244",
      "target": "03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f",
      "type": "authored"
    },
    {
      "source": "2269733338",
      "target": "03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f",
      "type": "authored"
    },
    {
      "source": "2260456251",
      "target": "03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f",
      "type": "authored"
    },
    {
      "source": "2275638250",
      "target": "03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f",
      "type": "authored"
    },
    {
      "source": "2148631790",
      "target": "54e14b407d246a73d0af8f22ed67c363d3aa6f54",
      "type": "authored"
    },
    {
      "source": "2261082206",
      "target": "54e14b407d246a73d0af8f22ed67c363d3aa6f54",
      "type": "authored"
    },
    {
      "source": "2067996",
      "target": "54e14b407d246a73d0af8f22ed67c363d3aa6f54",
      "type": "authored"
    },
    {
      "source": "2067996",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "120419790",
      "target": "54e14b407d246a73d0af8f22ed67c363d3aa6f54",
      "type": "authored"
    },
    {
      "source": "2257233215",
      "target": "54e14b407d246a73d0af8f22ed67c363d3aa6f54",
      "type": "authored"
    },
    {
      "source": "1486307451",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2149890773",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "1978097132",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "119609682",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2111073313",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2142833890",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "1943097969",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "30176974",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2081806483",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2081806483",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "103143311",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2117706920",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "148070327",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "148070327",
      "target": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "type": "authored"
    },
    {
      "source": "1583434563",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2154608209",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "1403602266",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2866708",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "1573482302",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "39182747",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2162194147",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2154610174",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "49604482",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2154608229",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2154608229",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2051128902",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2061321863",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2698777",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "31035595",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "31035595",
      "target": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
      "type": "authored"
    },
    {
      "source": "2115193883",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "52238703",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "37232298",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2056658938",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2053807409",
      "target": "0286b2736a114198b25fb5553c671c33aed5d477",
      "type": "authored"
    },
    {
      "source": "2053807409",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "145791315",
      "target": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
      "type": "authored"
    },
    {
      "source": "26890260",
      "target": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
      "type": "authored"
    },
    {
      "source": "34313265",
      "target": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
      "type": "authored"
    },
    {
      "source": "2330246606",
      "target": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
      "type": "authored"
    },
    {
      "source": "88726969",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2335860974",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2066900445",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "41152329",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2335860956",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2336037417",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2335857971",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2336035162",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2335860886",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2335859860",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2335859695",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2316350011",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "1976174397",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2190111475",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "98366006",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2335857516",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2335858715",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2336243144",
      "target": "fb22403ae112aa61756fce040473dd4c8b1fbddb",
      "type": "authored"
    },
    {
      "source": "2324074296",
      "target": "deeb72a8582a68403082f2da4e7fa6a3c4da26b4",
      "type": "authored"
    },
    {
      "source": "2310658071",
      "target": "deeb72a8582a68403082f2da4e7fa6a3c4da26b4",
      "type": "authored"
    },
    {
      "source": "2310709478",
      "target": "deeb72a8582a68403082f2da4e7fa6a3c4da26b4",
      "type": "authored"
    },
    {
      "source": "2310709478",
      "target": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "authored"
    },
    {
      "source": "2257135061",
      "target": "deeb72a8582a68403082f2da4e7fa6a3c4da26b4",
      "type": "authored"
    },
    {
      "source": "2315193840",
      "target": "deeb72a8582a68403082f2da4e7fa6a3c4da26b4",
      "type": "authored"
    },
    {
      "source": "2315193840",
      "target": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "authored"
    },
    {
      "source": "2325209062",
      "target": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "authored"
    },
    {
      "source": "2310650738",
      "target": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "authored"
    },
    {
      "source": "2310758205",
      "target": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "authored"
    },
    {
      "source": "2267467406",
      "target": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "authored"
    },
    {
      "source": "2315071527",
      "target": "1a71f7b216b710b936da666027014adb83af8e7a",
      "type": "authored"
    },
    {
      "source": "2297768601",
      "target": "81002626b9eeb896f2adcdc838a4451d9001cdb3",
      "type": "authored"
    },
    {
      "source": "2297769955",
      "target": "81002626b9eeb896f2adcdc838a4451d9001cdb3",
      "type": "authored"
    },
    {
      "source": "2167332264",
      "target": "81002626b9eeb896f2adcdc838a4451d9001cdb3",
      "type": "authored"
    },
    {
      "source": "2688928",
      "target": "81002626b9eeb896f2adcdc838a4451d9001cdb3",
      "type": "authored"
    },
    {
      "source": "2118046679",
      "target": "aa66639895a7dfce1e229293e546686912ba8320",
      "type": "authored"
    },
    {
      "source": "2266425853",
      "target": "aa66639895a7dfce1e229293e546686912ba8320",
      "type": "authored"
    },
    {
      "source": "2316960542",
      "target": "aa66639895a7dfce1e229293e546686912ba8320",
      "type": "authored"
    },
    {
      "source": "2284915404",
      "target": "aa66639895a7dfce1e229293e546686912ba8320",
      "type": "authored"
    },
    {
      "source": "2146373626",
      "target": "cf4fac3510b0e4fdab47fa7495c18ad32dfefde0",
      "type": "authored"
    },
    {
      "source": "2256011160",
      "target": "cf4fac3510b0e4fdab47fa7495c18ad32dfefde0",
      "type": "authored"
    },
    {
      "source": "2561045",
      "target": "cf4fac3510b0e4fdab47fa7495c18ad32dfefde0",
      "type": "authored"
    },
    {
      "source": "1576157233",
      "target": "21a6bd63a062acab5e738db4c593891060df6205",
      "type": "authored"
    },
    {
      "source": "2256473222",
      "target": "21a6bd63a062acab5e738db4c593891060df6205",
      "type": "authored"
    },
    {
      "source": "2196915039",
      "target": "23c8d86963639daf04bc6518c1731eafc85bdef1",
      "type": "authored"
    },
    {
      "source": "2195848881",
      "target": "23c8d86963639daf04bc6518c1731eafc85bdef1",
      "type": "authored"
    },
    {
      "source": "3054954",
      "target": "23c8d86963639daf04bc6518c1731eafc85bdef1",
      "type": "authored"
    },
    {
      "source": "144802290",
      "target": "e17aac3bbe0aabda715cafad392f31a1e046c17c",
      "type": "authored"
    },
    {
      "source": "2067821698",
      "target": "e17aac3bbe0aabda715cafad392f31a1e046c17c",
      "type": "authored"
    },
    {
      "source": "31726556",
      "target": "e17aac3bbe0aabda715cafad392f31a1e046c17c",
      "type": "authored"
    },
    {
      "source": "2323449745",
      "target": "fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "type": "authored"
    },
    {
      "source": "2323372219",
      "target": "fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "type": "authored"
    },
    {
      "source": "2323375681",
      "target": "fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "type": "authored"
    },
    {
      "source": "2323373523",
      "target": "fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "type": "authored"
    },
    {
      "source": "2338693661",
      "target": "fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "type": "authored"
    },
    {
      "source": "2313186079",
      "target": "fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "type": "authored"
    },
    {
      "source": "2338786700",
      "target": "fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "type": "authored"
    },
    {
      "source": "2323393220",
      "target": "fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "type": "authored"
    },
    {
      "source": "2323387847",
      "target": "fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "type": "authored"
    },
    {
      "source": "2323428668",
      "target": "fae1b477848fe7a6aac2ba6f887a2bae3f90964e",
      "type": "authored"
    },
    {
      "source": "2291515374",
      "target": "078a9c3dcd0575dccc45c861618e2363caf47986",
      "type": "authored"
    },
    {
      "source": "2266356137",
      "target": "078a9c3dcd0575dccc45c861618e2363caf47986",
      "type": "authored"
    },
    {
      "source": "2311864117",
      "target": "078a9c3dcd0575dccc45c861618e2363caf47986",
      "type": "authored"
    },
    {
      "source": "2261273470",
      "target": "078a9c3dcd0575dccc45c861618e2363caf47986",
      "type": "authored"
    },
    {
      "source": "2275576296",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "52151521",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2287053658",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "33546336",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "39670454",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "51020741",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2287130440",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2307469987",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2290487862",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2307548497",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2307556437",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2616463",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2288902682",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2294173946",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2257279869",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2296971",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2277809314",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "2091768",
      "target": "9e320d3a55b5a05a0eff3c2b28cdc2d707346930",
      "type": "authored"
    },
    {
      "source": "51114080",
      "target": "b47507f12a8b7d26f78a484e32a08d61d3f87358",
      "type": "authored"
    },
    {
      "source": "2176183932",
      "target": "b47507f12a8b7d26f78a484e32a08d61d3f87358",
      "type": "authored"
    },
    {
      "source": "2284590453",
      "target": "b47507f12a8b7d26f78a484e32a08d61d3f87358",
      "type": "authored"
    },
    {
      "source": "2306782862",
      "target": "b47507f12a8b7d26f78a484e32a08d61d3f87358",
      "type": "authored"
    },
    {
      "source": "2284591109",
      "target": "b47507f12a8b7d26f78a484e32a08d61d3f87358",
      "type": "authored"
    },
    {
      "source": "2284590611",
      "target": "b47507f12a8b7d26f78a484e32a08d61d3f87358",
      "type": "authored"
    },
    {
      "source": "2284592236",
      "target": "b47507f12a8b7d26f78a484e32a08d61d3f87358",
      "type": "authored"
    },
    {
      "source": "2258553001",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "2258779676",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "2297176886",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "2297673118",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "2176782672",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "2273515439",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "2296992073",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "145124447",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "2258552654",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "2268317474",
      "target": "603bb1eb011147bed0766c0b0c0bf4bb83ef4b9a",
      "type": "authored"
    },
    {
      "source": "37796923",
      "target": "344fa52471306a25133c9536992be15eecdd1c60",
      "type": "authored"
    },
    {
      "source": "1972292021",
      "target": "344fa52471306a25133c9536992be15eecdd1c60",
      "type": "authored"
    },
    {
      "source": "2308032002",
      "target": "344fa52471306a25133c9536992be15eecdd1c60",
      "type": "authored"
    },
    {
      "source": "2253400779",
      "target": "344fa52471306a25133c9536992be15eecdd1c60",
      "type": "authored"
    },
    {
      "source": "2136562",
      "target": "344fa52471306a25133c9536992be15eecdd1c60",
      "type": "authored"
    },
    {
      "source": "2298494612",
      "target": "879905229dd629570165ab92cfa4045d7de0cbe5",
      "type": "authored"
    },
    {
      "source": "2260379784",
      "target": "879905229dd629570165ab92cfa4045d7de0cbe5",
      "type": "authored"
    },
    {
      "source": "2307066146",
      "target": "879905229dd629570165ab92cfa4045d7de0cbe5",
      "type": "authored"
    },
    {
      "source": "2298275515",
      "target": "879905229dd629570165ab92cfa4045d7de0cbe5",
      "type": "authored"
    },
    {
      "source": "2298410609",
      "target": "879905229dd629570165ab92cfa4045d7de0cbe5",
      "type": "authored"
    },
    {
      "source": "2306947387",
      "target": "879905229dd629570165ab92cfa4045d7de0cbe5",
      "type": "authored"
    },
    {
      "source": "51464520",
      "target": "879905229dd629570165ab92cfa4045d7de0cbe5",
      "type": "authored"
    },
    {
      "source": "1785978",
      "target": "879905229dd629570165ab92cfa4045d7de0cbe5",
      "type": "authored"
    },
    {
      "source": "2307433841",
      "target": "879905229dd629570165ab92cfa4045d7de0cbe5",
      "type": "authored"
    },
    {
      "source": "35504092",
      "target": "76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d",
      "type": "authored"
    },
    {
      "source": "30819687",
      "target": "76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d",
      "type": "authored"
    },
    {
      "source": "2292184774",
      "target": "76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d",
      "type": "authored"
    },
    {
      "source": "2292208424",
      "target": "76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d",
      "type": "authored"
    },
    {
      "source": "3187768",
      "target": "76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d",
      "type": "authored"
    },
    {
      "source": "2292147095",
      "target": "76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d",
      "type": "authored"
    },
    {
      "source": "1993226927",
      "target": "76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d",
      "type": "authored"
    },
    {
      "source": "2292261580",
      "target": "76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d",
      "type": "authored"
    },
    {
      "source": "2261496744",
      "target": "76bee5115bb38e8ad0371e02e1dc82ea68c5ae3d",
      "type": "authored"
    },
    {
      "source": "2279541575",
      "target": "0ed0d844544c4a2981acccb9332dead922294664",
      "type": "authored"
    },
    {
      "source": "2279541575",
      "target": "9e6b56fa4f6ea626c6748b6386214b279dd8f76f",
      "type": "authored"
    },
    {
      "source": "2274072714",
      "target": "0ed0d844544c4a2981acccb9332dead922294664",
      "type": "authored"
    },
    {
      "source": "2149271292",
      "target": "0ed0d844544c4a2981acccb9332dead922294664",
      "type": "authored"
    },
    {
      "source": "2290075391",
      "target": "0ed0d844544c4a2981acccb9332dead922294664",
      "type": "authored"
    },
    {
      "source": "2273926416",
      "target": "0ed0d844544c4a2981acccb9332dead922294664",
      "type": "authored"
    },
    {
      "source": "2254576790",
      "target": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "authored"
    },
    {
      "source": "2109274417",
      "target": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "authored"
    },
    {
      "source": "1576223501",
      "target": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "authored"
    },
    {
      "source": "1576223501",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2284948588",
      "target": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "authored"
    },
    {
      "source": "2284931475",
      "target": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "authored"
    },
    {
      "source": "2284863107",
      "target": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "authored"
    },
    {
      "source": "48506411",
      "target": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "authored"
    },
    {
      "source": "48506411",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2284862784",
      "target": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "authored"
    },
    {
      "source": "2267033597",
      "target": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "authored"
    },
    {
      "source": "2141313179",
      "target": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "authored"
    },
    {
      "source": "2273551430",
      "target": "f05e84702562cb693dd68d3d1c88072519a7bd71",
      "type": "authored"
    },
    {
      "source": "2273551430",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2339267799",
      "target": "79d94b9c8f17f754c35bceb29eb76d7a39664c5b",
      "type": "authored"
    },
    {
      "source": "1654097718",
      "target": "79d94b9c8f17f754c35bceb29eb76d7a39664c5b",
      "type": "authored"
    },
    {
      "source": "2273966090",
      "target": "79d94b9c8f17f754c35bceb29eb76d7a39664c5b",
      "type": "authored"
    },
    {
      "source": "1405645138",
      "target": "79d94b9c8f17f754c35bceb29eb76d7a39664c5b",
      "type": "authored"
    },
    {
      "source": "2223172928",
      "target": "f02a5b86269fe40498e58061bb9b85a1c8b73a12",
      "type": "authored"
    },
    {
      "source": "2268784491",
      "target": "f02a5b86269fe40498e58061bb9b85a1c8b73a12",
      "type": "authored"
    },
    {
      "source": "2321401079",
      "target": "f02a5b86269fe40498e58061bb9b85a1c8b73a12",
      "type": "authored"
    },
    {
      "source": "2240987357",
      "target": "f02a5b86269fe40498e58061bb9b85a1c8b73a12",
      "type": "authored"
    },
    {
      "source": "2189520187",
      "target": "f8bb6887a591872b47f75d28b22baba7b0521655",
      "type": "authored"
    },
    {
      "source": "2144405890",
      "target": "f8bb6887a591872b47f75d28b22baba7b0521655",
      "type": "authored"
    },
    {
      "source": "2291606067",
      "target": "f8bb6887a591872b47f75d28b22baba7b0521655",
      "type": "authored"
    },
    {
      "source": "2130210943",
      "target": "f8bb6887a591872b47f75d28b22baba7b0521655",
      "type": "authored"
    },
    {
      "source": "2267734457",
      "target": "f8bb6887a591872b47f75d28b22baba7b0521655",
      "type": "authored"
    },
    {
      "source": "2312389838",
      "target": "f8bb6887a591872b47f75d28b22baba7b0521655",
      "type": "authored"
    },
    {
      "source": "2319449261",
      "target": "f8bb6887a591872b47f75d28b22baba7b0521655",
      "type": "authored"
    },
    {
      "source": "2316678337",
      "target": "f8bb6887a591872b47f75d28b22baba7b0521655",
      "type": "authored"
    },
    {
      "source": "2288749348",
      "target": "ca100914a70cf077eaff278b4827cdeeee742868",
      "type": "authored"
    },
    {
      "source": "2288552032",
      "target": "ca100914a70cf077eaff278b4827cdeeee742868",
      "type": "authored"
    },
    {
      "source": "2185907462",
      "target": "ca100914a70cf077eaff278b4827cdeeee742868",
      "type": "authored"
    },
    {
      "source": "2220666117",
      "target": "ca100914a70cf077eaff278b4827cdeeee742868",
      "type": "authored"
    },
    {
      "source": "2288524645",
      "target": "ca100914a70cf077eaff278b4827cdeeee742868",
      "type": "authored"
    },
    {
      "source": "2295694533",
      "target": "50fb8bcb90734b7b86cb257162038fcde3b47bad",
      "type": "authored"
    },
    {
      "source": "2295846651",
      "target": "50fb8bcb90734b7b86cb257162038fcde3b47bad",
      "type": "authored"
    },
    {
      "source": "2311384262",
      "target": "50fb8bcb90734b7b86cb257162038fcde3b47bad",
      "type": "authored"
    },
    {
      "source": "2306959990",
      "target": "50fb8bcb90734b7b86cb257162038fcde3b47bad",
      "type": "authored"
    },
    {
      "source": "2294928107",
      "target": "50fb8bcb90734b7b86cb257162038fcde3b47bad",
      "type": "authored"
    },
    {
      "source": "2145333268",
      "target": "50fb8bcb90734b7b86cb257162038fcde3b47bad",
      "type": "authored"
    },
    {
      "source": "2295845506",
      "target": "50fb8bcb90734b7b86cb257162038fcde3b47bad",
      "type": "authored"
    },
    {
      "source": "33859827",
      "target": "f82f49c20c6acc69f884f05e3a9f1ceea91061ce",
      "type": "authored"
    },
    {
      "source": "2064853201",
      "target": "f82f49c20c6acc69f884f05e3a9f1ceea91061ce",
      "type": "authored"
    },
    {
      "source": "39879848",
      "target": "f82f49c20c6acc69f884f05e3a9f1ceea91061ce",
      "type": "authored"
    },
    {
      "source": "2303846295",
      "target": "f82f49c20c6acc69f884f05e3a9f1ceea91061ce",
      "type": "authored"
    },
    {
      "source": "31039603",
      "target": "4bfd5e2221b88b9d38c7111fc0a525027f64b619",
      "type": "authored"
    },
    {
      "source": "2301312718",
      "target": "4bfd5e2221b88b9d38c7111fc0a525027f64b619",
      "type": "authored"
    },
    {
      "source": "2280919953",
      "target": "4bfd5e2221b88b9d38c7111fc0a525027f64b619",
      "type": "authored"
    },
    {
      "source": "2262988626",
      "target": "4bfd5e2221b88b9d38c7111fc0a525027f64b619",
      "type": "authored"
    },
    {
      "source": "2298888414",
      "target": "c37b66cae380ff6bc210338fef200ec1874ceb5f",
      "type": "authored"
    },
    {
      "source": "2298963125",
      "target": "c37b66cae380ff6bc210338fef200ec1874ceb5f",
      "type": "authored"
    },
    {
      "source": "3464094",
      "target": "c37b66cae380ff6bc210338fef200ec1874ceb5f",
      "type": "authored"
    },
    {
      "source": "2148928671",
      "target": "3882788d42339848f5828362e367437e9a5447e0",
      "type": "authored"
    },
    {
      "source": "35276441",
      "target": "3882788d42339848f5828362e367437e9a5447e0",
      "type": "authored"
    },
    {
      "source": "2203459",
      "target": "3882788d42339848f5828362e367437e9a5447e0",
      "type": "authored"
    },
    {
      "source": "7883212",
      "target": "3882788d42339848f5828362e367437e9a5447e0",
      "type": "authored"
    },
    {
      "source": "2266943614",
      "target": "3882788d42339848f5828362e367437e9a5447e0",
      "type": "authored"
    },
    {
      "source": "2307073216",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278223869",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278223869",
      "target": "35b142ea69598e6241f0011312128031df55895c",
      "type": "authored"
    },
    {
      "source": "2278834796",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278834796",
      "target": "53a803388e83ae89261624099d7be4287ace67cb",
      "type": "authored"
    },
    {
      "source": "2278834796",
      "target": "35b142ea69598e6241f0011312128031df55895c",
      "type": "authored"
    },
    {
      "source": "144485528",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "144485528",
      "target": "53a803388e83ae89261624099d7be4287ace67cb",
      "type": "authored"
    },
    {
      "source": "144485528",
      "target": "35b142ea69598e6241f0011312128031df55895c",
      "type": "authored"
    },
    {
      "source": "2278404250",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2289796300",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "1748844142",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2276752202",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2307219221",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278395340",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278395340",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2278830967",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2279051415",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2279051415",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2281036475",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2307422404",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278643857",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2307085654",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278218552",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278257096",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278218072",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "1797090",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2279107352",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2279107352",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2278220653",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2244285627",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2258088582",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2258088582",
      "target": "35b142ea69598e6241f0011312128031df55895c",
      "type": "authored"
    },
    {
      "source": "2274200088",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2274200088",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2239599436",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278219877",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2054452755",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278677183",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278218583",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2272467392",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2307222549",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2307394725",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278432120",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278221484",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278221484",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2278337977",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278337977",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2278389597",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278389597",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2278217940",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278217940",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2278218736",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278218736",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2278618633",
      "target": "2797cbda8c845504119b62ee25deb1500ec2dfaf",
      "type": "authored"
    },
    {
      "source": "2278618633",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2268491856",
      "target": "5eac2a40422a7085cb6f03285ad08210b6f6744b",
      "type": "authored"
    },
    {
      "source": "2143853895",
      "target": "5eac2a40422a7085cb6f03285ad08210b6f6744b",
      "type": "authored"
    },
    {
      "source": "2057973326",
      "target": "5eac2a40422a7085cb6f03285ad08210b6f6744b",
      "type": "authored"
    },
    {
      "source": "2285130258",
      "target": "5eac2a40422a7085cb6f03285ad08210b6f6744b",
      "type": "authored"
    },
    {
      "source": "2285251994",
      "target": "5eac2a40422a7085cb6f03285ad08210b6f6744b",
      "type": "authored"
    },
    {
      "source": "2276508494",
      "target": "5eac2a40422a7085cb6f03285ad08210b6f6744b",
      "type": "authored"
    },
    {
      "source": "2249847177",
      "target": "5eac2a40422a7085cb6f03285ad08210b6f6744b",
      "type": "authored"
    },
    {
      "source": "2284988933",
      "target": "5eac2a40422a7085cb6f03285ad08210b6f6744b",
      "type": "authored"
    },
    {
      "source": "2887562",
      "target": "8946891e94831adc8cddb0d32311cce2445c96d2",
      "type": "authored"
    },
    {
      "source": "103404553",
      "target": "8946891e94831adc8cddb0d32311cce2445c96d2",
      "type": "authored"
    },
    {
      "source": "2143749775",
      "target": "8946891e94831adc8cddb0d32311cce2445c96d2",
      "type": "authored"
    },
    {
      "source": "2144174497",
      "target": "8946891e94831adc8cddb0d32311cce2445c96d2",
      "type": "authored"
    },
    {
      "source": "2109738542",
      "target": "8946891e94831adc8cddb0d32311cce2445c96d2",
      "type": "authored"
    },
    {
      "source": "2548384",
      "target": "8946891e94831adc8cddb0d32311cce2445c96d2",
      "type": "authored"
    },
    {
      "source": "47413820",
      "target": "8946891e94831adc8cddb0d32311cce2445c96d2",
      "type": "authored"
    },
    {
      "source": "2256646491",
      "target": "8946891e94831adc8cddb0d32311cce2445c96d2",
      "type": "authored"
    },
    {
      "source": "2253458981",
      "target": "8946891e94831adc8cddb0d32311cce2445c96d2",
      "type": "authored"
    },
    {
      "source": "2256227183",
      "target": "8946891e94831adc8cddb0d32311cce2445c96d2",
      "type": "authored"
    },
    {
      "source": "3768186",
      "target": "fc6a2f7478f68adefd69e2071f27e38aa1647f2f",
      "type": "authored"
    },
    {
      "source": "2302149588",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2117902355",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2315247489",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2249899670",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2292213035",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2314888390",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2314831395",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2314861602",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2150606888",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2315388099",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2157954216",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2315068277",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2314827568",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2233320353",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2295929465",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2295789325",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2295809950",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "1398454307",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2144118403",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2301534001",
      "target": "bd0fae435b93819d407f36d9d52565d5dfb5f87c",
      "type": "authored"
    },
    {
      "source": "2261925600",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2261925600",
      "target": "d9c05ef795a82e35280ddc04f9a0c203114f546e",
      "type": "authored"
    },
    {
      "source": "2284580714",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2284580714",
      "target": "d9c05ef795a82e35280ddc04f9a0c203114f546e",
      "type": "authored"
    },
    {
      "source": "2306804471",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2220002319",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2143183255",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2306784963",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2307262806",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2116314158",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2116314158",
      "target": "d9c05ef795a82e35280ddc04f9a0c203114f546e",
      "type": "authored"
    },
    {
      "source": "47319720",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "47319720",
      "target": "d9c05ef795a82e35280ddc04f9a0c203114f546e",
      "type": "authored"
    },
    {
      "source": "2108080174",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2269171464",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2306790051",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2283846474",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2283846474",
      "target": "d9c05ef795a82e35280ddc04f9a0c203114f546e",
      "type": "authored"
    },
    {
      "source": "2283881403",
      "target": "250043fdae97d2ac87245a44923682e7a3decd50",
      "type": "authored"
    },
    {
      "source": "2283881403",
      "target": "d9c05ef795a82e35280ddc04f9a0c203114f546e",
      "type": "authored"
    },
    {
      "source": "2114891202",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297768912",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2942686",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "29956361",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "113916198",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "3032929",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297768888",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297768420",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2274106850",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "145560551",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "102222453",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297766346",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "46278353",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "121645690",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297768425",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2157424631",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2264439430",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "113810201",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297780966",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "50672277",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297768528",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297768894",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2315830",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "3310951",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2054713176",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2281352409",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297768377",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2266241191",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2945519",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297778093",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "51900416",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2268726222",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2160340819",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "1830939",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297803872",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2152658577",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297767306",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2279749803",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297805471",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2239163839",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2268435346",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2269852520",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2268632175",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297769037",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297828605",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "46781068",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2256988075",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297767663",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2274742737",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2172095",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "27419446",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "1416388980",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "6625302",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "4099006",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2279740366",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297777494",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "46177458",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "41016119",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2298400461",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2347792",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2282542305",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297768516",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "1413038175",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297814416",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2284868563",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2266307341",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2291873212",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2537545",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2537545",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2127734657",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2153689937",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2274157852",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2298020153",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297768553",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2226773110",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "46747953",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2257094139",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297814868",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "145338263",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2291073936",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2287794511",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2287118838",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297810599",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297818019",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2274195530",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2271712604",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2298130748",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "2297801063",
      "target": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3",
      "type": "authored"
    },
    {
      "source": "1646458461",
      "target": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "authored"
    },
    {
      "source": "2291438028",
      "target": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "authored"
    },
    {
      "source": "2261364306",
      "target": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "authored"
    },
    {
      "source": "2108635077",
      "target": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "authored"
    },
    {
      "source": "2291072588",
      "target": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "authored"
    },
    {
      "source": "2278894217",
      "target": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "authored"
    },
    {
      "source": "2278433392",
      "target": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "authored"
    },
    {
      "source": "2268398121",
      "target": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "authored"
    },
    {
      "source": "2290576964",
      "target": "afe0998d191f3ea8490c7df100a3ffc5dcc62c5e",
      "type": "authored"
    },
    {
      "source": "0c33df9fc20c9314a0883611e9912a94b511569c",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "cites",
      "contexts": "[\"While Show-o serves as the main model for our experiments, we also investigate variations in other unified transformers (Ma et al., 2024; Liu et al., 2024a; Wang et al., 2024) to gain a more comprehensive insight.\", \"In this section, we first introduce the unified transformers (Xie et al., 2024; Wang et al., 2024) and the Mixture of Depths (MoD)(Raposo et al., 2024) (Sec.3.1).\", \"\\u2026into two types: one where both generation and understanding tasks are handled using a fully autoregressive method (Ge et al., 2024b; Team, 2024; Wang et al., 2024), and another where generation tasks are addressed using diffusion or flow matching techniques, while autoregressive methods are\\u2026\", \"\\u2026(Ge et al., 2024a; Elhoushi et al., 2024a; Chen et al., 2024c; Xiao et al., 2024) focuses on sparse computation in LLMs. Mixture of Experts (MoE)(Cai et al., 2024; Xue et al., 2024a; Dai et al., 2024; Jiang et al., 2024) is a popular method that re-places the feed-forward (FFN) layers of\\u2026\", \"Chameleon (Team, 2024) and Emu3 (Wang et al., 2024) employs an autoregressive approach for both generation and understanding tasks.\", \"\\u20262024c; Xiao et al., 2024) focuses on sparse computation in LLMs. Mixture of Experts (MoE)(Cai et al., 2024; Xue et al., 2024a; Dai et al., 2024; Jiang et al., 2024) is a popular method that re-places the feed-forward (FFN) layers of transformer blocks with MoE layers, where input tokens are\\u2026\", \"We consider two representative unified multimodal transformer models: Show-o (Xie et al., 2024) and Emu3 (Wang et al., 2024).\", \"We choose Show-o (Xie et al., 2024) and Emu-3 (Wang et al., 2024) as representative models for our approach to token pruning, as they cover different types of unified trans-formers.\", \"\\u2026Wu et al., 2023b; Team, 2024; Xie et al., 2024; Liu et al., 2024e; Sun et al., 2023d; Zhou et al., 2024; Tang et al., 2024; Dong et al., 2024; Wang et al., 2024; Ma et al., 2024; Liu et al., 2024a; Wu et al., 2024a; Shi et al., 2024; Anil et al., 2023; Qu et al., 2024; Li et al., 2024b; Kou\\u2026\"]",
      "is_influential": true
    },
    {
      "source": "2258958963",
      "target": "0c33df9fc20c9314a0883611e9912a94b511569c",
      "type": "authored"
    },
    {
      "source": "2346316013",
      "target": "0c33df9fc20c9314a0883611e9912a94b511569c",
      "type": "authored"
    },
    {
      "source": "2344762475",
      "target": "0c33df9fc20c9314a0883611e9912a94b511569c",
      "type": "authored"
    },
    {
      "source": "a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "cites",
      "contexts": "[\"Ideally, we seek flexible model architectures (Jiang et al., 2024b; Cai et al., 2024a;b) that enable parameter scaling while maintaining computational efficiency.\", \"Sparsely-activated Transformer models, such as Sparse Mixture-of-Expert (MoE) architectures, leverage input adaptivity to achieve scalable and efficient computation.\", \"Research on conditional computation (Zhou et al., 2022; Jiang et al., 2024b; Dai et al., 2024) has shown that automatically learned routing strategies can often lead to load imbalance issues, where the model tends to select only a few sub-dimensions, leaving others underutilized and insufficiently\\u2026\", \"\\u2026et al., 2024; Liu et al., 2024) observe the sparsity of intermediate dimension activations and leverage it to design adaptive networks (e.g., MoE (Cai et al., 2024b; Dai et al., 2024; Xue et al., 2024a)) for parameter scaling or use pruning (Xia et al., 2023; Chen et al., 2023; Ma et al., 2023)\\u2026\", \"Using FFN as an example, the traditional method (MoE) exploits the sparsity of the intermediate dimension.\", \"Some studies (Qiu et al., 2024; Liu et al., 2024) observe the sparsity of intermediate dimension activations and leverage it to design adaptive networks (e.g., MoE (Cai et al., 2024b; Dai et al., 2024; Xue et al., 2024a)) for parameter scaling or use pruning (Xia et al., 2023; Chen et al., 2023; Ma et al., 2023) and local activation mechanisms (Liu et al., 2023a) to reduce computational costs.\"]",
      "is_influential": true
    },
    {
      "source": "2268796753",
      "target": "a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "type": "authored"
    },
    {
      "source": "2257305563",
      "target": "a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "type": "authored"
    },
    {
      "source": "2335037545",
      "target": "a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "type": "authored"
    },
    {
      "source": "2054250919",
      "target": "a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "type": "authored"
    },
    {
      "source": "2268772040",
      "target": "a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "type": "authored"
    },
    {
      "source": "104463827",
      "target": "a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "type": "authored"
    },
    {
      "source": "2296100360",
      "target": "a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "type": "authored"
    },
    {
      "source": "2190177674",
      "target": "a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "type": "authored"
    },
    {
      "source": "2238917361",
      "target": "a49b04882f4c508ba85a6585cbe56827a96b66a6",
      "type": "authored"
    },
    {
      "source": "1112c0b273c9466906ba6d51fc90881538d33769",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "cites",
      "contexts": "[\"The mixture of experts (MoE) approach has continued to experience signi\\ufb01cant growth, with notable advancements in 2024, including the introduction of Mixtral-8x7B (Jiang et al., 2024) and several other large-scale industrial language models such as Grok-1, DBRX, Arctic, DeepSeek-V2 (DeepSeek-AI and Liu, 2024a), and DeepSeek-V3 (DeepSeek-AI and Liu, 2024b) among others.\", \"\\u2026experts (MoE) approach has continued to experience signi\\ufb01cant growth, with notable advancements in 2024, including the introduction of Mixtral-8x7B (Jiang et al., 2024) and several other large-scale industrial language models such as Grok-1, DBRX, Arctic, DeepSeek-V2 (DeepSeek-AI and Liu, 2024a),\\u2026\", \"\\u2026et al., 2017; Team and Costa-juss`a, 2022), open-domain question answering (Du and Huang, 2022; Artetxe and Bhosale, 2022), code generation (Jiang et al., 2024; Wei et al., 2024; Dai et al., 2024), and mathematical problem solving (Jiang et al., 2024; DeepSeek-AI and Liu, 2024a; Dai et al., 2024).\", \"In this work, we will use the taxonomy introduced in (Cai et al., 2024).\", \"\\u2026et al., 2017; Team and Costa-juss`a, 2022), open-domain question answering (Du and Huang, 2022; Artetxe and Bhosale, 2022), code generation (Jiang et al., 2024; Wei et al., 2024; Dai et al., 2024), and mathematical problem solving (Jiang et al., 2024; DeepSeek-AI and Liu, 2024a; Dai et\\u2026\"]",
      "is_influential": true
    },
    {
      "source": "2338973383",
      "target": "1112c0b273c9466906ba6d51fc90881538d33769",
      "type": "authored"
    },
    {
      "source": "2334859470",
      "target": "1112c0b273c9466906ba6d51fc90881538d33769",
      "type": "authored"
    },
    {
      "source": "2334773796",
      "target": "1112c0b273c9466906ba6d51fc90881538d33769",
      "type": "authored"
    },
    {
      "source": "2335117100",
      "target": "1112c0b273c9466906ba6d51fc90881538d33769",
      "type": "authored"
    },
    {
      "source": "2334746553",
      "target": "1112c0b273c9466906ba6d51fc90881538d33769",
      "type": "authored"
    },
    {
      "source": "2339774429",
      "target": "1112c0b273c9466906ba6d51fc90881538d33769",
      "type": "authored"
    },
    {
      "source": "2334743866",
      "target": "1112c0b273c9466906ba6d51fc90881538d33769",
      "type": "authored"
    },
    {
      "source": "2334970043",
      "target": "1112c0b273c9466906ba6d51fc90881538d33769",
      "type": "authored"
    },
    {
      "source": "5d93440621bd49953be4aab0474b0d26dcae504e",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "cites",
      "contexts": "[\"\\u2026with state of the art models in data modalities beyond tabular data, such as image [Guo et al., 2019b, Liu et al., 2023, Islam et al., 2024, Wang et al., 2024], natural language processing [Guo et al., 2019a, Sun et al., 2020, Zhang et al., 2020b], graph [Verma et al., 2021, Han et al.,\\u2026\", \"\\u2026experts, which combines model outputs by multiplying their probability distributions [Ja-cobs et al., 1991, Hinton, 2002] and have been successfully incorporated into transformer architectures to train large scale large language models [Lepikhin et al., 2020, Jiang et al., 2024, Liu et al., 2024].\", \"In tandem with these developments and the rise of large generative language models, the idea of combining or \\u201cfus-ing\\\" multiple deep learning models has gained attention as a means to improve model capacity [Cai et al., 2024].\"]",
      "is_influential": false
    },
    {
      "source": "2346113879",
      "target": "5d93440621bd49953be4aab0474b0d26dcae504e",
      "type": "authored"
    },
    {
      "source": "2207655",
      "target": "5d93440621bd49953be4aab0474b0d26dcae504e",
      "type": "authored"
    },
    {
      "source": "2264978144",
      "target": "5d93440621bd49953be4aab0474b0d26dcae504e",
      "type": "authored"
    },
    {
      "source": "2330666",
      "target": "5d93440621bd49953be4aab0474b0d26dcae504e",
      "type": "authored"
    },
    {
      "source": "2346113190",
      "target": "5d93440621bd49953be4aab0474b0d26dcae504e",
      "type": "authored"
    },
    {
      "source": "9e6b56fa4f6ea626c6748b6386214b279dd8f76f",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "cites",
      "contexts": "[\"Retrieval-augmented prompting [Gao et al. , 2023; Fan et al. , 2024; Qiu et al. , 2024] excels at extracting the most relevant records from user data to enhance PLLMs (See Figure 3(b)).\", \"This lack of parameter personalization limits adaptability to user dynamics and preference shifts, potentially resulting in suboptimal performance [Cai et al. , 2024].\"]",
      "is_influential": false
    },
    {
      "source": "2144131350",
      "target": "9e6b56fa4f6ea626c6748b6386214b279dd8f76f",
      "type": "authored"
    },
    {
      "source": "2346820279",
      "target": "9e6b56fa4f6ea626c6748b6386214b279dd8f76f",
      "type": "authored"
    },
    {
      "source": "2345819983",
      "target": "9e6b56fa4f6ea626c6748b6386214b279dd8f76f",
      "type": "authored"
    },
    {
      "source": "2240695630",
      "target": "9e6b56fa4f6ea626c6748b6386214b279dd8f76f",
      "type": "authored"
    },
    {
      "source": "2249778653",
      "target": "9e6b56fa4f6ea626c6748b6386214b279dd8f76f",
      "type": "authored"
    },
    {
      "source": "2301207729",
      "target": "9e6b56fa4f6ea626c6748b6386214b279dd8f76f",
      "type": "authored"
    },
    {
      "source": "2309174208",
      "target": "9e6b56fa4f6ea626c6748b6386214b279dd8f76f",
      "type": "authored"
    },
    {
      "source": "349354e5886d566536bb30c11cf9c74bed458594",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "cites",
      "contexts": "[\"Based on a Mixture of Experts(MoE) architecture[6,11] with 671 billion parameters and utilizing unique reinforcement learning techniques, DeepSeek-R1 excels in various domains, including mathematical reasoning, code generation, and natural language processing.\"]",
      "is_influential": false
    },
    {
      "source": "2298863422",
      "target": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "authored"
    },
    {
      "source": "2306966575",
      "target": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "authored"
    },
    {
      "source": "2292657641",
      "target": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "authored"
    },
    {
      "source": "2308853745",
      "target": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "authored"
    },
    {
      "source": "2345816011",
      "target": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "authored"
    },
    {
      "source": "2345984737",
      "target": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "authored"
    },
    {
      "source": "2345874673",
      "target": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "authored"
    },
    {
      "source": "31698884",
      "target": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "authored"
    },
    {
      "source": "2345823846",
      "target": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "authored"
    },
    {
      "source": "2307181856",
      "target": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "authored"
    },
    {
      "source": "2292613998",
      "target": "349354e5886d566536bb30c11cf9c74bed458594",
      "type": "authored"
    },
    {
      "source": "98fda72e51ffecf914da5fe5c8f3d64d4078e144",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "cites",
      "contexts": "[\"As such, Mixture-of-Experts (MoE) architecture has emerged to efficiently scale large models [3,5,15,20].\"]",
      "is_influential": false
    },
    {
      "source": "2344765112",
      "target": "98fda72e51ffecf914da5fe5c8f3d64d4078e144",
      "type": "authored"
    },
    {
      "source": "2344765052",
      "target": "98fda72e51ffecf914da5fe5c8f3d64d4078e144",
      "type": "authored"
    },
    {
      "source": "ae57f38c1c62e2b0a143c0c862f94bb4c11f4fab",
      "target": "a56453c98c6244329f660d4fd74e2afd4fe7d349",
      "type": "cites",
      "contexts": "[]",
      "is_influential": false
    },
    {
      "source": "2342574394",
      "target": "ae57f38c1c62e2b0a143c0c862f94bb4c11f4fab",
      "type": "authored"
    },
    {
      "source": "2293559850",
      "target": "ae57f38c1c62e2b0a143c0c862f94bb4c11f4fab",
      "type": "authored"
    },
    {
      "source": "2065251473",
      "target": "ae57f38c1c62e2b0a143c0c862f94bb4c11f4fab",
      "type": "authored"
    },
    {
      "source": "2324627531",
      "target": "ae57f38c1c62e2b0a143c0c862f94bb4c11f4fab",
      "type": "authored"
    },
    {
      "source": "2266789166",
      "target": "ae57f38c1c62e2b0a143c0c862f94bb4c11f4fab",
      "type": "authored"
    },
    {
      "source": "2309672667",
      "target": "8970a42d5de27dcf0833f3f1387de586b6d85a3e",
      "type": "authored"
    },
    {
      "source": "2188993538",
      "target": "8970a42d5de27dcf0833f3f1387de586b6d85a3e",
      "type": "authored"
    },
    {
      "source": "2162081759",
      "target": "8970a42d5de27dcf0833f3f1387de586b6d85a3e",
      "type": "authored"
    },
    {
      "source": "2289837431",
      "target": "8970a42d5de27dcf0833f3f1387de586b6d85a3e",
      "type": "authored"
    },
    {
      "source": "2295008708",
      "target": "8970a42d5de27dcf0833f3f1387de586b6d85a3e",
      "type": "authored"
    },
    {
      "source": "2209882676",
      "target": "a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8",
      "type": "authored"
    },
    {
      "source": "31225166",
      "target": "a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8",
      "type": "authored"
    },
    {
      "source": "2273650910",
      "target": "a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8",
      "type": "authored"
    },
    {
      "source": "1989015",
      "target": "a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8",
      "type": "authored"
    },
    {
      "source": "2307032205",
      "target": "a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8",
      "type": "authored"
    },
    {
      "source": "2273651410",
      "target": "a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8",
      "type": "authored"
    },
    {
      "source": "2824500",
      "target": "a9cb7e1ee37ae9c2c4db576ea5fd6154f153e3b8",
      "type": "authored"
    },
    {
      "source": "2263421000",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2264387465",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2263759170",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2263934201",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2264501869",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2264120205",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2263436071",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2263534340",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2213848720",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2304645914",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2264407156",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2263715028",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2263413652",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2264103352",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2293095227",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2263493641",
      "target": "2b3fdda4f5ace23ded018ce9fddd03c8fa5ef731",
      "type": "authored"
    },
    {
      "source": "2303587299",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2141243564",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2268629065",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2268554345",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2222759010",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2269022612",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2268560784",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2303773199",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2303903609",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2303462671",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2303615702",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2303694488",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2303515829",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2303534151",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2303757494",
      "target": "697b1b17c9c27b43b8ee9a504317f1a32bb2a28c",
      "type": "authored"
    },
    {
      "source": "2164726084",
      "target": "7f6f34090d17d0523b91c0d3cd34fe9f66ebb189",
      "type": "authored"
    },
    {
      "source": "2302893462",
      "target": "7f6f34090d17d0523b91c0d3cd34fe9f66ebb189",
      "type": "authored"
    },
    {
      "source": "2166492136",
      "target": "7f6f34090d17d0523b91c0d3cd34fe9f66ebb189",
      "type": "authored"
    },
    {
      "source": "2257122070",
      "target": "7f6f34090d17d0523b91c0d3cd34fe9f66ebb189",
      "type": "authored"
    },
    {
      "source": "2302816223",
      "target": "d9c05ef795a82e35280ddc04f9a0c203114f546e",
      "type": "authored"
    },
    {
      "source": "2137407647",
      "target": "d9c05ef795a82e35280ddc04f9a0c203114f546e",
      "type": "authored"
    },
    {
      "source": "2302994789",
      "target": "d9c05ef795a82e35280ddc04f9a0c203114f546e",
      "type": "authored"
    },
    {
      "source": "10780897",
      "target": "53a803388e83ae89261624099d7be4287ace67cb",
      "type": "authored"
    },
    {
      "source": "10780897",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2156640188",
      "target": "53a803388e83ae89261624099d7be4287ace67cb",
      "type": "authored"
    },
    {
      "source": "2243360876",
      "target": "53a803388e83ae89261624099d7be4287ace67cb",
      "type": "authored"
    },
    {
      "source": "2238628841",
      "target": "53a803388e83ae89261624099d7be4287ace67cb",
      "type": "authored"
    },
    {
      "source": "49164966",
      "target": "4b879f069d023e03bf537309a99bdaeb39916ea5",
      "type": "authored"
    },
    {
      "source": "67284811",
      "target": "4b879f069d023e03bf537309a99bdaeb39916ea5",
      "type": "authored"
    },
    {
      "source": "50536468",
      "target": "4b879f069d023e03bf537309a99bdaeb39916ea5",
      "type": "authored"
    },
    {
      "source": "2261973116",
      "target": "4b879f069d023e03bf537309a99bdaeb39916ea5",
      "type": "authored"
    },
    {
      "source": "2269777270",
      "target": "ebcf108f8bc42140721ff02b6727b0a291362957",
      "type": "authored"
    },
    {
      "source": "2298231704",
      "target": "ebcf108f8bc42140721ff02b6727b0a291362957",
      "type": "authored"
    },
    {
      "source": "2298214784",
      "target": "ebcf108f8bc42140721ff02b6727b0a291362957",
      "type": "authored"
    },
    {
      "source": "2266012864",
      "target": "ebcf108f8bc42140721ff02b6727b0a291362957",
      "type": "authored"
    },
    {
      "source": "2269737823",
      "target": "ebcf108f8bc42140721ff02b6727b0a291362957",
      "type": "authored"
    },
    {
      "source": "2279917725",
      "target": "ebcf108f8bc42140721ff02b6727b0a291362957",
      "type": "authored"
    },
    {
      "source": "2297927648",
      "target": "ebcf108f8bc42140721ff02b6727b0a291362957",
      "type": "authored"
    },
    {
      "source": "2269766976",
      "target": "ebcf108f8bc42140721ff02b6727b0a291362957",
      "type": "authored"
    },
    {
      "source": "2296151561",
      "target": "c9ff9fbbe21985b35d6a070b67f22a0e065c4328",
      "type": "authored"
    },
    {
      "source": "2282899341",
      "target": "c9ff9fbbe21985b35d6a070b67f22a0e065c4328",
      "type": "authored"
    },
    {
      "source": "2295986722",
      "target": "c9ff9fbbe21985b35d6a070b67f22a0e065c4328",
      "type": "authored"
    },
    {
      "source": "2296026250",
      "target": "c9ff9fbbe21985b35d6a070b67f22a0e065c4328",
      "type": "authored"
    },
    {
      "source": "c814eb8684f2ce07a4f21a7ed4c06c75d63fea23",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "cites",
      "contexts": "[\"BTX [Sukhbaatar et al., 2024] extends this approach by upcycling not from a single dense model, but from multiple specialized dense expert models to encourage diversity in the MoE initialization.\", \"BTX [Sukhbaatar et al., 2024] generalize this approach to initialize each expert from the FFN layer of a different expert model, and all other parameters as the average over all of these models.\", \"Unlike Sukhbaatar et al. [2024], instead of using the original FFN of the seed model as one of the routed experts in an MoE layer, we use it as the shared expert (FFN s ) to better preserve the previous capabilities in the MoE model.\", \"These techniques often train completely separate experts and \\u201cupcycle\\u201d (combine) them into a single unified MoE model after dense training [Sukhbaatar et al., 2024].\"]",
      "is_influential": true
    },
    {
      "source": "2153302934",
      "target": "c814eb8684f2ce07a4f21a7ed4c06c75d63fea23",
      "type": "authored"
    },
    {
      "source": "2153302934",
      "target": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "authored"
    },
    {
      "source": "2283437085",
      "target": "c814eb8684f2ce07a4f21a7ed4c06c75d63fea23",
      "type": "authored"
    },
    {
      "source": "2283437085",
      "target": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "authored"
    },
    {
      "source": "153563548",
      "target": "c814eb8684f2ce07a4f21a7ed4c06c75d63fea23",
      "type": "authored"
    },
    {
      "source": "153563548",
      "target": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "authored"
    },
    {
      "source": "2261493078",
      "target": "c814eb8684f2ce07a4f21a7ed4c06c75d63fea23",
      "type": "authored"
    },
    {
      "source": "82290814",
      "target": "c814eb8684f2ce07a4f21a7ed4c06c75d63fea23",
      "type": "authored"
    },
    {
      "source": "82290814",
      "target": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "authored"
    },
    {
      "source": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "cites",
      "contexts": "[\"In particular, Branch-Train-MiX [BTX; 1] initializes an MoE with N FFN experts through a three-step upcycling process.\", \"Similar to the evaluations in Sukhbaatar et al. [1], BAM is evaluated in two settings: \\u2022 Data-Matching (DM) : We use exactly the same training data in terms of both content and quantity for BAM as used for the BTX baseline.\", \"BTX [1] improves on this approach by upcycling not just a single dense seed model but also multiple specialized dense models branched from the seed model.\", \"Efficient initialization for MoEs Two recent works have studied how to use pre-trained dense models to initialize MoEs [14, 1].\", \"To address this, recent works have explored more efficient alternatives by initializing MoEs using pre-trained dense models [14, 1], followed by a small number of MoE training steps.\"]",
      "is_influential": true
    },
    {
      "source": "1734996170",
      "target": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "authored"
    },
    {
      "source": "2316167977",
      "target": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "authored"
    },
    {
      "source": "2303257254",
      "target": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "authored"
    },
    {
      "source": "8795464",
      "target": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "authored"
    },
    {
      "source": "2301154187",
      "target": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "authored"
    },
    {
      "source": "2283848746",
      "target": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "authored"
    },
    {
      "source": "2884561",
      "target": "175b52728d7924a625e6ec99a63343128b3c03d0",
      "type": "authored"
    },
    {
      "source": "d433b0e7f3824d6fb34d84e19146a1857695e67e",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "cites",
      "contexts": "[\"Sukhbaatar et al. [2024] show that performing data-specific dense finetunes, averaging the attention weights, and creating different MLP experts is a more efficient training method than simply duplicating MLPs during upcycling.\"]",
      "is_influential": false
    },
    {
      "source": "2040790531",
      "target": "d433b0e7f3824d6fb34d84e19146a1857695e67e",
      "type": "authored"
    },
    {
      "source": "2076413731",
      "target": "d433b0e7f3824d6fb34d84e19146a1857695e67e",
      "type": "authored"
    },
    {
      "source": "120486138",
      "target": "d433b0e7f3824d6fb34d84e19146a1857695e67e",
      "type": "authored"
    },
    {
      "source": "990e38839261dd52678f0de44b67b62b4991dcfb",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "cites",
      "contexts": "[\"(Ferreira et al., 2024; Fedus et al., 2022; Shazeer et al., 2017; Jiang et al., 2024a; Yadav et al., 2024a; Sukhbaatar et al., 2024; Tang et al., 2024).\"]",
      "is_influential": false
    },
    {
      "source": "1518270974",
      "target": "990e38839261dd52678f0de44b67b62b4991dcfb",
      "type": "authored"
    },
    {
      "source": "47009988",
      "target": "990e38839261dd52678f0de44b67b62b4991dcfb",
      "type": "authored"
    },
    {
      "source": "2329183817",
      "target": "990e38839261dd52678f0de44b67b62b4991dcfb",
      "type": "authored"
    },
    {
      "source": "d0e5421fc93c99fbdcc2c3f045b11c351ea76364",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "cites",
      "contexts": "[\"Consequently, the most successful models in practice (Wei et al., 2024; Sukhbaatar et al., 2024; Xie et al., 2022; Dai et al., 2022) are still based on the original variant proposed in (Fedus et al., 2022) although there exists a plethora of advanced MoE algorithms (Zhou et al., 2022; Huang et al.,\\u2026\"]",
      "is_influential": false
    },
    {
      "source": "2324075316",
      "target": "d0e5421fc93c99fbdcc2c3f045b11c351ea76364",
      "type": "authored"
    },
    {
      "source": "2324059216",
      "target": "d0e5421fc93c99fbdcc2c3f045b11c351ea76364",
      "type": "authored"
    },
    {
      "source": "2329094191",
      "target": "d0e5421fc93c99fbdcc2c3f045b11c351ea76364",
      "type": "authored"
    },
    {
      "source": "2329520351",
      "target": "d0e5421fc93c99fbdcc2c3f045b11c351ea76364",
      "type": "authored"
    },
    {
      "source": "2324053774",
      "target": "d0e5421fc93c99fbdcc2c3f045b11c351ea76364",
      "type": "authored"
    },
    {
      "source": "680eedfe1f66c1a58105d2ee8ee071ae962a0fb6",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "cites",
      "contexts": "[\"BTX (Sukhbaatar et al., 2024) further refines MoE efficiency by upcycling specialized models.\"]",
      "is_influential": false
    },
    {
      "source": "2325945678",
      "target": "680eedfe1f66c1a58105d2ee8ee071ae962a0fb6",
      "type": "authored"
    },
    {
      "source": "2267007135",
      "target": "680eedfe1f66c1a58105d2ee8ee071ae962a0fb6",
      "type": "authored"
    },
    {
      "source": "2212240432",
      "target": "680eedfe1f66c1a58105d2ee8ee071ae962a0fb6",
      "type": "authored"
    },
    {
      "source": "2290186835",
      "target": "680eedfe1f66c1a58105d2ee8ee071ae962a0fb6",
      "type": "authored"
    },
    {
      "source": "2325912821",
      "target": "680eedfe1f66c1a58105d2ee8ee071ae962a0fb6",
      "type": "authored"
    },
    {
      "source": "2267007505",
      "target": "680eedfe1f66c1a58105d2ee8ee071ae962a0fb6",
      "type": "authored"
    },
    {
      "source": "1bd59bff99d5774388764bc42390e6732cf52a1f",
      "target": "07894aeadab9158fdb97647c4792816ede1b60b9",
      "type": "cites",
      "contexts": "[]",
      "is_influential": false
    },
    {
      "source": "2325729401",
      "target": "1bd59bff99d5774388764bc42390e6732cf52a1f",
      "type": "authored"
    },
    {
      "source": "51516859",
      "target": "1bd59bff99d5774388764bc42390e6732cf52a1f",
      "type": "authored"
    },
    {
      "source": "2325730893",
      "target": "1bd59bff99d5774388764bc42390e6732cf52a1f",
      "type": "authored"
    },
    {
      "source": "2175482685",
      "target": "1bd59bff99d5774388764bc42390e6732cf52a1f",
      "type": "authored"
    },
    {
      "source": "2292260669",
      "target": "1bd59bff99d5774388764bc42390e6732cf52a1f",
      "type": "authored"
    },
    {
      "source": "2292260070",
      "target": "1bd59bff99d5774388764bc42390e6732cf52a1f",
      "type": "authored"
    },
    {
      "source": "2047397646",
      "target": "1bd59bff99d5774388764bc42390e6732cf52a1f",
      "type": "authored"
    },
    {
      "source": "2325729518",
      "target": "1bd59bff99d5774388764bc42390e6732cf52a1f",
      "type": "authored"
    },
    {
      "source": "2124977416",
      "target": "b16c7d45183b9d595ab64301be019741b1528860",
      "type": "authored"
    },
    {
      "source": "2184031883",
      "target": "b16c7d45183b9d595ab64301be019741b1528860",
      "type": "authored"
    },
    {
      "source": "73775191",
      "target": "b16c7d45183b9d595ab64301be019741b1528860",
      "type": "authored"
    },
    {
      "source": "2257332063",
      "target": "b16c7d45183b9d595ab64301be019741b1528860",
      "type": "authored"
    },
    {
      "source": "2258957756",
      "target": "b16c7d45183b9d595ab64301be019741b1528860",
      "type": "authored"
    },
    {
      "source": "2278435713",
      "target": "b16c7d45183b9d595ab64301be019741b1528860",
      "type": "authored"
    },
    {
      "source": "2278435713",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2260297681",
      "target": "b16c7d45183b9d595ab64301be019741b1528860",
      "type": "authored"
    },
    {
      "source": "103476203",
      "target": "b16c7d45183b9d595ab64301be019741b1528860",
      "type": "authored"
    },
    {
      "source": "103476203",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "103476203",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "2129663",
      "target": "b16c7d45183b9d595ab64301be019741b1528860",
      "type": "authored"
    },
    {
      "source": "26958176",
      "target": "fdacf2a732f55befdc410ea927091cad3b791f13",
      "type": "authored"
    },
    {
      "source": "26958176",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "26958176",
      "target": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
      "type": "authored"
    },
    {
      "source": "144202874",
      "target": "35b142ea69598e6241f0011312128031df55895c",
      "type": "authored"
    },
    {
      "source": "2274917091",
      "target": "35b142ea69598e6241f0011312128031df55895c",
      "type": "authored"
    },
    {
      "source": "2274917091",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2278384213",
      "target": "35b142ea69598e6241f0011312128031df55895c",
      "type": "authored"
    },
    {
      "source": "2278599324",
      "target": "35b142ea69598e6241f0011312128031df55895c",
      "type": "authored"
    },
    {
      "source": "2278599324",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "49176273",
      "target": "35b142ea69598e6241f0011312128031df55895c",
      "type": "authored"
    },
    {
      "source": "49176273",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2144332771",
      "target": "37ac7683543f0e039197a56e71e752a9ebe5998e",
      "type": "authored"
    },
    {
      "source": "2282549674",
      "target": "37ac7683543f0e039197a56e71e752a9ebe5998e",
      "type": "authored"
    },
    {
      "source": "2294813888",
      "target": "37ac7683543f0e039197a56e71e752a9ebe5998e",
      "type": "authored"
    },
    {
      "source": "2282536569",
      "target": "37ac7683543f0e039197a56e71e752a9ebe5998e",
      "type": "authored"
    },
    {
      "source": "2109654065",
      "target": "37ac7683543f0e039197a56e71e752a9ebe5998e",
      "type": "authored"
    },
    {
      "source": "2249891947",
      "target": "37ac7683543f0e039197a56e71e752a9ebe5998e",
      "type": "authored"
    },
    {
      "source": "2282532623",
      "target": "37ac7683543f0e039197a56e71e752a9ebe5998e",
      "type": "authored"
    },
    {
      "source": "2279159169",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2278251752",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "3335836",
      "target": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
      "type": "authored"
    },
    {
      "source": "2257315251",
      "target": "1cff5549753a518ed9d6a3517b5050968d710b27",
      "type": "authored"
    },
    {
      "source": "2262654002",
      "target": "1cff5549753a518ed9d6a3517b5050968d710b27",
      "type": "authored"
    },
    {
      "source": "2257376355",
      "target": "1cff5549753a518ed9d6a3517b5050968d710b27",
      "type": "authored"
    },
    {
      "source": "2067331064",
      "target": "1cff5549753a518ed9d6a3517b5050968d710b27",
      "type": "authored"
    },
    {
      "source": "2257129989",
      "target": "1cff5549753a518ed9d6a3517b5050968d710b27",
      "type": "authored"
    },
    {
      "source": "1660848177",
      "target": "f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31",
      "type": "authored"
    },
    {
      "source": "2191691224",
      "target": "f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31",
      "type": "authored"
    },
    {
      "source": "2228824",
      "target": "f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31",
      "type": "authored"
    },
    {
      "source": "2104677959",
      "target": "f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31",
      "type": "authored"
    },
    {
      "source": "2266468147",
      "target": "f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31",
      "type": "authored"
    },
    {
      "source": "3376845",
      "target": "f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31",
      "type": "authored"
    },
    {
      "source": "1706809",
      "target": "f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31",
      "type": "authored"
    },
    {
      "source": "3149531",
      "target": "f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31",
      "type": "authored"
    },
    {
      "source": "2266802555",
      "target": "f56ad6a6a588945c4ee9ddd7b0d1efef2f2f3f31",
      "type": "authored"
    },
    {
      "source": "2401865",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2233293602",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "46879944",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2727584",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2301500865",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2756187",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2233293962",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "22229139",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "50420713",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "22253126",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2233294119",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "34672074",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2282478",
      "target": "0b0debb710366cdff461938c80763eace1651af6",
      "type": "authored"
    },
    {
      "source": "2118481100",
      "target": "464770587aece80cc9e3451050058e30c2aa6666",
      "type": "authored"
    },
    {
      "source": "35084211",
      "target": "464770587aece80cc9e3451050058e30c2aa6666",
      "type": "authored"
    },
    {
      "source": "3040379",
      "target": "464770587aece80cc9e3451050058e30c2aa6666",
      "type": "authored"
    },
    {
      "source": "1745524",
      "target": "464770587aece80cc9e3451050058e30c2aa6666",
      "type": "authored"
    },
    {
      "source": "144365875",
      "target": "464770587aece80cc9e3451050058e30c2aa6666",
      "type": "authored"
    },
    {
      "source": "144365875",
      "target": "399e7d8129c60818ee208f236c8dda17e876d21f",
      "type": "authored"
    },
    {
      "source": "1982950",
      "target": "464770587aece80cc9e3451050058e30c2aa6666",
      "type": "authored"
    },
    {
      "source": "1982950",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "2347348532",
      "target": "3d300c233e7f2c0dfda0266db077a6041a592aae",
      "type": "authored"
    },
    {
      "source": "2347345159",
      "target": "bdf8d1fae0387b7d49766c650fe9f0f7d5c887e4",
      "type": "authored"
    },
    {
      "source": "2319415753",
      "target": "bdf8d1fae0387b7d49766c650fe9f0f7d5c887e4",
      "type": "authored"
    },
    {
      "source": "2232080874",
      "target": "407c4f4237fc0defd4182b027d67af145ca59f3f",
      "type": "authored"
    },
    {
      "source": "3422551",
      "target": "407c4f4237fc0defd4182b027d67af145ca59f3f",
      "type": "authored"
    },
    {
      "source": "2317010356",
      "target": "407c4f4237fc0defd4182b027d67af145ca59f3f",
      "type": "authored"
    },
    {
      "source": "1798404",
      "target": "407c4f4237fc0defd4182b027d67af145ca59f3f",
      "type": "authored"
    },
    {
      "source": "2029521",
      "target": "407c4f4237fc0defd4182b027d67af145ca59f3f",
      "type": "authored"
    },
    {
      "source": "47613860",
      "target": "407c4f4237fc0defd4182b027d67af145ca59f3f",
      "type": "authored"
    },
    {
      "source": "2054098978",
      "target": "407c4f4237fc0defd4182b027d67af145ca59f3f",
      "type": "authored"
    },
    {
      "source": "46254985",
      "target": "407c4f4237fc0defd4182b027d67af145ca59f3f",
      "type": "authored"
    },
    {
      "source": "2347352643",
      "target": "407c4f4237fc0defd4182b027d67af145ca59f3f",
      "type": "authored"
    },
    {
      "source": "2256311663",
      "target": "c77e4e3e3fcc0572c901e4567aa671c9cbeda556",
      "type": "authored"
    },
    {
      "source": "2007771781",
      "target": "c77e4e3e3fcc0572c901e4567aa671c9cbeda556",
      "type": "authored"
    },
    {
      "source": "2347043166",
      "target": "c77e4e3e3fcc0572c901e4567aa671c9cbeda556",
      "type": "authored"
    },
    {
      "source": "2212045428",
      "target": "c77e4e3e3fcc0572c901e4567aa671c9cbeda556",
      "type": "authored"
    },
    {
      "source": "2347348065",
      "target": "c77e4e3e3fcc0572c901e4567aa671c9cbeda556",
      "type": "authored"
    },
    {
      "source": "2210213483",
      "target": "33a1c680ebce8d6b371a1628af201e5528cd14ab",
      "type": "authored"
    },
    {
      "source": "4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "target": "163b4d6a79a5b19af88b8585456363340d9efd04",
      "type": "cites",
      "contexts": "[\"The results, presented in Table 6, were compared against oracle Chinese transcriptions generated by GPT-4o (Achiam et al., 2023) from the corresponding English speech.\", \"Speech Summarization (semantic) assesses sum-mary quality with ROUGE-L (Lin, 2004) and ME-TEOR (Banerjee and Lavie, 2005), using three GPT-4o mini (OpenAI, 2024) reference summaries.\", \", We gave GPT-4o mini the transcription of the speech to generate three candidate summaries as the solutions.\", \"We wrote eight different questions for each task except Free Q&A, for which GPT-4o mini generates three questions and reference solutions uniquely for each utterance.\", \"Three pairs of freeform questions and answers were generated for either speaker of each utterance with the following to GPT-4o mini: \\\"You are listening to a conversation.\", \"Three questions and reference answers were generated by GPT-4o mini, with performance measured by ROUGE-L and METEOR.\", \"The content of the replacement is generated by GPT-4o (Achiam et al., 2023) with the following prompt: Foreground Speech: \\\"\"]",
      "is_influential": true
    },
    {
      "source": "4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "cites",
      "contexts": "[\"In transcription, AAD-LLM significantly lowered the WER to 10.6% compared to Qwen2-Audio\\u2019s 81.0%.\", \"They typically consist of a speech encoder and a pretrained textual LLM, which are jointly trained for tasks including speech description, recognition, and Q&A. Qwen2-Audio (Chu et al., 2024), the current state-of-the-art auditory LLM on speech understanding benchmarks (Sakshi et al., 2024; Wang et al., 2025), serves as both a baseline and the backbone of our proposed AAD-LLM.\", \"As a brief overview, Qwen2-Audio integrates a Whisper (Radford et al., 2023) speech encoder and a Qwen2 (Yang et al., 2024) LLM.\", \"AAD-LLM adopts the backbone of Qwen2-Audio (Chu et al., 2024).\", \"For other speech tasks, AAD-LLM outperforms all intention-uninformed auditory LLMs and a cascaded speech extractor and Qwen2-Audio (fine-tuned on the same data) on most metrics, particularly in transcription and summarization of the attended speaker.\", \"The results show that AAD-LLM significantly outperforms Qwen2-Audio on both speech mixtures and random speakers, while approaching the upper bound set by Qwen2-Audio with oracle speakers\\u2014a pattern consistent with the results observed in the tasks on which AAD-LLM was trained.\", \"The pretrained checkpoint Qwen2-Audio-7B-Instruct is publicly available 5 .\", \"In Q&A, AAD-LLM achieved a ROUGE-L of 64.2 and METEOR of 65.7, sur-passing Qwen2-Audio\\u2019s 41.5 and 42.4.\", \"In reality, <ATT> is <|extra_124|> from Qwen2-Audio\\u2019s reserved special token sets.\", \"We trained all ablation models and baseline Qwen2-Audio on a single speaker with the exact same training configuration.\", \"The acoustic embedding of the speech input encoded by Whisper is concatenated with the textual embedding of the question, and processed together by Qwen2 to output an answer.\", \"AAD-LLM achieved high classification accuracy in speaker description (99.4% gender, 90.9% pitch, 88.0% tempo), outperforming Qwen2-Audio and other baseline models.\", \"Existing models, such as LTU (Gong et al., 2024), SALMONN (Tang et al., 2024), and Qwen2-Audio (Chu et al., 2024), process all in-coming audio equally, making them ineffective in scenarios where distinguishing between attended and unattended speech is essential.\"]",
      "is_influential": true
    },
    {
      "source": "2243118841",
      "target": "4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "type": "authored"
    },
    {
      "source": "2294424733",
      "target": "4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "type": "authored"
    },
    {
      "source": "2283145483",
      "target": "4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "type": "authored"
    },
    {
      "source": "2275225078",
      "target": "4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "type": "authored"
    },
    {
      "source": "31593564",
      "target": "4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "type": "authored"
    },
    {
      "source": "2671690",
      "target": "4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "type": "authored"
    },
    {
      "source": "145898353",
      "target": "4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "type": "authored"
    },
    {
      "source": "2346983817",
      "target": "4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "type": "authored"
    },
    {
      "source": "1686269",
      "target": "4c7c29c784b4e2ebcec2b89ffb3fc3f6fbac0eb6",
      "type": "authored"
    },
    {
      "source": "2211526996",
      "target": "0369023acce546aa8ceabbbc36a17a2f2845ebe3",
      "type": "authored"
    },
    {
      "source": "2316444865",
      "target": "0369023acce546aa8ceabbbc36a17a2f2845ebe3",
      "type": "authored"
    },
    {
      "source": "2265652686",
      "target": "0369023acce546aa8ceabbbc36a17a2f2845ebe3",
      "type": "authored"
    },
    {
      "source": "2347351201",
      "target": "3d1e88f27198ab65aceb35034d0f2321555c0844",
      "type": "authored"
    },
    {
      "source": "2347321811",
      "target": "7007fc0fefeec40107984217f9cfa52c87b3bc8f",
      "type": "authored"
    },
    {
      "source": "2347327645",
      "target": "7007fc0fefeec40107984217f9cfa52c87b3bc8f",
      "type": "authored"
    },
    {
      "source": "e09f2fbd978500d9782ae52f93c33c5cd4af99d8",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "cites",
      "contexts": "[\"We use the Llama 3.1 8b model for our experiments, which is part of a project by Meta AI to provide large state-of-the-art pretrained LLMs to the research community free of charge (Touvron et al., 2023).\"]",
      "is_influential": true
    },
    {
      "source": "2347352054",
      "target": "e09f2fbd978500d9782ae52f93c33c5cd4af99d8",
      "type": "authored"
    },
    {
      "source": "2347353699",
      "target": "e09f2fbd978500d9782ae52f93c33c5cd4af99d8",
      "type": "authored"
    },
    {
      "source": "2257363922",
      "target": "32d01d0710586a50b95d12f504f94d7310a208fe",
      "type": "authored"
    },
    {
      "source": "2ede2aeaf08265ae32a2abf16df2263aa4c5491e",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "cites",
      "contexts": "[\"Rather than using a fixed function, the analysis-confidence pairs { ( r i , c i ) } are passed to LLaMA (Touvron et al., 2023), which adaptively integrates the outputs by balancing consensus with individual model expertise.\", \"2, ChatMotion is a multi-agent system that processes user queries involving motion and video data through the Planner, Executor, and Verifier, with LLaMA-70B (Touvron et al., 2023) employed for all agents.\", \"The module then integrates this context with the user\\u2019s specific requirements, represented as R , to generate a comprehensive response: where \\u0393( \\u00b7 ) denotes LLaMA (Touvron et al., 2023) by default.\", \"\\u2026this mechanism evaluates { ( r i , c i ) } pairs alongside the original motion or video data M , generating an initial estimate: LLaMA (Touvron et al., 2023) then re-examines the preliminary result r \\u2032 and the original pairs { ( r i , c i ) } to mitigate model bias and refine the\\u2026\"]",
      "is_influential": true
    },
    {
      "source": "2347139631",
      "target": "2ede2aeaf08265ae32a2abf16df2263aa4c5491e",
      "type": "authored"
    },
    {
      "source": "2347044005",
      "target": "2ede2aeaf08265ae32a2abf16df2263aa4c5491e",
      "type": "authored"
    },
    {
      "source": "2347161512",
      "target": "2ede2aeaf08265ae32a2abf16df2263aa4c5491e",
      "type": "authored"
    },
    {
      "source": "2347044495",
      "target": "2ede2aeaf08265ae32a2abf16df2263aa4c5491e",
      "type": "authored"
    },
    {
      "source": "2347165252",
      "target": "2ede2aeaf08265ae32a2abf16df2263aa4c5491e",
      "type": "authored"
    },
    {
      "source": "2347042311",
      "target": "2ede2aeaf08265ae32a2abf16df2263aa4c5491e",
      "type": "authored"
    },
    {
      "source": "2347043491",
      "target": "2ede2aeaf08265ae32a2abf16df2263aa4c5491e",
      "type": "authored"
    },
    {
      "source": "0fdad43469669767e9ce55b9eddab6620d989f0b",
      "target": "57e849d0de13ed5f91d086936296721d4ff75a75",
      "type": "cites",
      "contexts": "[\"The perplexity results for Llama2-7B [19] are shown in Table II.\", \"While weight quantization significantly reduces memory traffic for LLMs (e.g., Llama2-70B [19] requires 131.6 GB with FP16 weights but only 35.8 GB with INT4 weight-only quantization), it does not lower computational costs.\", \"Large language models (LLMs) have demonstrated exceptional performance across a wide range of complex natural language processing tasks [19], [23].\"]",
      "is_influential": true
    },
    {
      "source": "1820826857",
      "target": "0fdad43469669767e9ce55b9eddab6620d989f0b",
      "type": "authored"
    },
    {
      "source": "2265595883",
      "target": "0fdad43469669767e9ce55b9eddab6620d989f0b",
      "type": "authored"
    },
    {
      "source": "9352814",
      "target": "0fdad43469669767e9ce55b9eddab6620d989f0b",
      "type": "authored"
    },
    {
      "source": "2053620465",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "143724481",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "143724481",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "10721120",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "2082239112",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "2082239112",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "2162840444",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "35752280",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "35752280",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "2118914337",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "2154975456",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "2116235416",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "2116235416",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "2146367747",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "2064737506",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "2155451431",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "2065332326",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "2065332326",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "1709797",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "2137813791",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "1759422",
      "target": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
      "type": "authored"
    },
    {
      "source": "2003696840",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2949185",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2066663381",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "80424302",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2190282134",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2993731",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "1846431",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2907260",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "50195579",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "50195579",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "2531268",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "1451644426",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "68990982",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "46219923",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "48983885",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "32136590",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "1584940075",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "46181066",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2168170616",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "145814654",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "147846651",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2285868436",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2172404846",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2262249",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "143945447",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "49501003",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2402716",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2402716",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2402716",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "2402716",
      "target": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "type": "authored"
    },
    {
      "source": "2273789852",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2183598223",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2078619062",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "8129718",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "73769093",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "145046059",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2190281124",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "66247317",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "35966970",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "1591176064",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2261291789",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "89269402",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "71075073",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2518906",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "9215251",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "79512668",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2190281122",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2047591327",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2088048322",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2067891070",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "13656138",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2158858559",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2218938",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "90563027",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2057078797",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "47948569",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "1429833598",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2060080508",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "1404791152",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "144979591",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2164872258",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "34176020",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "144549416",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2116123009",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2146695800",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2094755167",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "143779690",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "90615055",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2157630500",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "46258841",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "51128119",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "20308468",
      "target": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
      "type": "authored"
    },
    {
      "source": "2108244542",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "3849208",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "2347956",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "1782969",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "2138579860",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "40511414",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "88728159",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "5382923",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "1785372925",
      "target": "13a0d8bb38f739990c8cd65a44061c6534f17221",
      "type": "authored"
    },
    {
      "source": "2044098905",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "2162462983",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "1404060481",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "2044198157",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "46350295",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "2044198134",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "2049410219",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "80842917",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "15043672",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "2162462141",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "2162467233",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "2049583158",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "2167077094",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "2024731554",
      "target": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
      "type": "authored"
    },
    {
      "source": "6062736",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "authored"
    },
    {
      "source": "13622184",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "authored"
    },
    {
      "source": "40527594",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "authored"
    },
    {
      "source": "40527594",
      "target": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "type": "authored"
    },
    {
      "source": "3407285",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "authored"
    },
    {
      "source": "2052366271",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "authored"
    },
    {
      "source": "2052366271",
      "target": "77d956cdab4508d569ae5741549b78e715fd0749",
      "type": "authored"
    },
    {
      "source": "144239765",
      "target": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
      "type": "authored"
    },
    {
      "source": "48639938",
      "target": "77d956cdab4508d569ae5741549b78e715fd0749",
      "type": "authored"
    },
    {
      "source": "47107786",
      "target": "77d956cdab4508d569ae5741549b78e715fd0749",
      "type": "authored"
    },
    {
      "source": "2058365883",
      "target": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "authored"
    },
    {
      "source": "2058365883",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2624088",
      "target": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "authored"
    },
    {
      "source": "51150953",
      "target": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "authored"
    },
    {
      "source": "40377863",
      "target": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "authored"
    },
    {
      "source": "40377863",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "122064392",
      "target": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "authored"
    },
    {
      "source": "145941081",
      "target": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "authored"
    },
    {
      "source": "2053829286",
      "target": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "authored"
    },
    {
      "source": "1397917613",
      "target": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "authored"
    },
    {
      "source": "152549864",
      "target": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
      "type": "authored"
    },
    {
      "source": "2054194196",
      "target": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "type": "authored"
    },
    {
      "source": "2090511698",
      "target": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "type": "authored"
    },
    {
      "source": "1962694751",
      "target": "399e7d8129c60818ee208f236c8dda17e876d21f",
      "type": "authored"
    },
    {
      "source": "2729164",
      "target": "399e7d8129c60818ee208f236c8dda17e876d21f",
      "type": "authored"
    },
    {
      "source": "1699545",
      "target": "399e7d8129c60818ee208f236c8dda17e876d21f",
      "type": "authored"
    },
    {
      "source": "15652489",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "52578817",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "90784578",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "123052390",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "144729897",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "114577307",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "153215783",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "3443442",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "3443442",
      "target": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "type": "authored"
    },
    {
      "source": "39172707",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "2544107",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "3259253",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "145024664",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "145024664",
      "target": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "type": "authored"
    },
    {
      "source": "2554321",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "1744179",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "2555924",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "39328010",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "39328010",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "authored"
    },
    {
      "source": "39328010",
      "target": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "type": "authored"
    },
    {
      "source": "2827616",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "2827616",
      "target": "4f57f486adea0bf95c252620a4e8af39232ef8bc",
      "type": "authored"
    },
    {
      "source": "1754497",
      "target": "17dbd7b72029181327732e4d11b52a08ed4630d0",
      "type": "authored"
    },
    {
      "source": "718c86daf8e8e59446a7c14f19e8c06ed4659581",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "cites",
      "contexts": "[\"Sparse Upcycling (Komatsuzaki et al., 2022) has been proposed as a technique to initialize MoE models by copying Feed-Forward Networks (FFN) from dense models as multiple experts.\", \"Sparse Upcycling (Komatsuzaki et al., 2022) is a widely adopted method for initializing a Mixture of Experts (MoE) model using a pre-trained dense checkpoint.\", \"Our experiments show that CLIP-MoE not only outperforms other fine-tuning baselines but also surpasses popular MoE-construction methods like Sparse Upcycling (Komatsuzaki et al., 2022).\"]",
      "is_influential": true
    },
    {
      "source": "2284727955",
      "target": "718c86daf8e8e59446a7c14f19e8c06ed4659581",
      "type": "authored"
    },
    {
      "source": "2265753258",
      "target": "718c86daf8e8e59446a7c14f19e8c06ed4659581",
      "type": "authored"
    },
    {
      "source": "2265753258",
      "target": "f67fc0ac665d7b999ce4d67866688f564fd0f55f",
      "type": "authored"
    },
    {
      "source": "1914586128",
      "target": "718c86daf8e8e59446a7c14f19e8c06ed4659581",
      "type": "authored"
    },
    {
      "source": "1914586128",
      "target": "f67fc0ac665d7b999ce4d67866688f564fd0f55f",
      "type": "authored"
    },
    {
      "source": "2307325455",
      "target": "718c86daf8e8e59446a7c14f19e8c06ed4659581",
      "type": "authored"
    },
    {
      "source": "2307325455",
      "target": "f67fc0ac665d7b999ce4d67866688f564fd0f55f",
      "type": "authored"
    },
    {
      "source": "ff8a40349db17daaed78def5f192229c3c2e2527",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "cites",
      "contexts": "[\"Unlike sparse upcycling [16], which merely copies the Multilayer Perceptron (MLP) to construct experts, checkpoint recycling leverages various dense checkpoints and multiple weight selection methods.\", \"3.1 and the method of using duplicated MLPs to construct experts in Sparse Upcycling [16], we conducted experiments on ImageNet.\", \"However, few studies explore leveraging dense model checkpoints to accelerate MoE training [16].\", \"MoEfica-tion [51] partitions a dense model into MoE components, while Sparse Upcycling [16] replicates a dense model multiple times to form a MoE model.\"]",
      "is_influential": true
    },
    {
      "source": "2284636478",
      "target": "ff8a40349db17daaed78def5f192229c3c2e2527",
      "type": "authored"
    },
    {
      "source": "2305630565",
      "target": "ff8a40349db17daaed78def5f192229c3c2e2527",
      "type": "authored"
    },
    {
      "source": "94882716",
      "target": "ff8a40349db17daaed78def5f192229c3c2e2527",
      "type": "authored"
    },
    {
      "source": "2305566339",
      "target": "ff8a40349db17daaed78def5f192229c3c2e2527",
      "type": "authored"
    },
    {
      "source": "2266428398",
      "target": "ff8a40349db17daaed78def5f192229c3c2e2527",
      "type": "authored"
    },
    {
      "source": "2238115894",
      "target": "ff8a40349db17daaed78def5f192229c3c2e2527",
      "type": "authored"
    },
    {
      "source": "4915538917afdfebbdc97132b6a430497db4fc54",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "cites",
      "contexts": "[\"While most MoE models are trained from scratch, sparse up-183 cycling (Komatsuzaki et al., 2023)\", \"We demonstrate the importance of the shared ex-500 pert of X FT by comparing its performance with the sparse upcycling (Komatsuzaki et al., 2023) baseline that does not employ any shared expert.\", \"\\\"- Shared Expert\\\" removes the shared expert setting, making MoE DS the same architecture as original sparse upcycling (Komatsuzaki et al., 2023).\"]",
      "is_influential": true
    },
    {
      "source": "2269694074",
      "target": "4915538917afdfebbdc97132b6a430497db4fc54",
      "type": "authored"
    },
    {
      "source": "2296736695",
      "target": "4915538917afdfebbdc97132b6a430497db4fc54",
      "type": "authored"
    },
    {
      "source": "2237736409",
      "target": "4915538917afdfebbdc97132b6a430497db4fc54",
      "type": "authored"
    },
    {
      "source": "2080123731",
      "target": "4915538917afdfebbdc97132b6a430497db4fc54",
      "type": "authored"
    },
    {
      "source": "2289125201",
      "target": "4915538917afdfebbdc97132b6a430497db4fc54",
      "type": "authored"
    },
    {
      "source": "a12bc2ddaa3a91edab682f6822e381ce9323e8fc",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "cites",
      "contexts": "[\"In a different approach, Soft MoEs (Puigcerver et al., 2023) instead allow experts to process weighted combinations of tokens, offering more flexibility.\", \"Puigcerver et al. (2023) formulated soft MoEs that are fully differentiable while being as efficient as sparse MoEs.\", \"In contrast, the recently proposed soft MoE (Puigcerver et al., 2023) router matches linearly combined inputs with experts.\", \"Compared to traditional dense models, MoEs yield better performance\\u2013compute trade-off (Shazeer et al., 2017; Riquelme et al., 2021; You et al., 2021; Mustafa et al., 2022; Puigcerver et al., 2023).\", \"Notably, SoftMoE performs best with a larger number of experts (e.g., E = 2048) and few slots per expert, as shown in Puigcerver et al. (2023, Figure 6).\", \"Komatsuzaki et al. (2023) create sparse MoEs from pre-existing dense models as a way to reuse the sunk cost for training dense models.\"]",
      "is_influential": true
    },
    {
      "source": "2281864351",
      "target": "a12bc2ddaa3a91edab682f6822e381ce9323e8fc",
      "type": "authored"
    },
    {
      "source": "2281742464",
      "target": "a12bc2ddaa3a91edab682f6822e381ce9323e8fc",
      "type": "authored"
    },
    {
      "source": "145814174",
      "target": "a12bc2ddaa3a91edab682f6822e381ce9323e8fc",
      "type": "authored"
    },
    {
      "source": "145814174",
      "target": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
      "type": "authored"
    },
    {
      "source": "f2e78a574925486d1f13440f55688bcffde80101",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "cites",
      "contexts": "[\"Consequently, we can introduce parameter-efficient MoE layers by integrating adapters, thereby achieving sparsity in a more parameter-efficient manner.\", \"PESC involves inserting adapters (Houlsby et al., 2019) into the MoE layers of sparse models, allowing differentiation between experts without altering each expert\\u2019s weights in the MoE layers when guaranteeing the quality of the approximation in function space compared to original sparse upcycling\\u2026\", \"5.\", \"( Komatsuzaki et al., 2023) presented a method for upcycling dense models into sparse activated mixture-of-experts (MoE) models, which boast greater capacity (Shazeer et al., 2017; Lepikhin et al., 2020; Du et al., 2022; Fedus et al., 2022; Zhou et al., 2022; Ra-jbhandari et al., 2022; Puigcerver\\u2026\", \"Nonetheless, most LLMs are pre-trained dense models de-1 signed based on transformer architecture, which limits scalability during instruction tuning.\"]",
      "is_influential": true
    },
    {
      "source": "2275801674",
      "target": "f2e78a574925486d1f13440f55688bcffde80101",
      "type": "authored"
    },
    {
      "source": "2275801674",
      "target": "639e84addcaf1784ff3c57ed3f6eeacb5a099d98",
      "type": "authored"
    },
    {
      "source": "67219756",
      "target": "f2e78a574925486d1f13440f55688bcffde80101",
      "type": "authored"
    },
    {
      "source": "67219756",
      "target": "639e84addcaf1784ff3c57ed3f6eeacb5a099d98",
      "type": "authored"
    },
    {
      "source": "2278380755",
      "target": "f2e78a574925486d1f13440f55688bcffde80101",
      "type": "authored"
    },
    {
      "source": "2278380755",
      "target": "639e84addcaf1784ff3c57ed3f6eeacb5a099d98",
      "type": "authored"
    },
    {
      "source": "ab7d320cbae173aef86c31faa087780cba44551f",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "cites",
      "contexts": "[\"To efficiently tackle the above, recent works in scaling language models such as a mixture of experts (MoE) (Shazeer et al., 2017; Komatsuzaki et al., 2022) have been proposed.\", \"Moreover, their computational efficiency, especially when implemented in a sparse form, has made them valuable in scenarios where resource constraints are a consideration (Shazeer et al., 2017; Komatsuzaki et al., 2022).\", \"An additional benefit may be the potential for fast performance recovery after the up-scaling is done, as is also observed in MoE (Komatsuzaki et al., 2022).\", \"In the landscape of machine learning architectures, the Mixture of Experts (MoE) models like (Shazeer et al., 2017; Shen et al., 2019; Komatsuzaki et al., 2022) has gained attention for its capability to address the challenges posed by complex and heterogeneous data.\"]",
      "is_influential": true
    },
    {
      "source": "2276488417",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2115195904",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276457059",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2257348655",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276493203",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276638684",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2256984804",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276638686",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276460413",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276491119",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276426727",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276490531",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276483653",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276491256",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276424847",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276424927",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276482720",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "2276457144",
      "target": "ab7d320cbae173aef86c31faa087780cba44551f",
      "type": "authored"
    },
    {
      "source": "6f88133bc591cd964667a626a06debad17775757",
      "target": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e",
      "type": "cites",
      "contexts": "[\"works exist in the computer vision field [34; 45; 44; 16; 19].\", \"In addition to routing network improvements, many works have explored MoE\\u2019s implementation on modern hardwares [14; 31; 28; 16; 27], scaling properties [12; 1; 11; 4], and applications [23; 34; 46; 26; 45; 44; 19; 49; 3].\", \"Secondly, current MoE ViTs [34; 45; 44; 16; 19] are focused on 2D visual tasks, and there are no MoE ViTs for 3D visual tasks (see the fourth row of Tab.\", \"[19] use a pre-trained dense transformer checkpoint to warm-start the training of a MoE model, and achieve promising results on JFT-300M pre-training and ImageNet-1K fine-tuning.\"]",
      "is_influential": true
    },
    {
      "source": "10271383",
      "target": "6f88133bc591cd964667a626a06debad17775757",
      "type": "authored"
    },
    {
      "source": "2142512521",
      "target": "6f88133bc591cd964667a626a06debad17775757",
      "type": "authored"
    },
    {
      "source": "2116084167",
      "target": "6f88133bc591cd964667a626a06debad17775757",
      "type": "authored"
    },
    {
      "source": "39541577",
      "target": "6f88133bc591cd964667a626a06debad17775757",
      "type": "authored"
    },
    {
      "source": "144799987",
      "target": "6f88133bc591cd964667a626a06debad17775757",
      "type": "authored"
    },
    {
      "source": "3001348",
      "target": "6f88133bc591cd964667a626a06debad17775757",
      "type": "authored"
    },
    {
      "source": "145625142",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "145625142",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "145625142",
      "target": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "type": "authored"
    },
    {
      "source": "3351938",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "3351938",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "2159632445",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2065251344",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "3365603",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "144104130",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "1579862074",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "41231781",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "102549875",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "3251354",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2807540",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "7685850",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2118879033",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2160888237",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "1994065972",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2362210",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "143936294",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2148023",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2107790634",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "1914502282",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "144797264",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2108320352",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "69045302",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "144720379",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "1406775898",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "22640071",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "22640071",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "35474601",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "102291298",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2813347",
      "target": "1ed66e048bb025e75aa5ea660545285212e5341f",
      "type": "authored"
    },
    {
      "source": "2060377146",
      "target": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
      "type": "authored"
    },
    {
      "source": "2068720",
      "target": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
      "type": "authored"
    },
    {
      "source": "1809220",
      "target": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
      "type": "authored"
    },
    {
      "source": "51027911",
      "target": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
      "type": "authored"
    },
    {
      "source": "2743563",
      "target": "2a805d0e1b067444a554c5169d189fa1f649f411",
      "type": "authored"
    },
    {
      "source": "2743563",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "authored"
    },
    {
      "source": "144629422",
      "target": "2a805d0e1b067444a554c5169d189fa1f649f411",
      "type": "authored"
    },
    {
      "source": "144629422",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "authored"
    },
    {
      "source": "39611591",
      "target": "2a805d0e1b067444a554c5169d189fa1f649f411",
      "type": "authored"
    },
    {
      "source": "39611591",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "authored"
    },
    {
      "source": "144447820",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "79215748",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "1380243217",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "1380243217",
      "target": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "type": "authored"
    },
    {
      "source": "1666667717",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "34692532",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "2389316",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "2389316",
      "target": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "type": "authored"
    },
    {
      "source": "2157338362",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "2157338362",
      "target": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "type": "authored"
    },
    {
      "source": "2066767241",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "2059685709",
      "target": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
      "type": "authored"
    },
    {
      "source": "2841331",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "authored"
    },
    {
      "source": "3319373",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "authored"
    },
    {
      "source": "2465270",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "authored"
    },
    {
      "source": "2274215058",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "authored"
    },
    {
      "source": "46352821",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "authored"
    },
    {
      "source": "2280399",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "authored"
    },
    {
      "source": "1802148",
      "target": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
      "type": "authored"
    },
    {
      "source": "34946720",
      "target": "1882f194cb43828852cc052887671e55a80f945a",
      "type": "authored"
    },
    {
      "source": "7167328",
      "target": "1882f194cb43828852cc052887671e55a80f945a",
      "type": "authored"
    },
    {
      "source": "2345617",
      "target": "1882f194cb43828852cc052887671e55a80f945a",
      "type": "authored"
    },
    {
      "source": "2145438541",
      "target": "1882f194cb43828852cc052887671e55a80f945a",
      "type": "authored"
    },
    {
      "source": "2145438541",
      "target": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
      "type": "authored"
    },
    {
      "source": "2545358",
      "target": "1882f194cb43828852cc052887671e55a80f945a",
      "type": "authored"
    },
    {
      "source": "3844009",
      "target": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "type": "authored"
    },
    {
      "source": "35025299",
      "target": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
      "type": "authored"
    },
    {
      "source": "144906624",
      "target": "d9f6ada77448664b71128bb19df15765336974a6",
      "type": "authored"
    },
    {
      "source": "144906624",
      "target": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
      "type": "authored"
    },
    {
      "source": "100984698",
      "target": "d9f6ada77448664b71128bb19df15765336974a6",
      "type": "authored"
    },
    {
      "source": "10666396",
      "target": "d9f6ada77448664b71128bb19df15765336974a6",
      "type": "authored"
    },
    {
      "source": "50286460",
      "target": "d9f6ada77448664b71128bb19df15765336974a6",
      "type": "authored"
    },
    {
      "source": "50286460",
      "target": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
      "type": "authored"
    },
    {
      "source": "38614754",
      "target": "d9f6ada77448664b71128bb19df15765336974a6",
      "type": "authored"
    },
    {
      "source": "38614754",
      "target": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
      "type": "authored"
    },
    {
      "source": "145783676",
      "target": "d9f6ada77448664b71128bb19df15765336974a6",
      "type": "authored"
    },
    {
      "source": "145783676",
      "target": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
      "type": "authored"
    },
    {
      "source": "39455775",
      "target": "d9f6ada77448664b71128bb19df15765336974a6",
      "type": "authored"
    },
    {
      "source": "39455775",
      "target": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
      "type": "authored"
    },
    {
      "source": "3644767",
      "target": "d9f6ada77448664b71128bb19df15765336974a6",
      "type": "authored"
    },
    {
      "source": "3644767",
      "target": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
      "type": "authored"
    },
    {
      "source": "144872294",
      "target": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
      "type": "authored"
    },
    {
      "source": "eec6b87ade0f50555d2639317b83d39e1210fed4",
      "target": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "type": "cites",
      "contexts": "[\"All these approaches are applied to an improved and now widely adopted Transformer architecture (Touvron et al., 2023) with rotary positional encoding (RoPE) (Su et al., 2024), SwiGLU MLP (Shazeer, 2020), etc. (often called Transformer++).\", \"On the other hand, mechanistic interpretability studies reveal that Transformers do in-context learning tasks by composing model components (attention heads and MLPs) across different layers to form circuits, (Elhage et al., 2020; Wang et al., 2023; Merullo et al., 2024; Ni et al., 2025), where layers communicate with each other by writing to and reading from different subspaces of the residual stream.\", \"To stabilize training models with large depth/width ratios, we propose a variant of MUDDFormer by applying RM-SNorm before and after DA module, and adding a residual connection to DA module after the post-RMSNorm: It is similar to the hybrid-norm strategy used by recent models such as Gemma 2 (Team et al., 2024) and Grok-1 (xai org, 2024), though we apply it to DA modules instead of MHA/MLP modules.\", \"These studies reveal the critical role of cross-layer interactions between attention heads and MLPs in enabling complex reasoning - a key insight motivating MUDD connections\\u2019 design, which explicitly facilitates such interactions.\", \"We instantiate A i : R D \\u2192 R i +1 with an MLP parameterized by W 1 and W 2 which computes connection weights position-wise: We apply RMSNorm to X i before MLP to stabilize training.\"]",
      "is_influential": true
    },
    {
      "source": "2301203832",
      "target": "eec6b87ade0f50555d2639317b83d39e1210fed4",
      "type": "authored"
    },
    {
      "source": "2301433926",
      "target": "eec6b87ade0f50555d2639317b83d39e1210fed4",
      "type": "authored"
    },
    {
      "source": "2301254852",
      "target": "eec6b87ade0f50555d2639317b83d39e1210fed4",
      "type": "authored"
    },
    {
      "source": "50242841",
      "target": "eec6b87ade0f50555d2639317b83d39e1210fed4",
      "type": "authored"
    },
    {
      "source": "a3fdb2e08bd0a632184918ea9b177144f478c18d",
      "target": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "type": "cites",
      "contexts": "[\"GEGLU 19 works by performing two parallel inputs: one is a pure linear transformation, and the other is a linear transformation through the GELU activation function.\", \"Moreover, GPT-GEGLU outperforms the basic GPT model in terms of validity and similarity ratio, which indicates that the activation function with the introduction of a gating mechanism is more competent for the conditional molecular generation task than the traditional activation function is.\", \"Validity Uniqueness Novelty FCD KL-divergence Mamba 0.963 \\u00b1 0.001 0.999 \\u00b1 0.0 1.000 \\u00b1 0.0 0.914 \\u00b1 0.002 0.995 \\u00b1 0.006 GPT 0.969 \\u00b1 0.001 0.999 \\u00b1 0.0 1.000 \\u00b1 0.0 0.907 \\u00b1 0.003 0.987 \\u00b1 0.011 GPT-RoPE 0.980 \\u00b1 0.003 0.999 \\u00b1 0.0 1.000 \\u00b1 0.0 0.867 \\u00b1 0.002 0.991 \\u00b1 0.017 GPT-Deep 0.964 \\u00b1 0.002 0.999 \\u00b1 0.0 1.000 \\u00b1 0.0 0.899 \\u00b1 0.005 0.989 \\u00b1 0.021 GPT-GEGLU 0.970 \\u00b1 0.004 0.999 \\u00b1 0.0 1.000 \\u00b1 0.0 0.905 \\u00b1 0.004 0.993 \\u00b1 0.013 GPT-con 0.966 \\u00b1 0.002 0.999 \\u00b1 0.0 1.000 \\u00b1 0.0 0.881 \\u00b1 0.003 0.991 \\u00b1 0.018 where \\u00b5 G is the average of distribution G, G represents the covariance matrix of distribution G, and Tr is the trace of the matrix.\", \"Validity Uniqueness Novelty Similarity ratio Mamba 0.960 \\u00b1 0.002 0.753 \\u00b1 0.012 1.000 \\u00b1 0.0 0.821 \\u00b1 0.024 T5MolGe 0.989 \\u00b1 0.001 0.729 \\u00b1 0.009 1.000 \\u00b1 0.0 0.975 \\u00b1 0.017 GPT 0.945 \\u00b1 0.007 0.946 \\u00b1 0.045 1.000 \\u00b1 0.0 0.862 \\u00b1 0.056 GPT-RoPE 0.984 \\u00b1 0.003 0.769 \\u00b1 0.022 1.000 \\u00b1 0.0 0.941 \\u00b1 0.021 GPT-Deep 0.916 \\u00b1 0.004 0.844 \\u00b1 0.034 1.000 \\u00b1 0.0 0.843 \\u00b1 0.038 GPT-GEGLU 0.971 \\u00b1 0.006 0.769 \\u00b1 0.055 1.000 \\u00b1 0.0 0.899 \\u00b1 0.059 GPT-con 0.965 \\u00b1 0.005 0.766 \\u00b1 0.047 1.000 \\u00b1 0.0 0.833 \\u00b1 0.043 Table 4 .\", \"The result of these two transformations is then multiplied element-by-element: GEGLU ( x, W, V, b, c ) = GELU ( xW + b ) \\u2297 ( xV + c ) , where vector x is the input of the hidden representation at a particular position in the sequence, W, V represents linear transformation matrices, and b represents the bias vector.\", \"4, exhibited gradually decreasing and converging at approximately 20 epochs, except for GPT-GEGLU, which showed an increasing trend after 20 epochs, indicating overfitting during the training process.\", \"For the conditional molecular generation task, we compared the performance of six models trained and evaluated under the same conditions, including T5MolGe, Mamba, GPT, GPT-RoPE, GPT-Deep, and GPT-GEGLU.\"]",
      "is_influential": true
    },
    {
      "source": "2140282634",
      "target": "a3fdb2e08bd0a632184918ea9b177144f478c18d",
      "type": "authored"
    },
    {
      "source": "2296078432",
      "target": "a3fdb2e08bd0a632184918ea9b177144f478c18d",
      "type": "authored"
    },
    {
      "source": "2296222589",
      "target": "a3fdb2e08bd0a632184918ea9b177144f478c18d",
      "type": "authored"
    },
    {
      "source": "7298446",
      "target": "a3fdb2e08bd0a632184918ea9b177144f478c18d",
      "type": "authored"
    },
    {
      "source": "2ecaba7bc63baf9ee68940c04714e4a6420e731b",
      "target": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "type": "cites",
      "contexts": "[\"Swish GLU (Shazeer, 2020) was used as the hidden activation function, and dropout layers were added with a rate of 0.3.\"]",
      "is_influential": true
    },
    {
      "source": "104853363",
      "target": "2ecaba7bc63baf9ee68940c04714e4a6420e731b",
      "type": "authored"
    },
    {
      "source": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "target": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "type": "cites",
      "contexts": "[\"We find close to no difference be-tween the two and choose to use GeGLU layers.\", \"We show that encoders benefit from both recent pretraining data scales and architecture improvements from autoregressive LLMs. ModernBERT has a native sequence length of 8,192 tokens and incorporates recent architecture improvements, such as GeGLU layers, RoPE positional embeddings, and alternating local-global attention.\", \"To select the updates to add in the ModernBERT architecture, we performed different ablations, except where stated, most ablations where ran at the 8-20 billion token scale: \\u2022 We compared two GLU layers, GeGLU and SwiGLU.\", \"This is in line with recent work showing consistent empirical improvements when using GLU variants (Shazeer, 2020; Geiping and Goldstein, 2023).\", \"Activation We adopt GeGLU (Shazeer, 2020), a Gated-Linear Units (GLU)-based (Dauphin et al., 2017) activation function built on top of the original BERT\\u2019s GeLU (Hendrycks and Gimpel, 2016) activation function.\"]",
      "is_influential": true
    },
    {
      "source": "2335869834",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "2322445069",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "2276425110",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "2342275696",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "2341914557",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "2335869726",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "2335870173",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "2335870535",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "8759332",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "2335870101",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "2335870756",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "2322441927",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "2335861967",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "2335869805",
      "target": "8dc5a5f57b5a4564536badf3ca98e5680f313314",
      "type": "authored"
    },
    {
      "source": "ca997f1a733e53ad0fa29041246ff655243e8c1b",
      "target": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "type": "cites",
      "contexts": "[\"Figure 2: Block diagrams of Transformer MLP blocks utilizing ReLU/GELU, SwiGLU, PolyReLU and PolyNorm.\", \"In Table 3, we also observe that PolyNorm outperforms SwiGLU on 8 downstream tasks.\", \"The results of both models demonstrate that PolyCom can accelerate the converging speed and significantly outperform SwiGLU, GELU, and ReLU et al. The outline of this paper is structured as follows: In Section 2, we present the mathematical formulation of PolyCom and discuss its integration within transformer architectures.\", \"For both dense and MoE models, we compare SwiGLU with .\", \"PolyNorm out-performs SwiGLU on all tasks, with notable improvements, demonstrating superior generalization capabilities.\", \"Additional, nonlinear activations such as GeLU and SwiGLU can be locally approximated by Taylor polynomials around the origin, which allows us to primarily compare PolyReLU with ReLU and polynomial activations.\", \"Figure 6 shows that PolyReLU and PolyNorm result in higher weight ranks compared to other activation functions such as SwiGLU, GELU, and ReLU.\", \"In Figure 8, we present the training loss scaling curves for dense models utilizing the activation functions SwiGLU, PolyReLU, and PolyNorm.\", \"As illustrated in the figure, both PolyReLU and PolyNorm consistently outperform SwiGLU across model sizes ranging from 110M to 1.3B parameters.\", \"PolyNorm consistently achieves lower validation losses than SwiGLU across all datasets, with an average improvement of 0.02.\", \"Overall, after applying the gradient checkpointing technique, the overhead and memory footprint are acceptable, and there is negligible difference in the training budget required compared to the widely used SwiGLU.\", \"For example, the Swish activation (Ramachandran et al., 2017; Shazeer, 2020) and the Mish activation (Misra, 2019) are smooth and non-monotonic functions that offer potential benefits in model performance and training stability.\", \"Models using PolyNorm consistently show lower losses compared to those using SwiGLU, indicating that PolyNorm enables faster learning.\", \"Additionally, Gated Linear Units (GLU) were proposed by Dauphin et al. (2017), with SwiGLU (Shazeer, 2020), a prominent variant, being used in models such as LLaMA-Series (Touvron et al., 2023).\"]",
      "is_influential": true
    },
    {
      "source": "2210797120",
      "target": "ca997f1a733e53ad0fa29041246ff655243e8c1b",
      "type": "authored"
    },
    {
      "source": "2125061111",
      "target": "ca997f1a733e53ad0fa29041246ff655243e8c1b",
      "type": "authored"
    },
    {
      "source": "122290781",
      "target": "ca997f1a733e53ad0fa29041246ff655243e8c1b",
      "type": "authored"
    },
    {
      "source": "2108764287",
      "target": "ca997f1a733e53ad0fa29041246ff655243e8c1b",
      "type": "authored"
    },
    {
      "source": "2323563576",
      "target": "ca997f1a733e53ad0fa29041246ff655243e8c1b",
      "type": "authored"
    },
    {
      "source": "2329571633",
      "target": "ca997f1a733e53ad0fa29041246ff655243e8c1b",
      "type": "authored"
    },
    {
      "source": "6e8bfe58a437a7c87fbadccff1c4bb5b992b68a3",
      "target": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "type": "cites",
      "contexts": "[\"In the context of vision transformers, an additional variant known as GeGLU (Gaussian Error Gated Linear Unit) and ReGLU (Recti\\ufb01ed Gated Linear Unit) are explored [23].\"]",
      "is_influential": false
    },
    {
      "source": "2188243752",
      "target": "6e8bfe58a437a7c87fbadccff1c4bb5b992b68a3",
      "type": "authored"
    },
    {
      "source": "2282533462",
      "target": "6e8bfe58a437a7c87fbadccff1c4bb5b992b68a3",
      "type": "authored"
    },
    {
      "source": "2baf5fc029effc9a6b3790a7221dc84dba9a0f8e",
      "target": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "type": "cites",
      "contexts": "[\"1) Multi-Head Attention With Gated Unit: Here we introduce the fundamental component of our network, the attention layer [32] with a gated mechanism [33], which could capture the dependency or correlation between each element of the input by calculating weights to learn better representations.\"]",
      "is_influential": false
    },
    {
      "source": "2315510079",
      "target": "2baf5fc029effc9a6b3790a7221dc84dba9a0f8e",
      "type": "authored"
    },
    {
      "source": "2342818160",
      "target": "2baf5fc029effc9a6b3790a7221dc84dba9a0f8e",
      "type": "authored"
    },
    {
      "source": "2284773169",
      "target": "2baf5fc029effc9a6b3790a7221dc84dba9a0f8e",
      "type": "authored"
    },
    {
      "source": "2146175818",
      "target": "2baf5fc029effc9a6b3790a7221dc84dba9a0f8e",
      "type": "authored"
    },
    {
      "source": "2292917033",
      "target": "2baf5fc029effc9a6b3790a7221dc84dba9a0f8e",
      "type": "authored"
    },
    {
      "source": "7105c73fdcb84617e3a02b192bb4f0f9df2c9d95",
      "target": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4",
      "type": "cites",
      "contexts": "[\"Two advanced activation functions of Mish [29] and GeLU [30] were designed to address this problem via setting a small gradient, showing successful applications.\"]",
      "is_influential": false
    },
    {
      "source": "2175564679",
      "target": "7105c73fdcb84617e3a02b192bb4f0f9df2c9d95",
      "type": "authored"
    },
    {
      "source": "2271674741",
      "target": "7105c73fdcb84617e3a02b192bb4f0f9df2c9d95",
      "type": "authored"
    },
    {
      "source": "2149395933",
      "target": "7105c73fdcb84617e3a02b192bb4f0f9df2c9d95",
      "type": "authored"
    },
    {
      "source": "2275416366",
      "target": "7105c73fdcb84617e3a02b192bb4f0f9df2c9d95",
      "type": "authored"
    },
    {
      "source": "2117994803",
      "target": "7105c73fdcb84617e3a02b192bb4f0f9df2c9d95",
      "type": "authored"
    },
    {
      "source": "2347341549",
      "target": "35b9b9404695ec566f554cc2138ee60e9c45e7a9",
      "type": "authored"
    },
    {
      "source": "3377142",
      "target": "4f57f486adea0bf95c252620a4e8af39232ef8bc",
      "type": "authored"
    },
    {
      "source": "40348417",
      "target": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "type": "authored"
    },
    {
      "source": "3877127",
      "target": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "type": "authored"
    },
    {
      "source": "19177000",
      "target": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
      "type": "authored"
    },
    {
      "source": "2921469",
      "target": "88caa4a0253a8b0076176745ebc072864eab66e1",
      "type": "authored"
    },
    {
      "source": "2325985",
      "target": "88caa4a0253a8b0076176745ebc072864eab66e1",
      "type": "authored"
    },
    {
      "source": "2529182",
      "target": "88caa4a0253a8b0076176745ebc072864eab66e1",
      "type": "authored"
    },
    {
      "source": "1700980",
      "target": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb",
      "type": "authored"
    },
    {
      "source": "2706258",
      "target": "05dd7254b632376973f3a1b4d39485da17814df5",
      "type": "authored"
    },
    {
      "source": "2151810148",
      "target": "05dd7254b632376973f3a1b4d39485da17814df5",
      "type": "authored"
    },
    {
      "source": "2787620",
      "target": "05dd7254b632376973f3a1b4d39485da17814df5",
      "type": "authored"
    },
    {
      "source": "145419642",
      "target": "05dd7254b632376973f3a1b4d39485da17814df5",
      "type": "authored"
    },
    {
      "source": "3119801",
      "target": "67107f78a84bdb2411053cb54e94fa226eea6d8e",
      "type": "authored"
    },
    {
      "source": "1713934",
      "target": "67107f78a84bdb2411053cb54e94fa226eea6d8e",
      "type": "authored"
    },
    {
      "source": "1751762",
      "target": "67107f78a84bdb2411053cb54e94fa226eea6d8e",
      "type": "authored"
    },
    {
      "source": "4b4dd452323e2126a9ba3b8a9bc70a0d7d991a16",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "cites",
      "contexts": "[\"Although the equations might appear to describe a single, multi-year training run, organizations typically develop AI systems iteratively across multiple releases\\u2014 upcycling existing models [16, 17], refining data pipelines, and introducing new hardware.\", \"Approaches such as upcycling pretrained models [23, 16, 17], and dynamically adapting model size or precision introduce new optimization strategies.\"]",
      "is_influential": false
    },
    {
      "source": "2338865687",
      "target": "4b4dd452323e2126a9ba3b8a9bc70a0d7d991a16",
      "type": "authored"
    },
    {
      "source": "f67fc0ac665d7b999ce4d67866688f564fd0f55f",
      "target": "d4fb143e6adbc86e0b200d1d131908db1ff24770",
      "type": "cites",
      "contexts": "[\"Previous works (Komatsuzaki et al., 2022; He et al., 2024; Zhu et al., 2024b; Team, 2024; Wei et al., 2024) construct the MoE models from the dense pre-trained LLMs by converting the Multi-Layer Perceptron (MLP) parameters into experts.\"]",
      "is_influential": false
    },
    {
      "source": "f67fc0ac665d7b999ce4d67866688f564fd0f55f",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "cites",
      "contexts": "[\"For all experiments and all MoE models, we set the hyper-parameter \\u03b1 to 0.01 (Zhu et al., 2024b; Muennighoff et al., 2024).\", \"Directly training an MoE model may face load imbalance issues (Muennighoff et al., 2024), namely, there is the risk of routing collapse, where the model consistently chooses only a limited number of experts, thereby hindering the adequate training of other experts and limiting the overall\\u2026\"]",
      "is_influential": true
    },
    {
      "source": "2187286687",
      "target": "f67fc0ac665d7b999ce4d67866688f564fd0f55f",
      "type": "authored"
    },
    {
      "source": "2332306988",
      "target": "f67fc0ac665d7b999ce4d67866688f564fd0f55f",
      "type": "authored"
    },
    {
      "source": "2225238340",
      "target": "f67fc0ac665d7b999ce4d67866688f564fd0f55f",
      "type": "authored"
    },
    {
      "source": "102413194",
      "target": "de4e7a50d29554c807159ae316cf6487cbfcd2db",
      "type": "authored"
    },
    {
      "source": "102413194",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2310608832",
      "target": "de4e7a50d29554c807159ae316cf6487cbfcd2db",
      "type": "authored"
    },
    {
      "source": "66870756",
      "target": "de4e7a50d29554c807159ae316cf6487cbfcd2db",
      "type": "authored"
    },
    {
      "source": "66870756",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "9358910",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2287849760",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2253531461",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2287768443",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2283871700",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2251007290",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "151472559",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "80563762",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "1654185172",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2287874054",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "1789266135",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "1739735954",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2267493444",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2287835428",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2287826306",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2287859963",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "51028721",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "3081566",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2287854990",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2260131747",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2787022",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2287847441",
      "target": "b54c3599a17db5a71349877a8567400117efbade",
      "type": "authored"
    },
    {
      "source": "2261391307",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "authored"
    },
    {
      "source": "2261391884",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "authored"
    },
    {
      "source": "2287756379",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "authored"
    },
    {
      "source": "2261392151",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "authored"
    },
    {
      "source": "2261391303",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "authored"
    },
    {
      "source": "2184103686",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "authored"
    },
    {
      "source": "2184103686",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2278427988",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "authored"
    },
    {
      "source": "2278427725",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "authored"
    },
    {
      "source": "2261392065",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "authored"
    },
    {
      "source": "2268495134",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "authored"
    },
    {
      "source": "2261392232",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "authored"
    },
    {
      "source": "25898662",
      "target": "9548bacc4c7714151b674748dc86e2cc185a4955",
      "type": "authored"
    },
    {
      "source": "2278435471",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2278434888",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2168785331",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2278434464",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2278428931",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2278460092",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "81588783",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "2278590503",
      "target": "411114f989a3d1083d90afd265103132fee94ebe",
      "type": "authored"
    },
    {
      "source": "49596195",
      "target": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
      "type": "authored"
    },
    {
      "source": "2140321952",
      "target": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
      "type": "authored"
    },
    {
      "source": "48448318",
      "target": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
      "type": "authored"
    },
    {
      "source": "e320ec2c51ff2c25371d9d6ba3f8ac38eec1bc9b",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "cites",
      "contexts": "[\"Specifically, we evaluate the pretrained OLMoE model 1 (Muennighoff et al., 2024) on the quiz-based MMLU benchmark (Hendrycks et al., 2020) to address the following questions: \\u2022 How many experts were activated at least once during inference on this benchmark?\", \"The main reason MoE models are preferred over dense models is that they tend to achieve similar performance while activating significantly fewer parameters, thereby reducing training time compared to dense LLMs (Muennighoff et al., 2024).\", \"Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers, instead of fully dense layers, have gained popularity (Du et al., 2022; Wan et al., 2023).\", \"MMLU is a quiz-based benchmark that evaluates the knowledge and reasoning abilities of large language models (LLMs).\"]",
      "is_influential": true
    },
    {
      "source": "2346974350",
      "target": "e320ec2c51ff2c25371d9d6ba3f8ac38eec1bc9b",
      "type": "authored"
    },
    {
      "source": "9307d546f9c253d82086a9793f27cd53bdd14164",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "cites",
      "contexts": "[\"Additionally, we leverage OLMoE (Muennighoff et al., 2024) and Deepseek-MoE (Dai et al., 2024) as the backbone to deploy Router-Tuning on the Mixture of Experts framework.\", \"Through extensive experiments, we demonstrate the effectiveness of our approach across multiple open-source language models, including Llama (Touvron et al., 2023), Mistral (Jiang et al., 2023), Qwen (Bai et al., 2023), Deepseek-MoE (Dai et al., 2024), and OLMoE (Muennighoff et al., 2024).\", \"Therefore, we further extend Router-Tuning to MoE, where we take OLMoE (Muennighoff et al., 2024) and DeepSeek-MoE (Dai et al., 2024) as the back-bones and equip each Expert network with Router-Tuning.\", \"Mixture of Experts (MoE) employs sparse activation, dynamically selecting expert networks for each input, which delivers promising performance in various tasks (Jiang et al., 2024; Dai et al., 2024; Muennighoff et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "2152235390",
      "target": "9307d546f9c253d82086a9793f27cd53bdd14164",
      "type": "authored"
    },
    {
      "source": "2325002094",
      "target": "9307d546f9c253d82086a9793f27cd53bdd14164",
      "type": "authored"
    },
    {
      "source": "2299920928",
      "target": "9307d546f9c253d82086a9793f27cd53bdd14164",
      "type": "authored"
    },
    {
      "source": "2326301764",
      "target": "9307d546f9c253d82086a9793f27cd53bdd14164",
      "type": "authored"
    },
    {
      "source": "2250363276",
      "target": "9307d546f9c253d82086a9793f27cd53bdd14164",
      "type": "authored"
    },
    {
      "source": "2307735047",
      "target": "9307d546f9c253d82086a9793f27cd53bdd14164",
      "type": "authored"
    },
    {
      "source": "2282412203",
      "target": "9307d546f9c253d82086a9793f27cd53bdd14164",
      "type": "authored"
    },
    {
      "source": "896d1d46901af227786ef39ec2a8e27f425dcca6",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "cites",
      "contexts": "[\"Simulation 3 (OLMoE-1B-7B-0924 [Muennighoff et al., 2024]) .\"]",
      "is_influential": false
    },
    {
      "source": "2346992153",
      "target": "896d1d46901af227786ef39ec2a8e27f425dcca6",
      "type": "authored"
    },
    {
      "source": "2264028895",
      "target": "043c7415a827769704a25088f9f8ed95b94b602e",
      "type": "authored"
    },
    {
      "source": "2216413613",
      "target": "043c7415a827769704a25088f9f8ed95b94b602e",
      "type": "authored"
    },
    {
      "source": "2261278385",
      "target": "043c7415a827769704a25088f9f8ed95b94b602e",
      "type": "authored"
    },
    {
      "source": "626751398e48cdaa7b395301e8436fe9d65f15f7",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "cites",
      "contexts": "[\"\\u2026(Liu et al., 2023; Tan et al., 2024), M-A-P\\u2019s MAP-Neo (Zhang et al., 2024b), and AI2\\u2019s OLMO series (Groeneveld et al., 2024a; OLMo et al., 2024; Muennighoff et al., 2024), Mixture of Experts (MOE) (Jacobs et al., 1991) was first proposed in 1991 and is widely used in the field of\\u2026\", \"\\u2026LLM360\\u2019s Amber (Liu et al., 2023; Tan et al., 2024), M-A-P\\u2019s Neo (Zhang et al., 2024b), and AI2\\u2019s OLMO series (Groeneveld et al., 2024a; OLMo et al., 2024; Muennighoff et al., 2024), have addressed these limitations by releasing complete training pipelines, datasets, and intermediate checkpoints.\"]",
      "is_influential": false
    },
    {
      "source": "2344766119",
      "target": "626751398e48cdaa7b395301e8436fe9d65f15f7",
      "type": "authored"
    },
    {
      "source": "2344787271",
      "target": "626751398e48cdaa7b395301e8436fe9d65f15f7",
      "type": "authored"
    },
    {
      "source": "2344819453",
      "target": "626751398e48cdaa7b395301e8436fe9d65f15f7",
      "type": "authored"
    },
    {
      "source": "2314548935",
      "target": "626751398e48cdaa7b395301e8436fe9d65f15f7",
      "type": "authored"
    },
    {
      "source": "e3b033f839d0db90e98884c89347194ad109fbc9",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "cites",
      "contexts": "[\"Recently, state-of-the-art sparse Mixture-of-Experts models, such as Mixtral-8x7B [35] and OLMoE [46], have been released, outperforming their open-source dense transformer counterparts across several benchmarks.\", \"We evaluate our proposed approach on open-source, dense transformer models of varying scales, including Phi-3-mini-4k-instruct (3.8B) [1] and Gemma (7B) [44], as well as the recently introduced sparse Mixture-of-Experts (MoE) model, OlMoE (1B-7B) [46].\"]",
      "is_influential": false
    },
    {
      "source": "29769330",
      "target": "e3b033f839d0db90e98884c89347194ad109fbc9",
      "type": "authored"
    },
    {
      "source": "40465379",
      "target": "e3b033f839d0db90e98884c89347194ad109fbc9",
      "type": "authored"
    },
    {
      "source": "2237796924",
      "target": "e3b033f839d0db90e98884c89347194ad109fbc9",
      "type": "authored"
    },
    {
      "source": "2284683150",
      "target": "e3b033f839d0db90e98884c89347194ad109fbc9",
      "type": "authored"
    },
    {
      "source": "cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "target": "6470152deb5355a3af43a95fcb9797e86dfe66f4",
      "type": "cites",
      "contexts": "[\"This aligns with existing findings that router choices tend to become fixed early in training (Xue et al., 2024; Muennighoff et al., 2024b Since micro-batch balance is a tighter constraint than global-batch balance, we further test reducing the load balance weight of micro-batch balance in Tab 4.\"]",
      "is_influential": false
    },
    {
      "source": "2337236034",
      "target": "cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "type": "authored"
    },
    {
      "source": "2259588806",
      "target": "cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "type": "authored"
    },
    {
      "source": "2341721830",
      "target": "cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "type": "authored"
    },
    {
      "source": "2341530596",
      "target": "cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "type": "authored"
    },
    {
      "source": "2338357176",
      "target": "cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "type": "authored"
    },
    {
      "source": "2315980300",
      "target": "cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "type": "authored"
    },
    {
      "source": "2326803484",
      "target": "cf7f15e93bc4151f39a01b95f58d03179ab10696",
      "type": "authored"
    },
    {
      "source": "2274106056",
      "target": "1da79ca1d102a4a1d0e0caa574c3c05cdc617ffa",
      "type": "authored"
    },
    {
      "source": "2283974603",
      "target": "1da79ca1d102a4a1d0e0caa574c3c05cdc617ffa",
      "type": "authored"
    },
    {
      "source": "40d6c9d96114e697bff6ff10991d5d0c5a821ee4",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "cites",
      "contexts": "[\"Using the step described in \\u00a72.1, we adopt QuaSAR to instruct three LLMs (i.e., GPT-4o, Llama-3-70B, and Qwen2-72B).\", \"We observe general robust improvement over the baseline models (with an improvement of 19.1% for GPT-4o, 11.8% for Llama-3-70B and 17.2% for Qwen2-72B); the results show that the role of QuaSAR as ICL is foremost noticeable for higher-scale LLMs. QuaSAR consistently outperforms CoT, Faithful CoT and CoMAT.\", \"Experiments were performed on GPT-4o (Achiam et al., 2023), Qwen2 (Yang et al., 2024) and Llama-3 (Grattafiori et al., 2024).\", \"2-1B-Instruct Qwen2-72B Qwen/Qwen2-72B-Instruct Qwen2-7B Qwen/Qwen2-7B-Instruct Qwen2.\", \"Table 2 shows an improvement over the baseline of 5.2% for Llama-3-8B, 13.4% for Llama-3-1B, 10.5% for Qwen2-7B and 8.3% for Qwen2-1.\", \"From the results, it clearly emerges that QuaSAR is consistently effective in enhancing the performance of Llama and Qwen2 models when used to generate reasoning demonstrations via GPT-4o.\", \"We fine-tuned the Llama-3 models for 3 epochs with a batch size of 32 and a learning rate equal to 3e-5 with a 0.001 weight decay and the Qwen2 models for the same epochs and batch size.\", \"As introduced in \\u00a72, we use our Step-wise Instruction Chain ( QuaSAR ) to lead Llama-3-1B, -8B, Qwen2-7B and 1B in solving complex tasks by breaking down the solution using the reasoning process described in \\u00a72.3.\", \"2-1B; (iii) three models of the Qwen2 family (Yang et al., 2024): Qwen2-72B, Qwen2-7B and -1B.\", \"5-1.5B Qwen/Qwen2.\"]",
      "is_influential": true
    },
    {
      "source": "2008183566",
      "target": "40d6c9d96114e697bff6ff10991d5d0c5a821ee4",
      "type": "authored"
    },
    {
      "source": "34102057",
      "target": "40d6c9d96114e697bff6ff10991d5d0c5a821ee4",
      "type": "authored"
    },
    {
      "source": "2345924508",
      "target": "40d6c9d96114e697bff6ff10991d5d0c5a821ee4",
      "type": "authored"
    },
    {
      "source": "2242981659",
      "target": "40d6c9d96114e697bff6ff10991d5d0c5a821ee4",
      "type": "authored"
    },
    {
      "source": "639e84addcaf1784ff3c57ed3f6eeacb5a099d98",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "cites",
      "contexts": "[\"Consequently, Li et al. (2023b) proposes GTE to transform Qwen2-7B-instruct into gte-Qwen2-7B-instruct, enabling the model to capture bidirectional context while preserving the original capabilities in understanding Verilog codes.\", \"Qwen2-7B-instruct has been extensively trained on a diverse corpus of Verilog and demonstrates an extraordinary ability to understand and process various styles and complexities of Verilog/System Verilog code, including less standardized or non-optimized representations.\", \"As for the LLM, we utilize gte-Qwen2-7B-instruct (Li et al., 2023b), trained with bidirectional attention mechanisms based on Qwen2-7B (Yang et al., 2024), which has a comprehensive understanding of abstract circuit function described in Verilog codes (Liu et al., 2023; Pei et al., 2024; Tsai et al., 2024; Fang et al., 2024).\", \"The base model of gte-Qwen2-7B-instruct, Qwen2-7B-instruct (Yang et al., 2024), is a decoder-based model with causal attention.\", \"We utilize gte-Qwen2-7B-instruct (Li et al., 2023b) model for Verilog code representation extraction, which is based on a BERT-like encoder transformer architecture with bidirectional attention.\", \"As for the LLM, we utilize gte-Qwen2-7B-instruct (Li et al., 2023b), trained with bidirectional attention mechanisms based on Qwen2-7B (Yang et al., 2024), which has a comprehensive understanding of abstract circuit function described in Verilog codes (Liu et al., 2023; Pei et al., 2024; Tsai et\\u2026\"]",
      "is_influential": true
    },
    {
      "source": "2220363044",
      "target": "639e84addcaf1784ff3c57ed3f6eeacb5a099d98",
      "type": "authored"
    },
    {
      "source": "2346160582",
      "target": "0c0d935a560a766e0e77b146be6bc35a3458bc81",
      "type": "authored"
    },
    {
      "source": "26982950",
      "target": "0c0d935a560a766e0e77b146be6bc35a3458bc81",
      "type": "authored"
    },
    {
      "source": "2107222678",
      "target": "0c0d935a560a766e0e77b146be6bc35a3458bc81",
      "type": "authored"
    },
    {
      "source": "2346894661",
      "target": "0c0d935a560a766e0e77b146be6bc35a3458bc81",
      "type": "authored"
    },
    {
      "source": "e8dfb0ce6684413a5b3206e1e238432405d5dad9",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "cites",
      "contexts": "[\"Recent developments in large language models have further pushed the boundaries, with models such as e5-mistral-7b-instruct (Wang et al., 2023) and gte-Qwen2-1.\", \"\\u2022 Persona and Task Identification: We first employ Qwen2.\", \"5B-instruct (Li et al., 2023), developed from Qwen2 (Yang et al., 2024); bge-large-en-v1.\", \"5-7B-instruct (Li et al., 2023) built on the Qwen2 (Yang et al., 2024) architecture.\", \"1 (Jiang et al., 2023); gte-Qwen2-1.\", \"5B-instruct (Yang et al., 2024) achieving better performance in various embedding tasks.\"]",
      "is_influential": true
    },
    {
      "source": "2260449655",
      "target": "e8dfb0ce6684413a5b3206e1e238432405d5dad9",
      "type": "authored"
    },
    {
      "source": "2246043972",
      "target": "e8dfb0ce6684413a5b3206e1e238432405d5dad9",
      "type": "authored"
    },
    {
      "source": "ca77b83cd1f4710d6df665f28530230bc8b6dece",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "cites",
      "contexts": "[\"Given the high training costs, modern LLMs have returned to Mixture-of-Experts (MoE) architectures [1, 10, 21, 46, 53, 57] as their backbone implementations.\", \"Compared to traditional dense LLMs, MoE-based LLMs only activate a subset of parameters during training and inference, reducing computational overhead while delivering superior generation performance compared to dense LLMs with a comparable number of parameters [1, 10, 21, 46, 53, 57].\", \"MoE inference can serve heterogeneous models [10, 21, 46, 53, 57] with varying prompts [45, 60] in real-world scenarios.\", \"5-MoE [57], and Phi-3.\", \"This sparse activation mechanism significantly reduces the number of floating point operations (FLOPs), enabling MoE-based LLMs to achieve substantially lower training costs compared to dense LLMs [10, 21, 57].\"]",
      "is_influential": true
    },
    {
      "source": "2019682283",
      "target": "ca77b83cd1f4710d6df665f28530230bc8b6dece",
      "type": "authored"
    },
    {
      "source": "2344972958",
      "target": "ca77b83cd1f4710d6df665f28530230bc8b6dece",
      "type": "authored"
    },
    {
      "source": "2146244292",
      "target": "ca77b83cd1f4710d6df665f28530230bc8b6dece",
      "type": "authored"
    },
    {
      "source": "2346026694",
      "target": "ca77b83cd1f4710d6df665f28530230bc8b6dece",
      "type": "authored"
    },
    {
      "source": "263e245a872d59a110c2d08297ec839c8135349c",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "cites",
      "contexts": "[\"5 (Yang et al., 2024), and EAPMT (Wang et al., 2024a).\", \"We train our InstructChild on a Tesla A40 48GB GPU card and it is initialized with Qwen2-7B-Instruct 3 .\", \"\\u2026prompts as guidance to generate text with desired attributes, we develop instruction incorporating personality and narrative structure to fine-tune the LLM ( i.e. , Qwen2-7B-Instruct (Yang et al., 2024)), aiming to adapt text with vivid character portrayals and concise narrative structure.\", \"The integrative instruction is directly fed into the frozen large language model ( i.e. , Qwen2-7B-Instruct) with learnable LoRA layers for child-friendly style text adaptation.\", \"In particular, EAPMT first generates a detailed explanation for each input sentence, encompassing both the literal content and deeper mean-3 https://huggingface.co/Qwen/Qwen2-7B-Instruct ings.\", \"We also expand the Chinese vocabulary for diffusion-based Paraguide and re-trained it for our task. iii) Open-source large language models with relatively moderate parameters ( FT-based ): Llama2-13B (Touvron et al., 2023), Llama3-8B (Dubey et al., 2024), Qwen2-7B (Yang et al., 2024) and GLM4-9B (GLM et al., 2024).\", \"Inspired by previous studies (Wang et al., 2023; Ouyang et al., 2022; Yuan et al., 2024) that successfully apply well-designed prompts as guidance to generate text with desired attributes, we develop instruction incorporating personality and narrative structure to fine-tune the LLM ( i.e. , Qwen2-7B-Instruct (Yang et al., 2024)), aiming to adapt text with vivid character portrayals and concise narrative structure.\", \"\\u2026Chinese vocabulary for diffusion-based Paraguide and re-trained it for our task. iii) Open-source large language models with relatively moderate parameters ( FT-based ): Llama2-13B (Touvron et al., 2023), Llama3-8B (Dubey et al., 2024), Qwen2-7B (Yang et al., 2024) and GLM4-9B (GLM et al., 2024).\"]",
      "is_influential": true
    },
    {
      "source": "2108090448",
      "target": "263e245a872d59a110c2d08297ec839c8135349c",
      "type": "authored"
    },
    {
      "source": "2327887351",
      "target": "263e245a872d59a110c2d08297ec839c8135349c",
      "type": "authored"
    },
    {
      "source": "2328803568",
      "target": "263e245a872d59a110c2d08297ec839c8135349c",
      "type": "authored"
    },
    {
      "source": "2293561416",
      "target": "263e245a872d59a110c2d08297ec839c8135349c",
      "type": "authored"
    },
    {
      "source": "1387837930",
      "target": "263e245a872d59a110c2d08297ec839c8135349c",
      "type": "authored"
    },
    {
      "source": "2279360884",
      "target": "263e245a872d59a110c2d08297ec839c8135349c",
      "type": "authored"
    },
    {
      "source": "0bf1446500757764ac72a0c450b1f295ae5b8d3f",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "cites",
      "contexts": "[\"\\u2026use with models ranging from millions (Devlin et al., 2019; Conneau et al., 2020; Chalkidis et al., 2020; Ca\\u00f1ete et al., 2023; Guti\\u00e9rrez-Fandi\\u00f1o et al., 2021) to billions (OpenAI et al., 2024; Yang et al., 2024; Dubey et al., 2024; Gemma Team et al., 2024; Mistral AI Team, 2024a,b) of parameters.\", \"(Yang et al., 2024) 72b, 7b 152k 128k 18T 30 languages LLama3.\", \"5 (Yang et al., 2024), Llama3.\", \"5 (Yang et al., 2024), the latest iteration of Alibaba Group\\u2019s Qwen series, was chosen for its emphasis on multilingual support for approximately 30 languages and its handling of long context lengths (128k).\", \"\\u2026online ToS (Lippi et al., 2019; Ruggeri et al., 2022; Chalkidis et al., 2022; Dadas et al., 2024; Galassi et al., 2024) and update the methodology to recent developments in Natural Language Processing (NLP) (OpenAI et al., 2024; Dubey et al., 2024; Yang et al., 2024; Mistral AI Team, 2024a).\"]",
      "is_influential": true
    },
    {
      "source": "2343637672",
      "target": "0bf1446500757764ac72a0c450b1f295ae5b8d3f",
      "type": "authored"
    },
    {
      "source": "2343637351",
      "target": "0bf1446500757764ac72a0c450b1f295ae5b8d3f",
      "type": "authored"
    },
    {
      "source": "2343637131",
      "target": "0bf1446500757764ac72a0c450b1f295ae5b8d3f",
      "type": "authored"
    },
    {
      "source": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "target": "54fb839f621e3fe787437ab8ca5f37e7e4726bfe",
      "type": "cites",
      "contexts": "[\"Most experiments are conducted on a single L40S, while RepNoise and other LLMs (Gemma2-9B and Qwen2-7B) are run on a single A100-80G.\", \"Additionally, we observe that in Gemma2-9B, the middle and final layers hold greater importance for safety, while in Qwen2-7B, the safety importance gradually increases across layers, reaching its peak in the final layers.\", \"For the complicated AlpacaEval dataset, it even improves fine-tuning performance by 3.85%, while only Qwen2-7B shows a slight decrease of 0.3% in other LLM experiments.\", \"To verify the robustness of the approach, two state-of-the-art LLMs, Gemma2-9B (Team et al., 2024) and Qwen2-7B (Yang et al., 2024), are included in the evaluation.\", \"In the experiments above, the default model used is Llama2-7B, and the evaluation is further extended to other mainstream LLMs, Gemma2-9B and Qwen2-7B.\"]",
      "is_influential": true
    },
    {
      "source": "2337059751",
      "target": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "type": "authored"
    },
    {
      "source": "2253860508",
      "target": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "type": "authored"
    },
    {
      "source": "2325984735",
      "target": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "type": "authored"
    },
    {
      "source": "2343055466",
      "target": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "type": "authored"
    },
    {
      "source": "2341879393",
      "target": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "type": "authored"
    },
    {
      "source": "2342937556",
      "target": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "type": "authored"
    },
    {
      "source": "2341570262",
      "target": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "type": "authored"
    },
    {
      "source": "2333251144",
      "target": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "type": "authored"
    },
    {
      "source": "2135519749",
      "target": "c2718c3ce5ca5a2e68e4f718c473d73816c9d407",
      "type": "authored"
    },
    {
      "source": "2307468401",
      "target": "50650e66ce7c454595862aac70c2a3e9dde23387",
      "type": "authored"
    },
    {
      "source": "2307076042",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2288066971",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2260453208",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2303795844",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2307075814",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2307075650",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2307077651",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2300177144",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2263428192",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2285134718",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2253869803",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2307208477",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2298413671",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2109077637",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2307075328",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2295923423",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2268783318",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2284734101",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2302140984",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2239424207",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2307893366",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2302812544",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2289785849",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2291469424",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2294349685",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2263498652",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2307075663",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2300177449",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "1712738522",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2307176162",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "1403621152",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2294801385",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2111312892",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2301110844",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2205862099",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2290625851",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2250016690",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2294845418",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2274455560",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2291800800",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2265550676",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2307219813",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2052144945",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2268847370",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "10680347",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2307187635",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2294907281",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2141377570",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2243402027",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2286747770",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2144718801",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2220673267",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "66395694",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2298783034",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    },
    {
      "source": "2291734244",
      "target": "c7f9706898bdfa3241601e075b1305649b174ff1",
      "type": "authored"
    }
  ]
}